# Clustering

El clustering es un método cuyo objetivo es el de crear grupos en base a las relaciones multivariantes que existen en los datos, este método es un método previo a las técnicas de clasificación que existen. La base del clustering es la definición de la **similaridad entre las filas**. Similaridad es definida como una función de distancia entre un par de filas.

Es importante distinguir la existencia de grupos naturales dentro de los datos, normalmente estos grupos son características naturales de las observaciones de interés. 

## Medidas de Disimilaridad

Dado el objetivo del clustering, el aspecto mas importante dentro de estos métodos es utilizar de forma correcta la medida de (di)similaridad entre un para de casos dentro de la base de datos.

La definición de las medidas de distancia es crucial para aplicar estos modelos. Funciones de distancia incorrecta pueden generar sesgos en los resultados y ser un problema para etapas posteriores de la mineria de datos. Debemos distinguir las funciones de distancia según la naturaleza de las variables.

Sean las filas $x$ e $y$ dentro de una base de datos, estos vectores tienen una dimensión $p$, es decir, se observan  $p$ variables para las 2 observaciones.

### Distancia Euclideana: Variables numéricas

$$d(x,y)=\sqrt{\sum_{i=1}^p{(x_i-y_i)^2}}$$

Donde los $x_i$ y $_y_i$ son los valores para la variable $i$ de las observaciones $x$ e $y$.

### Distancia Manhattan: $p$ grande

$$d(x,y)=\sum_{i=1}^p{|x_i-y_i|}$$

### Distancia Minkowski 

$$d(x,y)=\left(\sum_{i=1}^p{|x_i-y_i|^d}\right)^{1/d}$$

```{r}
x<-rnorm(10)
y<-rnorm(10)

sqrt(sum((x-y)^2)) #euclideana
sum(abs(x-y))  #manhattan
mink<-function(x,y,d){
  aux<-sum(abs(x-y)^d)^(1/d)
  return(aux)
}

x1<-rnorm(10)
y1<-rnorm(10)
mink(x=x1,y=y1,d=0.5)
# se recomienda que d -> 0, p->grande 
```


```{r}
aux<-matrix(rnorm(100),nrow=5)
dist(aux) #euclideana
dist(aux, method="manhattan")
dist(aux, method="minkowski", p=3)
```

### programando

```{r}
minkowski<-function(x,y,d){
  dd<-(sum(abs(x-y)**d))**(1/d)
  return(dd)
}
minkowski(c(1,2,3),c(4,2,1),d=2)
x<-c(1,2,3)
y<-c(4,2,1)
sum(abs(x-y)) # manhattan
sqrt(sum(abs(x-y)**2)) # euclideana
# la función de distancia
distancia<-function(bd,d=2){
  nf<-dim(bd)[1]
  DD<-matrix(NA,nf-1,nf-1)
  colnames(DD)<-1:(nf-1)
  rownames(DD)<-2:nf
  for(i in 1:(nf-1)){
    for(j in (i+1):nf){
      DD[j-1,i]<-minkowski(bd[i,],bd[j,],d)
    }
  }
  return(DD)
}
distancia(aux,d=2)
dist(aux)
```

### Variables cualitativas

Para las variables cualitativas se debe considerar los casos cuando estas son nominales y ordinales, distinguir tambien los casos de variables binarias.

```{r,eval=F}
install.packages("vegan")
library(vegan)
```

### Datos mixtos 
Una de los mayores desafios es cuando las variables son mixtas, es decir cuantitativas y cualitativas.

```{r,eval=FALSE}
install.packages("cluster")
library(cluster)
```

## Métodos de clustering

* Partición (k-center)
* Jerárquicos (dendograma)
* Basados en densidad
* Basados en cuadrículas (grid)

```{r}
load(url('https://raw.githubusercontent.com/AlvaroLimber/EST-383/master/data/oct20.RData'))
aux<-matrix(rnorm(100),20,5)
aux1<-cbind(aux,rep(c(1,2,3,4),5))

aux2<-cbind(aux,c(rep(1,5),rep(2,5),rep(3,5),rep(4,5)))

aux3<-cbind(aux,sample(1:4,20,replace = T))


vmean<-rbind(colMeans(aux1[aux1[,6]==1,1:5]),
colMeans(aux1[aux1[,6]==2,1:5]),
colMeans(aux1[aux1[,6]==3,1:5]),
colMeans(aux1[aux1[,6]==4,1:5]))

colMeans(aux2[aux2[,6]==1,1:5])
colMeans(aux2[aux2[,6]==2,1:5])
colMeans(aux2[aux2[,6]==3,1:5])
colMeans(aux2[aux2[,6]==4,1:5])

colMeans(aux3[aux3[,6]==1,1:5])
colMeans(aux3[aux3[,6]==2,1:5])
colMeans(aux3[aux3[,6]==3,1:5])
colMeans(aux3[aux3[,6]==4,1:5])

dist(rbind(aux[1,],vmean))

```

## K-center Clustering (no jerárquicos)

Algoritmo

0. Definición del valor de $k$.
1. Partición de las observaciones en $k$ grupos, obtener el vector de centros de cada grupo (centroides). Se puede trabajar con la media, la mediana o el medoide.
2. Para cada observación calcular las distancia euclidiana (u otra) a los centroides y reasignar la observación en base a la menor distancia, re calcular los centroides en base a la re asignación de cada observación
3. Repetir el paso 2 hasta que que ya no existan más re asignaciones

```{r,eval=F}
library(dplyr)
load(url('https://raw.githubusercontent.com/AlvaroLimber/EST-383/master/data/oct20.RData'))
# preparación de la base de datos
unique(computo$Elección)
bd<-computo %>% filter(País=="Bolivia" & Elección=="Presidente y Vicepresidente")#cobertura

#municipios
bdmun<-bd %>% group_by(Departamento, Provincia, Municipio) %>% summarise_at(vars(Inscritos:Nulos),sum)  

#prop.table(as.matrix(bdmun[,5:13]),1)
bdmun[,5:13]<-bdmun[,5:13]/apply(bdmun[,5:13],1,sum)

#recintos
bdrec<-bd %>% group_by(Departamento, Provincia, Municipio,Localidad,Recinto) %>% summarise_at(vars(Inscritos:Nulos),sum)

# Modelado: K-center
mod1<-kmeans(bdmun[5:13],4)
bdmun<-cbind(bdmun,cl1=mod1$cluster)

mod2<-kmeans(bdmun[5:13],5)
bdmun<-cbind(bdmun,cl2=mod2$cluster)

table(bdmun$cl1)
table(bdmun$cl2)

table(bdmun$Departamento,bdmun$cl2)

bdmun %>% filter(cl1==1)
bdmun %>% filter(cl1==2)
bdmun %>% filter(cl1==3)
bdmun %>% filter(cl1==4)

mod1$centers
mod2$centers

library(ggplot2)

ggplot(bdmun,aes(x=`MAS - IPSP`,y=CC,size=Inscritos))+geom_point(aes(shape=as.factor(cl2)))

ggplot(bdmun,aes(x=`MAS - IPSP`,y=CC, colour=as.factor(cl2),size=Inscritos))+geom_point()

ggplot(bdmun,aes(x=`MAS - IPSP`,y=`21F`, colour=as.factor(cl2),size=Inscritos))+geom_point()


Hmisc::describe(t(mod2$centers))

scale()
```

Ejemplo: 

* Implementar el algoritmo para el k-center, con la distancia de Minkowski, para la media y mediana.


```{r}
rm(list=ls())
library(dplyr)
set.seed(12345)
bd<-data.frame(matrix(rnorm(1000),100,10))

kcenter<-function(bd,k=2,distancia="euclidean",centro="media",semilla=12345){
#0. Definición del valor de $k$.
#k=6;distancia="euclidean";centro="media";semilla=12345
#1. Partición de las observaciones en $k$ grupos, obtener el vector de centros de cada grupo (centroides). Se puede trabajar con la media, la mediana o el medoide.
nf<-nrow(bd)
nc<-ncol(bd)
#bd$k<-rep(seq(1:3),ceiling(nf/k))[1:nf]
set.seed(semilla)
bd$k<-c(1:k,sample(1:k,nf-k,replace = T))
  if(centro=="media"){
      centros<-bd %>% group_by(k) %>% summarise_at(vars(1:nc),mean)
    } else if(centro=="mediana"){
      centros<-bd %>% group_by(k) %>% summarise_at(vars(1:nc),median)
    }
#2. Para cada observación calcular las distancia euclidiana (u otra) a los centroides y reasignar la observación en base a la menor distancia, re calcular los centroides en base a la re asignación de cada observación
  reasig<-1
  while(reasig>0){
    reasig<-0
    for(i in 1:nf){
    #i<-30
    aux<-as.matrix(dist(rbind(bd[i,1:nc],centros[,-1]),method =distancia     ))[-1,1]
    rk<-as.numeric(which(aux==min(aux)))
    #re asignación
      if(rk!=bd$k[i]){
        bd$k[i]<-rk
        reasig<-reasig+1
          if(centro=="media"){
              centros<-bd %>% group_by(k) %>% summarise_at(vars(1:nc),mean)
            } else if(centro=="mediana"){
              centros<-bd %>% group_by(k) %>% summarise_at(vars(1:nc),median)
            }
      } 
    }
  }
#3. Repetir el paso 2 hasta que que ya no existan más re asignaciones
  return(list(k=bd$k,centros=centros))
}
set.seed(12345)
bdtest<-data.frame(matrix(rnorm(1000),100,10))
mod1<-kcenter(bdtest,k=5)# media
mod2<-kcenter(bdtest,k=5,centro = "mediana")
mod3<-kmeans(bdtest,5)

table(mod1$k)
table(mod2$k)
table(mod3$cluster)
```


```{r}
#Nota: La entrada de la funcion es un data frame
kcenter<-function(bd,k=3,d=2,tipo="media",seed=123456){
  nf<-dim(bd)[1]
  nc<-dim(bd)[2]
  #paso1: asignar las k (nf>=k)
  set.seed(seed)
  bd$k<-sample(1:k,nf,replace=T)
  centroide<-NULL
  for(i in 1:k){
    if(tipo=="media"){  
      centroide<-rbind(centroide,apply(bd[bd$k==i,],2, mean))
    } else if(tipo=="mediana"){
      centroide<-rbind(centroide,apply(bd[bd$k==i,],2, median))
    }
  }
  #paso2 (recalcular los centroides al final)
  cc<-1
  while(cc!=0){ #paso3
    cc<-0
    for(i in 1:nf){
      auxd<-NULL
      for(j in 1:k){
        auxd<-c(auxd,minkowski(bd[i,1:nc],centroide[j,1:nc],d=d))
      }
      newk<-which(auxd==min(auxd))
        if(newk!=bd$k[i]){
          bd$k[i] <- newk
          cc<-cc+1
        }
    }
    centroide<-NULL
    for(i in 1:k){
      if(tipo=="media"){  
        centroide<-rbind(centroide,apply(bd[bd$k==i,],2, mean))
      } else if(tipo=="mediana"){
        centroide<-rbind(centroide,apply(bd[bd$k==i,],2, median))
      }
    }
  }
  return(bd)
}
```

* Pensar en un gráfico que permita ver como se asignaron los cluster 

```{r}
bdtest$k<-mod1$k
head(bdtest)
#componentes principales
```


### Validación cluster

* La estructura de los cluster es aleatoria (¿funciona?)
* ¿Cómo definimos el valor de $K$? 

> Silhouette coefficient: 

1. Se obtiene para la observación $i$ el promedio de distancia a todos los objetos en el mismo cluster ($a_i$)
2. Se obtiene para la observación $i$ el promedio de distancia a todos los objetos de los otros clusters ($b_i$)
3. Se define a $s_i$ como el coeficiente, con un recorrido entre $[-1,1]$, para cada observación $i$

$$s_i=\frac{b_i-a_i}{max(a_i,b_i)}$$

Idealmente se espera que $a_i < b_i$ y los $a_i$ cercanos a $0$. 

```{r}
  library(cluster)
aux<-dist(bdtest[,1:10])
#mod1
s1<-silhouette(mod1$k,aux)
#mod2
s2<-silhouette(mod2$k,aux)
#mod1
s3<-silhouette(mod3$cluster,aux)
##############################
mean(s1[,3]);mean(s2[,3]);mean(s3[,3])

plot(s1)
plot(s2)
plot(s3)

#sobre la base IRIS
data("iris")
aux<-kmeans(iris[,-5],3)
s <- silhouette(aux$cluster, dist(iris[,-5]))
plot(s)
```

> Medoide: es el punto de datos que es "menos diferente" de todos los otros puntos de datos. A diferencia del centroide, el medoide tiene que ser uno de los puntos originales.

```{r}
mod4<-pam(bdtest,k=5)
mod4$medoids
mod4$clustering

aux<-dist(bdtest[,1:10])
s4<-silhouette(mod4$clustering,aux)
plot(s4)
#iris
aux2<-pam(iris[,-5],3)
ss <- silhouette(aux2$clustering, dist(iris[,-5]))
plot(ss)
```

* ¿Cuál es el número óptimo de $k$? 

```{r}
library(fpc)# Flexible Procedures for Clustering
sol <- pamk(iris[,-5], krange=2:10, criterion="asw", usepam=TRUE)
sol$nc
pamk(bdtest,krange=2:10,usepam = T)
```


### Distancias para variables nominales (todas nominales)
En este caso la mejor estrategia es llevar las variables con sus categorias a variables binarias. Existen múltiples medidas de distancia para variables binarias, muchas de estas medidas son aproximaciones a las medidas mas conocidas. Entre ellas:

Sean las filas $i$, $j$ que contienen los valores binarios de las variables de estudio. Sea $A$ el total de $1$ que existe en $i$, $B$ el total de $1$ que existe en $j$ y sea $J$ el total de casos en los que los $1$ ocurren simultaneamente en $i$ y $j$.

  * Euclideana
  
  $$d_{ij}=\sqrt{A+B-2J}$$
  * Manhattan
  
  $$d_{ij}=A+B-2J$$
  * Bray
  
  $$d_{ij}=\frac{A+B-2J}{A+B}$$

  * Binomial
  
  $$d_{ij}=log(2)(A+B-2J)$$

```{r}
aux<-rbind(c(0,0,0,0,1,1,1),c(1,0,1,0,0,1,1))
A<-sum(aux[1,])
B<-sum(aux[2,])
J<-sum(apply(aux, 2, sum)==2)
#euclideana
sqrt(A+B-2*J)
#manhathan
A+B-2*J
#bray
(A+B-2*J)/(A+B)
#binomial
log(2)*(A+B-2*J)
library(vegan)
vegdist(aux,binary = T)
vegdist(aux,binary = F)
#una base de datos mas grandes
set.seed(999)
aux1<-matrix(rbinom(200,1,0.4),nrow = 20)
vegdist(aux1,method = "binomial",binary = T)
dist(aux1)
```


```{r,eval=F}
# creando variables binarias (dummy)
library(fastDummies)
dummy_cols()
crime <- data.frame(city = c("SF", "SF", "NYC"),
    year = c(1990, 2000, 1990),
    crime = 1:3)
dummy_cols(crime, select_columns = c("city", "year"))
```


### Distancias para variables mixtas (cuantitativas, nominales, ordinales)

```{r,eval=F}
library(cluster)
data("flower")
str(flower)
dd<-daisy(flower,metric = "gower")
summary(dd)
```

Ejercicios

1. Busque funciones en R que permitan calcular los k-center para variables mixtas y medoides 
2. Crear una función k-center para variables mixtas y alternativas para incluir el medoide.

## Cluster Jerárquico

El objetivo es obtener una jerarquía de posibles soluciones que van desde un solo grupo a $n$ grupos, donde $n$ es el número de observaciones en el conjunto de datos.

### Algoritmo (Johnson)

1. Se inicia con $n$ grupos y se genera una matriz de $nxn$ de distancias, $D=\{d_{ik}\}$
2. Buscar en la matriz de distancia los pares de cluster más cercanos entre ellos, "los cluster mas similares", si definimos los clusters $V$ y $U$, estamos interesados en encontrar $d_{UV}$
3. Unir los cluster $U$ y $V$, re etiquetar el nuevo cluster como $UV$. Actualizar la matriz de distancias a) remover las filas y columnas correspondientes a $U$ y $V$ b) incluimos las nuevas filas y columnas para el nuevo cluster $UV$.
4. Repetimos el paso 2 y 3 un total de $n-1$ veces.

El momento de definir el cluster más cercano, se puede emplear los siguientes enlaces:

* Single linkage (Enlace simple): Se elige al cluster más cercano, con la regla de que las distincia individual entre las observaciones dentro de los clusters es la más corta 
* Complete linkage (Enlace completo): Se elige el cluster mas cercano, con la regla que las distancias individuales entre las observaciones dentro de los cluster es la más larga
* Average linkage (Enlace promedio): Se elige el cluster mas cercano, considerando el promedio de las distancias entre los cluster.

> Nota: Se debe elegir la matriz de distancias acorde a la naturaleza de los datos, se recomienda:

  * Todas Numéricas: Euclideana o Manhatan
  * Todas nominales: Transformación a binarias y usar la distancia binomial
  * Mixtas: Distancia de Gower


```{r,eval=F}
aux2<-matrix(rbinom(200,1,0.4),nrow = 20)
md<-vegdist(aux2,binary = T)
mod1<-hclust(md,method = "single")
mod2<-hclust(md,method = "complete")
mod3<-hclust(md,method = "average")
#dendograma
plot(mod1,main="single")
plot(mod2,main="complete")
plot(mod3,main="average")

plot(mod1,main="single",hang = -0.1,cex=0.8)
amod1<-cutree(mod1,5)
amod2<-cutree(mod2,5)
amod3<-cutree(mod3,5)

plot(mod1,main="single",hang = -0.1,cex=0.8)
rect.hclust(mod1,3)

plot(mod2,main="complete",hang = -0.1,cex=0.8)
rect.hclust(mod2,3)

plot(mod3,main="average",hang = -0.1,cex=0.8)
rect.hclust(mod3,4)

plot(silhouette(amod1,md))
plot(silhouette(amod2,md))
plot(silhouette(amod3,md))


d <- dist(scale(iris[,-5]))#euclideana
h <- hclust(d)
plot(h,hang=-0.1,labels=iris[["Species"]],cex=0.5)
#elegir la cantidad de grupos
clus3 <- cutree(h, 5)
plot(h,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(h,k=5)
```

Probando los tres tipos de enlaces para $k=3$

```{r,eval=F}
hs <- hclust(d,method = "single")
hc <- hclust(d,method = "complete")
ha <- hclust(d,method = "average")
cs<-cutree(hs,3)
cc<-cutree(hc,3)
ca<-cutree(ha,3)
table(cs,cc)
table(cs,ca)
table(cc,ca)
par(mfrow=c(1,3))
plot(hs,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(hs,k=3)
plot(hc,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(hc,k=3)
plot(ha,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(ha,k=3)
```

Nota: El dendograma es muy útil para ver las relaciones que existen basadas en las distancias y la creación de las jerarquias, a partir de estos se puede definir un $k$ (de forma visual)

Nota: El dendograma pierde su utilidad cuando la cantidad de observaciones es muy alta, 

Ejemplo, Usar los datos de las elecciones del 20 de octubre, agregar los resultados en términos relativos para los municipios y generar el dendograma para los tres tipos de enlaces, de forma visual sugerir un valor de $k$ para cada tipo de enlace.

```{r,eval=F}
library(dplyr)
load("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data\\oct20.RData")
#filtrar los casos
aux<-c("Número departamento","Departamento" ,"Número municipio","Municipio"   ,"CC","FPV","MTS","UCS","MAS - IPSP","21F","PDC","MNR","PAN-BOL","Votos Válidos","Blancos","Nulos")               
names(computo)[1]<-"pais"
names(computo)[12]<-"eleccion"
bd<-computo %>% filter(pais=="Bolivia" & eleccion=="Presidente y Vicepresidente") %>% select(aux)
names(bd)[1:4]<-c("idep","ddep","imun","dmun")
bdmun<-aggregate(bd[,5:16],bd[,1:4],sum)
bdmun<-bdmun[,-14]
bdmun[,5:15]<-prop.table(as.matrix(bdmun[,5:15]),1)
#cluster jerarquico
d<-dist(bdmun[,5:15])
plot(hclust(d),hang=-0.1,label=bdmun$dmun,cex=0.5)
# determinar el mejor k y el mejor enlace
mm<-c("single", "complete", "average") # método
k<-2:20 # cantidad de cluster
d<-dist(bdmun[,5:15]) # matriz de distancia
# matriz de resultados
res<-matrix(NA, nrow = 19,ncol=3)
colnames(res)<-mm
rownames(res)<-k
#######################
for(i in k){
  for(j in 1:3){
    h<-hclust(d,method = mm[j])
    c<-cutree(h,i)
    s<-silhouette(c,d)
    res[i-1,j]<-median(s[,3])
  }
}
#la mejor opción es k=2 con el método average
h<-hclust(d,method = "average")
c<-cutree(h,2)
plot(h,hang=-0.1,labels=bdmun$dmun,cex=0.4)
rect.hclust(h,k=2)
bdmun$cluster<-c
group_by(bdmun,cluster) %>% summarise(mean(CC),mean(`MAS - IPSP`))
```

Algunas alternativas para la visualización son:

```{r,eval=F}
library(ape)
h$labels<-bdmun$dmun
plot(as.phylo(h),type="fan")


library(dendextend)
library(circlize)

dend <- as.dendrogram(h)

# modify the dendrogram to have some colors in the branches and labels
dend <- dend %>% 
   color_branches(k=4) %>% 
   color_labels

# plot the radial plot
png("dendo.png",width = 1500,height = 1500)
par(mar = rep(0,4))
# circlize_dendrogram(dend, ) 
circlize_dendrogram(dend,dend_track_height = 0.8) 
dev.off()
```

![](dendo.png)

## Ejercicios
1. Pensar en un gráfico que permita ver como se asignaron los cluster 
2. Pensar en optimizar el código empleado para el k-center
3. Hacer que la función desarrollada para el k-center retorne también los centroides
4. Utilizando la base de datos de las elecciones del 20 de octubre, crear una base de datos a nivel municipal, aplicar el método k-center con medoides para los resultados a nivel municipal y terminar el $k$ óptimo en un rango de $k=2:10$ (Usar datos relativos).