--- 
title: "Minería de datos con R"
subtitle: "EST-384"
author: "Alvaro Chirino Gutierrez"
description: "Este libro esta destinado a la materia de Programación Estadística II de la carrera de Estadística de la Universidad Mayor de San Andres."
date: "`r Sys.Date()`"
delete_merged_file: true
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
graphics: yes
github-repo: alvarolimber/est-384
bibliography: book.bib
nocite: '@*'
pandoc_args: ["-Fpandoc-crossref"]
---

# Prefacio {-}

Placeholder


## Audiencia  {-}
## Estructura del libro  {-}
## Software y acuerdos {-}
## Datos {-}
## Agradecimiento  {-}
## Bibliografía {-}

<!--chapter:end:index.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Minería de Datos

## Motivación para la Minería de datos

* Los métodos de recolección de datos han evolucionado muy rápidamente.
* Las bases de datos han crecido exponencialmente
* Estos datos contienen información útil para las empresas, países, etc..
* El tamaño hace que la inspección manual sea casi imposible
* Se requieren métodos de análisis de datos automáticos para optimizar el uso de estos enormes conjuntos de datos

## ¿Qué es la minería de datos?

Es el análisis de **conjuntos de datos** (a menudo grandes) para encontrar **relaciones insospechadas** (conocimiento) y resumir los datos de **formas novedosas** que sean **comprensibles y útiles** para el propietario/usuario de los datos.

<p style='text-align: right;'> Principles of Data Mining (Hand et.al. 2001) </p>

## Datos y conocimiento

### Datos:

* se refieren a instancias únicas y primitivas (single objetos, personas, eventos, puntos en el tiempo, etc.)
* describir propiedades individuales
* a menudo son fáciles de recolectar u obtener (por ejemplo, cajeros de escáner, internet, etc.)
* no nos permiten hacer predicciones o pronósticos

### Conocimiento:

* se refiere a clases de instancias (conjuntos de ...)
* describe patrones generales, estructuras, leyes,
* consta de la menor cantidad de declaraciones posibles
* a menudo es difícil y lleva mucho tiempo encontrar u obtener
* nos permite hacer predicciones y pronósticos

## Requerimientos

* Disponibilidad para aprender 
* Mucha paciencia
    - Interactúa con otras áreas
    - Preprocesamiento de datos 
* Creatividad
* Rigor, prueba y error

## knowledge discovery in databases (KDD)

![](images/f1.png)

## Preparación de los datos

### Recopilación

* Instituto de Estadística 
* UDAPE, ASFI 
* Ministerio Salud (SNIS), Ministerio de educación (SIE)
* APIs, Twitter, Facebook, etc.
* Kaggle
* Banco Mundial, UNICEF, FAO, BID (Open Data)

### Data Warehouse

![](images/dataware.png)

### Data Warehouse in R

![](images/rdata.png)

### Importación

```{r, echo=T,eval=F}
library(foreign)
library(readr)
apropos("read")
getwd()
setwd("C:\\Users\\ALVARO\\Downloads\\bd49 (1)\\Base EH2019")
dir()
eh19v<-read.spss("EH2019_Vivienda.sav",to.data.frame = T)
eh19p<-read.spss("EH2019_Persona.sav",to.data.frame = T)
object.size(eh19p)/10^6
#exportación de datos
setwd("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data")
save(eh19p,eh19v,file="eh19.RData")
# cargando la base de datos que acabamos de guardar
rm(list=ls())
load("eh19.RData")
load("oct20.RData")
# cargando desde github
rm(list=ls())
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/oct20.RData"))
```

### Recopilación

```{r,echo=T,eval=F}
read.table("clipboard",header = T)
library(readxl) # excel 
library(DBI)  # Bases de datos relacionales en el sistema
#library(help=DBI)  
library(RMySQL) # bases de datos en mysql
# web scraping (API)
library(gtrendsR) # API
gg<-gtrends(c("data mining","machine learning"),time="today 12-m")
gg$interest_over_time
plot(gg)
gg<-gtrends(c("data mining","machine learning"),time="today 12-m",geo="BO")
```

### Limpieza

```{r,echo=T}
std<-data.frame(name=c("ana","juan","carla"),math=c(86,43,80),stat=c(90,75,82))
std
```

```{r,echo=T}
library(tidyr)
bd<-gather(std,materia,nota,math:stat)
bd
# otra opción más relacionada a bases de datos con información de tiempo, 
# es el comando reshape
```

### Ejercicio (reshape)

http://www.udape.gob.bo/portales_html/dossierweb2019/htms/CAP07/C070311.xls

###  Limpieza (fechas)

```{r,echo=T}
library(lubridate)
date()
today()
ymd("20151021")
ymd("2015/11/30")
myd("11.2012.3")
dmy_hms("2/12/2013 14:05:01")
mdy("120112")
d1<-dmy("15032020")
class(d1)
#ts()
```

### Limpieza (String)

```{r,echo=TRUE}

toupper("hola")
tolower("HOLA")

abc<-letters[1:10]
toupper(abc)

tolower("Juan")
# Extraer partes de un texto
substr("hola como estan",1,3)
substr("hola como estan",3,7)

# contar la cantidad de caracteres
nchar("hola")

nchar(c("hola","chau","LA                         paz"))

x<-c("LA-.paz","La Paz", "La pas", "La    paz","lapaz","la 78 paz")

x<-toupper(x)

x<-gsub("PAS","PAZ",x)

library(tm)
x<-removeNumbers(x)
x<-removePunctuation(x)
x<-gsub("LAPAZ","LA PAZ",x)
x<-stripWhitespace(x)

nchar(x)

nchar(gsub("  "," ",x))

gsub("a","x","hola como estas")
grepl("a",c("hola","como"))
grepl("o",c("hola","como"))

#otra alternativa
x<-c("LA-.paz","La Paz", "La pas", "La    paz","lapaz","la 78 paz")
x<-toupper(x)
x[grepl("PAZ",x)]<-"LA PAZ"
x<-gsub("PAS","PAZ",x)

# para llevar a ascii
utf8ToInt("la paz")
utf8ToInt("@")

library(stringi)
```

Ejemplo de web scraping sobre la página https://www.worldometers.info/

```{r}
library(rvest)
url<-"https://www.worldometers.info/coronavirus/"
covid<-read_html(url)
bdcov<-html_table(covid)
bdnow<-bdcov[[1]]
str(bdnow)
```

Tarea: limpiar la base de datos

  * Convertir las variables necesarias a numéricas
  * Debe ser una base de solo países
  
### Transformación

```{r}
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))
names(eh19p)
```

* Estandarizar variables

```{r}
summary(eh19p$s02a_03) #edad
summary(eh19p$tothrs) # total de horas de trabajo semanal
summary(eh19p$ylab)  # ingreso laboral mensual

sd(eh19p$s02a_03)
sd(eh19p$tothrs,na.rm = T)
sd(eh19p$ylab,na.rm = T)

x1<-scale(eh19p$s02a_03)
x2<-scale(eh19p$tothrs)
x3<-scale(eh19p$ylab)

sd(x1);sd(x2,na.rm = T);sd(x3,na.rm = T)
par(mfrow=c(2,3))
boxplot(eh19p$s02a_03,ylim=c(0,25000))
boxplot(eh19p$tothrs,ylim=c(0,25000))
boxplot(eh19p$ylab,ylim=c(0,25000))

boxplot(x1,ylim=c(-3,3))
boxplot(x2,ylim=c(-3,3))
boxplot(x3,ylim=c(-3,3))

par(mfrow=c(2,3))
plot(density(eh19p$s02a_03))
plot(density(eh19p$tothrs,na.rm=T))
plot(density(eh19p$ylab,na.rm=T))

plot(density(x1))
plot(density(x2,na.rm=T))
plot(density(x3,na.rm=T))

mean(eh19p$ylab,na.rm=T)
median(eh19p$ylab,na.rm=T)
```


* Función logarítmo

```{r}
dev.off()
curve(log,xlim=c(10,30000))

x1<-log(eh19p$s02a_03)
x2<-log(eh19p$tothrs)
x3<-log(eh19p$ylab)

par(mfrow=c(2,3))
plot(density(eh19p$s02a_03))
plot(density(eh19p$tothrs,na.rm=T))
plot(density(eh19p$ylab,na.rm=T))

plot(density(x1))
plot(density(x2,na.rm=T))
plot(density(x3,na.rm=T))
```


* Creación de variables



* Recodificar variables



## Imputación de variables 

**We should be suspicious of any dataset (large or small) which appears perfect.**

— David J. Hand

### La falta de información es información

* MCAR missing completely at random 
* MAR missing at random 
* MNAR missing not at random 

### Aproximación formal
Sea $Y$ una matriz de datos con $n$ observaciones y $p$ variables. Sea $R$ una matriz de respuesta binaria, tal que si $y_{ij}$ es observada, entonces $r_{ij}=1$.

Los valores observados son colectados en $Y_{obs}$, las observaciones perdidas en $Y_{mis}$. Así, $Y=(Y_{obs},Y_{mis})$.

La distribución de $R$ depende de $Y=(Y_{obs},Y_{mis})$. Sea $\psi$ que contiene los parametros del modelos de los datos perdidos, asi la expresion del modelo de los datos perdidos es $\Pr(R|Y_\mathrm{obs},Y_\mathrm{mis},\psi)$

### MCAR, MAR, MNAR

MCAR (missing completely at random )
$$
\Pr(R=0|{\mbox{$Y_\mathrm{obs}$}},{\mbox{$Y_\mathrm{mis}$}},\psi) = \Pr(R=0|\psi)
$$

MAR (missing at random )
$$
\Pr(R=0|{\mbox{$Y_\mathrm{obs}$}},{\mbox{$Y_\mathrm{mis}$}},\psi) = \Pr(R=0|{\mbox{$Y_\mathrm{obs}$}},\psi) 
$$

MNAR (missing not at random )
$$
\Pr(R=0|{\mbox{$Y_\mathrm{obs}$}},{\mbox{$Y_\mathrm{mis}$}},\psi)
$$

### Alternativas para trabajar con los Missings (Ad-hoc)

* Listwise deletion 
* Pairwise deletion
* Mean imputation
* Regression imputation
* Stochastic regression imputation
* Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) 
* Indicator method

### Imputación Multiple

![](images/impu.png){width=800px}


### Patrones en datos multivariados

![](images/impu2.png){width=800px}



### Influx and outflux

$$
I_j = \frac{\sum_j^p\sum_k^p\sum_i^n (1-r_{ij})r_{ik}}{\sum_k^p\sum_i^n r_{ik}}
$$

La variable con mayor influx está mejor conectada a los datos observados y, por lo tanto, podría ser más fácil de imputar.

$$
O_j = \frac{\sum_j^p\sum_k^p\sum_i^n r_{ij}(1-r_{ik})}{\sum_k^p\sum_i^n 1-r_{ij}}
$$
La variable con mayor outflux está mejor conectada a los datos faltantes, por lo tanto, es potencialmente más útil para imputar otras variables.

### Imputación de datos monótonos

![](images/algoritmo1.png)


### Multivariate Imputation by Chained Equations (mice)
(Imputación multivariante por ecuaciones encadenadas)

![](images/algoritmo1.png)

<!--chapter:end:1_introMD.Rmd-->


# Modelado en Minería de datos

Placeholder


## Explorando los datos
### Resumen de los datos
### Visualización
## Componentes Principales
## Análisis de correspondencia

<!--chapter:end:2_modelado.Rmd-->


# Clustering

Placeholder


## Medidas de Disimilaridad
### Distancia Euclideana: Variables numéricas
### Distancia Manhattan: $p$ grande
### Distancia Minkowski 
### programando
### Variables cualitativas
### Datos mixtos 
## Métodos de clustering
## K-center Clustering (no jerárquicos)
### Validación cluster
### Distancias para variables nominales (todas nominales)
### Distancias para variables mixtas (cuantitativas, nominales, ordinales)
## Cluster Jerárquico
### Algoritmo (Johnson)
## Ejercicios

<!--chapter:end:3_clustering.Rmd-->


# Regresión

Placeholder


## Regresión lineal
### Paso 1: Base de datos
### Paso 2: Establecer la relación de interés.
### Paso 3: Definir el modelo a utilizar
### Paso 4: Optimizar el modelo
### Paso 5: Validar el modelo
### Paso 6: Predicir a partir del modelo
## Probit y Logit
### En R:

<!--chapter:end:4_regresion.Rmd-->


# Clasificación

Placeholder


## Logit Probit
### Pasos
## Arboles de clasificación (CART)
### Pasos
### Proceso de pruning (podado) 
## Naive Bayes
## Ejercicios.

<!--chapter:end:5_clasificacion.Rmd-->


# Minería de Texto

Placeholder


## Introducción
### En la práctica
### ¿Por qué importa?
### Las consecuencias de ignorarlo
### Beneficios
### Flujo de trabajo en la minería de texto
### Librerías en R para texto
## Recolección de texto
### CSV
### Colección de documentos
### Twitter (API)
## Nubes de palabras 
## Análisis de sentimiento (sentimental scoring)
### Polarización y léxico subjetivo
### Polarización en QDAP
### Librería syuzhet

<!--chapter:end:6_mineriatexto.Rmd-->

