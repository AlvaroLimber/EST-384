table(sample(c(1,2),10,replace = T))
table(sample(c(1,2),10,replace = T))
table(sample(c(1,2),10,replace = T))
table(sample(c(1,2),10,replace = T))
table(sample(c(1,2),10,replace = T))
table(sample(c(1,2),10,replace = T))
table(sample(c(1,2),10,replace = T))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
table(sample(c(1,2),10,replace = T))
table(sample(c(1,2),10,replace = T))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
sample(c(1,2),10,replace = T,prob = c(0.7,0.3))
index = sample(1:2, nrow(covid), replace = TRUE, prob=c(0.7, 0.3))
index
table(index)
prop.table(table(index))
index
trainbd<-covid %>% filter(index==1)
testbd<-covid %>% filter(index==2)
table(index)
m1<-glm(muerte~.,data=trainbd,family = binomial(link="logit"))
summary(m1)
step(m1)
m2<-step(m1)
summary(m2)
?step
summary(m2)
m2
predict(m2,testbd,type="response")
hist(predict(m2,testbd,type="response"))
hist(predict(m2,testbd,type="response"))
predict(m2,testbd,type="response")
predict(m2,testbd,type="response")>0.5
clase<-predict(m2,testbd,type="response")>0.5
table(clase)
prop.table(table(clase))
table(clase)
table(testbd$muerte,clase)
library(caret)
confusionMatrix(table(testbd$muerte,clase))
t1<-table(testbd$muerte,clase)
t1
n(t1)
sum(t1)
diag(t1)
suma(diag(t1))/sum(t1)
sum(diag(t1))/sum(t1)
t1
library(mfx)
info<-logitmfx(formula(m2),data=testbd)
info
barplot(info$mfxest[,1],horiz = T,las=1,cex.names = 0.5,xlim=c(-0.1,0.1),pos=-0.02)
barplot(info$mfxest[,1],horiz = T,las=1,cex.names = 0.5,xlim=c(-0.1,0.1),pos=-0.02)
m1
m2
info
info$mfxest[,1]
0.000906721*50
0.000906721*80
0.000906721*80+info$mfxest[2,1]
0.000906721*80
clase
testbd$muerte
table(testbd$muerte,clase)
confusionMatrix(table(testbd$muerte,clase))
library(rpart)
ininstall.packages("rpart")
install.packages("rpart")
install.packages("rpart")
library(rpart)
library(dplyr)
covid<-read.csv("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data\\covid_mx\\200627COVID19MEXICO.csv",sep=",",na.strings = c(99,98))
vv<-c("SEXO","FECHA_DEF","NEUMONIA","EDAD","HABLA_LENGUA_INDIG","DIABETES","EPOC","ASMA","INMUSUPR","HIPERTENSION","OTRA_COM","CARDIOVASCULAR","OBESIDAD","RENAL_CRONICA","TABAQUISMO","RESULTADO")
covid<-covid %>% select(vv)
#variable muerte
covid<-covid %>% mutate(muerte=(FECHA_DEF!="9999-99-99")) %>% select(-FECHA_DEF)
# valores perdidos
covid<-na.omit(covid)
##################################################
#a factor (X)
##################################################
#sexo
covid$SEXO<-factor(covid$SEXO,levels=1:2,labels=c("Mujer","Hombre"))
#resultado
covid$RESULTADO<-factor(covid$RESULTADO,levels = 1:3,labels=c("COVID +","COVID -","COVID pendiende"))
covid2<-covid#para cart
covid2$muerte<-factor(covid2$muerte,c(T,F),labels = c("Muerte","No muerte"))
#si/no
aux<-c("NEUMONIA","HABLA_LENGUA_INDIG","DIABETES","EPOC","ASMA","INMUSUPR","HIPERTENSION","OTRA_COM","CARDIOVASCULAR","OBESIDAD","RENAL_CRONICA","TABAQUISMO")
for(i in aux){
covid2[[i]]<-factor(covid2[[i]],1:2,c("SI","NO"))
}
str(covid2)
## Bases: trainbd, testbd
set.seed(123)
index = sample(1:2, nrow(covid2), replace = TRUE, prob=c(0.7, 0.3))
## Bases: trainbd, testbd
set.seed(123)
index = sample(1:2, nrow(covid2), replace = TRUE, prob=c(0.7, 0.3))
prop.table(table(index))
names(trainbd)
trainbd<-covid2[index==1,]
testbd<-covid2[index==2,]
names(trainbd)
mod1<-rpart(muerte~.,data=trainbd)
mod1
printcp(mod1)
plotcp(mod1)
summary(mod1)
plot(mod1)
plot(mod1)
text(mod1,all=T,use.n=T)
plot(mod1)
text(mod1,all=T,use.n=T)
#install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(mod1)
plot(mod1)
text(mod1,all=T,use.n=T)
rpart.plot(mod1)
table(trainbd$muerte)
prop.table(table(trainbd$muerte))
plot(mod1,uniform=T, branch=1, margin=0.1)
text(mod1,all=T,use.n=T)
rpart.plot(mod1)
mod1
clase<-predict(mod1,testbd,type = "class")
clase<-predict(mod1,testbd,type = "class")
table(clase,testbd$muerte)
library(caret)
confusionMatrix(table(clase,testbd$muerte))
table(clase,testbd$muerte)
mod1
mod1$cptable
which.min(mod1$cptable[,"xerror"])
mod1.cp<-mod1$cptable[2,"CP"]
mod1.cp
mod1.prune<-prune(mod1,cp=mod1.cp)
mod1.prune
rpart.plot(mod1.prune)
mod1.cp<-mod1$cptable[1,"CP"]
mod1.prune<-prune(mod1,cp=mod1.cp)
rpart.plot(mod1.prune)
mod1.cp<-mod1$cptable[2,"CP"]
mod1.prune<-prune(mod1,cp=mod1.cp)
rpart.plot(mod1.prune)
rpart.plot(mod1.prune)
mod1$cptable
?prune
z.auto <- rpart(Mileage ~ Weight, car.test.frame)
rpart.plot(z.auto)
z.auto$cptable
zp <- prune(z.auto, cp = 0.1)
rpart.plot(zp)
car.test.frame
zp <- prune(z.auto, cp = 0.1)
rpart.plot(zp)
rpart.plot(z.auto)
zp <- prune(z.auto, cp = 0.1)
rpart.plot(zp)
rpart.plot(z.auto)
rpart.plot(zp)
mod1$cptable
clase<-predict(mod1.prune,testbd,type = "class")
confusionMatrix(table(clase,testbd$muerte))
mod1<-rpart(RESULTADO~.,data=trainbd)
mod1
rpart.plot(mod1)
rpart.plot(mod1)
rpart.plot(mod1.prune)
library(e1071)
library(help=e1071)
?svm
mod1<-naiveBayes(muerte~.,data=trainbd)
mod1
clase<-predict(mod1,testbd,type = "class")
clase
confusionMatrix(table(clase,testbd$muerte))
x<-"Hola a todos, ¿Cómo estan?. hoy es 30 de Noviembre"
x<-"Hola a todos, ¿Cómo estan?.    hoy es 30 de Noviembre"
x
nchar(x)
paste("Pregunta 1:",x,"Fin de la oración")
paste0("Pregunta 1:",x,"Fin de la oración")
gsub("Hola","xxx",x)
grep("Hola",x)
y<-c("Hola a todos","chau")
y
grep("Hola",y)
y<-c("Hola a todos","chau","Hola")
grep("Hola",y)
y
grep("Hola",y)
tolower(x)
toupper(x)
library(tm)
library(tm)
removePunctuation(x)
removePunctuation(x)
?removePunctuation(x)
removePunctuation(x)
stripWhitespace(x)
removeNumbers(x)
?removeWords()
removeWords(x,c("¿"))
removeWords(x,c("¿"))
removeWords(x,c("Hola"))
removeWords(x,c("Hola","?"))
removeWords(x,c("Hola"))
removeWords(x,c("Hola","hoy"))
stopwords()
stopwords("es")
removeWords(x,stopwords("es"))
x
removeWords(x,stopwords("es"))
removeWords(x,stopwords("es")[-47])
library(tm)
library(dplyr)
setwd("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data")
fb<-read.csv("bd_sc.csv")
View(fb)
fb<-read.csv("bd_sc.csv",encoding = "UTF-8")
View(fb)
#fb<-read.csv("bd_sc.csv",encoding = "Latin-1")
fb$post_text[5]
library(pdftools)
dir<-"C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data\\pdf"
pdfdocs <- VCorpus(DirSource(dir, pattern = ".pdf"),
readerControl = list(reader = readPDF))
View(pdfdocs)
pdfdocs[["Cuestionario EH- 2019.pdf"]][["content"]]
library(rtweet)
library(rtweet)
?search_tweets
tw<-search_tweets("Bolivia",n=100,include_rts = F)
View(tw)
tw$text
library(twetteR)
library(twitteR)
library(help=twitteR)
?searchTwitter
stopwords()
stopwords("es")
aux<-stopwords("es")
aux
library(syuzhet)
ww<-get_sentiment_dictionary("nrc",language = "spanish")
ww
library(rtweet)
library(tm)
library(dplyr)
setwd("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data")
fb<-read.csv("bd_sc.csv")
fb<-read.csv("bd_sc.csv",encoding = "UTF-8")
#fb<-read.csv("bd_sc.csv",encoding = "Latin-1")
fb$post_text[5]
library(pdftools)
dir<-"C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data\\pdf"
pdfdocs <- VCorpus(DirSource(dir, pattern = ".pdf"),
readerControl = list(reader = readPDF))
library(rtweet)
tw<-search_tweets("Bolivia",n=500,include_rts = F)
library(wordcloud2)
##funciones
#vectores de texto
nube<-function(aux){
docs<-Corpus(VectorSource(aux))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("sp"))
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
return(df)
}
#objetos corpus
nube2<-function(aux){
docs <- aux %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("sp"))
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
return(df)
}
#csv
df<-nube(fb$post_text)
wordcloud2(data=df,color='random-dark',size=0.5,shape = 'pentagon')
#colección de documentos
df<-nube2(pdfdocs)
wordcloud2(data=df,color='random-dark',size = 0.3,shape = 'pentagon')
#scrape
df<-nube(tw$text)
wordcloud2(data=df[df$freq>1,],color='random-dark',shape = 'pentagon')
wordcloud2(data=df[df$freq>1,],color='random-dark',size = 0.4,shape = 'pentagon')
wordcloud2(data=df[df$freq>1,],color='random-dark',size = 1,shape = 'pentagon')
head(df)
wordcloud2(data=df[-1,],color='random-dark',size = 1,shape = 'pentagon')
wordcloud2(data=df[-1,],color='random-dark',size = 0.4,shape = 'pentagon')
library(ggplot2)
library(ggthemes)
docs<-VCorpus(VectorSource(fb$text))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("sp"))
tdm<-TermDocumentMatrix(docs)
associations<-findAssocs(tdm, 'evo', 0.55)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
associations
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
wordcloud2(data=df,color='random-dark',size=0.5,shape = 'pentagon')
#csv
df<-nube(fb$post_text)
wordcloud2(data=df,color='random-dark',size=0.5,shape = 'pentagon')
docs<-VCorpus(VectorSource(fb$text))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("sp"))
tdm<-TermDocumentMatrix(docs)
associations<-findAssocs(tdm, 'domicilio', 0.55)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
associations<-findAssocs(tdm, 'domicilio', 0.4)
View(associations)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
#solo ingles
library(qdap)
library(rtweet)
tw<-search_tweets("Bolivia",n=100,include_rts = F,lang="en")
detach(package:dplyr, unload=TRUE)
detach(package:rtweet, unload=TRUE)
detach(package:qdap, unload=TRUE)
`[[.qdap_hash` <- `[[.data.frame`
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
#solo ingles
library(qdap)
score<-polarity(tw$text[1:2])
#solo ingles
library(qdap)
library(rtweet)
detach(package:dplyr, unload=TRUE)
detach(package:rtweet, unload=TRUE)
detach(package:qdap, unload=TRUE)
`[[.qdap_hash` <- `[[.data.frame`
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
#solo ingles
library(qdap)
score<-polarity(tw$text[1:2])
detach(package:qdap, unload=TRUE)
score<-polarity(tw$text[1:2])
#solo ingles
library(qdap)
library(rtweet)
tw<-search_tweets("Bolivia",n=100,include_rts = F,lang="en")
detach(package:dplyr, unload=TRUE)
detach(package:rtweet, unload=TRUE)
`[[.qdap_hash` <- `[[.data.frame`
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
?polarity
with(DATA, polarity(state, list(sex, adult)))
#solo ingles
library(dplyr)
#solo ingles
library(dplyr)
library(qdap)
library(rtweet)
detach(package:rtweet, unload=TRUE)
`[[.qdap_hash` <- `[[.data.frame`
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
with(DATA, polarity(state, list(sex, adult)))
(poldat <- with(sentSplit(DATA, 4), polarity(state, person)))
detach(package:dplyr, unload=TRUE)
detach(package:rtweet, unload=TRUE)
detach(package:qdap, unload=TRUE)
`[[.qdap_hash` <- `[[.data.frame`
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
library(qdap)
score<-polarity(tw$text[1:2])
`[[.qdap_hash` <- `[[.data.frame`
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
library(syuzhet)
library(help=syuzhet)
tw<-search_tweets("coronavirus",n=1000,include_rts = F,lang="es")
library(rtweet)
tw<-search_tweets("coronavirus",n=1000,include_rts = F,lang="es")
aa<-get_nrc_sentiment(tw$text,language = "spanish")
head(aa)
tw$text[1]
tw$text[2]
barplot(apply(aa,2,sum),horiz = T,las=1)
barplot(apply(aa,2,sum),horiz = T,las=1)
tw<-search_tweets("Bolivia",n=1000,include_rts = F,lang="es")
aa<-get_nrc_sentiment(tw$text,language = "spanish")
barplot(apply(aa,2,sum),horiz = T,las=1)
#ampliar el léxico
ww<-get_sentiment_dictionary("nrc",language = "spanish")
View(ww)
head(ww)
ww$value
table(ww$value)
head(ww)
ww<-rbind(ww,c("spanish","xxxx","negative","1"))
tail(ww)
get_nrc_sentiment
mean()
mean
sum
search_tweets
library(xtable)
xtable
get_nrc_sentiment
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
install.packages(bookdown)
install.packages("bookdown")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
install.packages("DMwR2")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
