label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
associations<-findAssocs(tdm, 'jeanine', 0.70)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
tdm<-TermDocumentMatrix(docs)
associations<-findAssocs(tdm, 'evo', 0.70)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
associations<-findAssocs(tdm, 'evo', 0.40)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
associations<-findAssocs(tdm, 'evo', 0.50)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
associations<-findAssocs(tdm, 'evo', 0.55)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
View(dtm)
getwd()
install.packages("syuzhet")
library(syuzhet)
library(help=syuzhet)
install.packages("syuzhet")
library(syuzhet)
library(help=syuzhet)
library(rtweet)
?search_tweets
tw<-search_tweets("coronavirus",n=1000,include_rts = F,geocode ="BOL" ,lang="es")
tw<-search_tweets("coronavirus",n=1000,include_rts = F,geocode ="BOL" ,lang="sp")
tw<-search_tweets("coronavirus",n=1000,include_rts = F,geocode ="BO" ,lang="sp")
geocode
tw<-search_tweets("coronavirus",n=1000,include_rts = F,lang="sp")
tw<-search_tweets("coronavirus",n=1000,include_rts = F,lang="es")
tw$text
get_nrc_sentiment(tw$text[1:5])
get_nrc_sentiment(tw$text[1:5],language = "spanish" )
rm(list=ls())
library(syuzhet)
setwd("/Library/Frameworks/R.framework/Versions/3.5/Resources/library/syuzhet/R")
?get_nrc_sentiment()
get_nrc_sentiment()
get_nrc_sentiment
nrc
get_nrc_values()
get_nrc_values
#ampliando el lexico
nrc_lexicon <- get_sentiments("nrc")
#install.packages("syuzhet")
library(syuzhet)
#ampliando el lexico
nrc_lexicon <- get_sentiments("nrc")
#ampliando el lexico
nrc_lexicon <- get_sentiment("nrc")
library(help=syuzhet)
get_sentiment_dictionary("nrc")
?get_sentiment_dictionary("nrc")
get_sentiment_dictionary("nrc",language = "spanish")
get_sent_values
?get_sent_values
get_stanford_sentiment
?get_stanford_sentiment
?get_nrc_values
get_nrc_sentiment(tw$text[1:5],language = "spanish")
tw<-search_tweets("coronavirus",n=1000,include_rts = F,lang="es")
get_nrc_values(tw$text, language = "spanish", lexicon = NULL)
get_nrc_values(tw$text[1:5], language = "spanish", lexicon = NULL)
get_nrc_values(tw$text[1:5], language = "spanish", lexicon = "nrc")
get_nrc_sentiment(tw$text[1:5],language = "spanish")
ww<-get_sentiment_dictionary("nrc",language = "spanish")
get_nrc_values(tw$text[1:5], language = "spanish", lexicon = ww)
ww
tail(ww)
get_nrc_values(tw$text[1:5], lexicon = ww[,2:3])
get_nrc_values(tw$text[1:5], lexicon = ww[,3:4])
get_nrc_values(tw$text[1:5],language = "spanish")
aa<-get_nrc_values(tw$text[1:5],language = "spanish")
get_nrc_values(tw$text[1:5],language = "spanish",lexicon = ww[,2:4])
get_nrc_values(tw$text[1:5],lexicon = ww[,2:4])
ww
?simple_plot()
simpleplot(get_nrc_sentiment(tw$text[1:5],language = "spanish"))
simple_plot(get_nrc_sentiment(tw$text[1:5],language = "spanish"))
get_nrc_values(tw$text[1:5],language = "spanish")
get_nrc_values(aa)
library(qdap)
library(help=qdap)
library(qdap)
key.pol
?key.pol
names(key.pol)
?polarity()
with(DATA, polarity(state, list(sex, adult)))
(poldat <- with(sentSplit(DATA, 4), polarity(state, person)))
counts(poldat)
scores(poldat)
plot(poldat)
poldat2 <- with(mraja1spl, polarity(dialogue,
list(sex, fam.aff, died)))
colsplit2df(scores(poldat2))
plot(poldat2)
plot(scores(poldat2))
cumulative(poldat2)
polarity('ROFL, look at that!')
plot(polarity('ROFL, look at that!'))
#solo ingles
library(qdap)
library(rtweet)
tw<-search_tweets("Bolivia",n=1000,include_rts = F,lang="en")
polarity(tw$text)
tw$text
score<-polarity(tw$text[1:10])
sentSplit
?sentSplit
df<-sentSplit(tw,text)
df<-sentSplit(tw$text)
score<-polarity(tw$text[1:5])
score<-polarity(tw$text[1])
tw$text
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1])
score<-polarity(tw$text[1:2])
score<-polarity(as.vector(tw$text[1:2]))
score<-polarity(list(tw$text[1:2]))
aux<-as.raw(tw$text)
score<-polarity(aux)
aux
score<-polarity(tw$text[2])
tw$text[1]
class(tw$text[1])
class(tw$text[1:""])
class(tw$text[1:2])
class(as.vector(tw$text[1:2]))
str(as.vector(tw$text[1:2]))
score<-polarity(as.vector(tw$text[1:2]))
score<-polarity("The neolib who declared herself president of Bolivia after the US backed coup late last year")
score
aux<-tw$text
score<-polarity(aux)
aux<-tw$text[1]
score<-polarity(aux)
aux<-tw$text[1,2]
score<-polarity(aux)
aux<-tw$text[1:2]
score<-polarity(aux)
aux<-c(tw$text[1:2])
score<-polarity(aux)
aux<-vector(tw$text[1:2])
aux<-as.vector(tw$text[1:2])
score<-polarity(aux)
class(tw)
tw<-as.data.frame(tw)
score<-polarity(tw$text)
str(tw)
class(tw)
tw$text<-removePunctuation(tw$text)
tw$text[1]
aux<-c("hi fuck you","is terrible")
polarity(aux)
aux<-c("hi fuck you","is terrible")
polarity(aux)
score<-qdap::polarity(tw$text)
aux<-c("hi fuck you","is terrible")
polarity(aux)
qdap::polarity(aux)
score<-polarity(tw$text)
?remove()
detach(rtweet)
unloadNamespace(rtweet)
detach(package:rtweet, unload=TRUE)
tw<-search_tweets("Bolivia",n=1000,include_rts = F,lang="en")
#solo ingles
library(qdap)
score<-polarity(tw$text)
score<-polarity(tw$text)
detach(package:qdap, unload=TRUE)
#solo ingles
library(qdap)
score<-polarity(tw$text)
`[[.qdap_hash` <- `[[.data.frame`
score<-polarity(tw$text)
library(openssl)
`[[.qdap_hash` <- `[[.data.frame`
score<-polarity(tw$text)
score<-polarity(tw$text[1:2])
`[[.qdap_hash` <- `[[.data.frame`
score<-polarity(tw$text[1:2])
#solo ingles
library(qdap)
`[[.qdap_hash` <- `[[.data.frame`
score<-polarity(tw$text[1:2])
score<-polarity(tw$text[1:2])
`[[.qdap_hash` <- `[[.data.frame`
score<-polarity(tw$text[1:2])
`[[.qdap_hash` <- `[[.data.frame`
score<-polarity(tw$text[1:2])
tw<-as.data.frame(tw)
`[[.qdap_hash` <- `[[.data.frame`
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
polarity("hi")
polarity(c("hi"))
polarity(c("hi","jo"))
polarity(tw$text)
score<-polarity(tw$text[1:2])
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
library(rtweet)
score<-polarity(tw$text[1:2])
detach(package:rtweet, unload=TRUE)
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
library(sentiment)
install.packages("sentiment")
install.packages("sentiment.")
library(sentiment)
install.packages("sentiment")
library(Rsentiment)
library(wordcloud)
?comparison.cloud()
if(require(tm)){
data(SOTU)
corp <- SOTU
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, function(x)removeWords(x,stopwords()))
term.matrix <- TermDocumentMatrix(corp)
term.matrix <- as.matrix(term.matrix)
colnames(term.matrix) <- c("SOTU 2010","SOTU 2011")
comparison.cloud(term.matrix,max.words=40,random.order=FALSE)
comparison.cloud(term.matrix,max.words=40,random.order=FALSE,
title.colors=c("red","blue"),title.bg.colors=c("grey40","grey70"))
comparison.cloud(term.matrix,max.words=40,random.order=FALSE,
match.colors=TRUE)
}
if(require(tm)){
data(SOTU)
corp <- SOTU
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, content_transformer(tolower))
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, function(x)removeWords(x,stopwords()))
term.matrix <- TermDocumentMatrix(corp)
term.matrix <- as.matrix(term.matrix)
colnames(term.matrix) <- c("SOTU 2010","SOTU 2011")
comparison.cloud(term.matrix,max.words=40,random.order=FALSE)
comparison.cloud(term.matrix,max.words=40,random.order=FALSE,
title.colors=c("red","blue"),title.bg.colors=c("grey40","grey70"))
comparison.cloud(term.matrix,max.words=40,random.order=FALSE,
match.colors=TRUE)
}
term.matrix
25*190
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
library(tm)
library(dplyr)
setwd("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data")
fb<-read.csv("bd_sc.csv")
fb<-read.csv("bd_sc.csv",encoding = "UTF-8")
library(ggplot2)
library(ggthemes)
?findAssocs
docs<-VCorpus(VectorSource(fb$text))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("sp"))
tdm<-TermDocumentMatrix(docs)
associations<-findAssocs(tdm, 'evo', 0.55)
associations<-as.data.frame(associations)
associations$terms<-row.names(associations)
associations$terms<-factor(associations$terms,
levels=associations$terms)
names(associations)[1]<-"palabra"
ggplot(associations, aes(y=terms)) +
geom_point(aes(x=palabra), data=associations,
size=5)+
theme_gdocs()+ geom_text(aes(x=palabra,
label=palabra),
colour="darkred",hjust=-.25,size=8)+
theme(text=element_text(size=20),
axis.title.y=element_blank())
View(associations)
#solo ingles
library(qdap)
library(rtweet)
tw<-search_tweets("Bolivia",n=100,include_rts = F,lang="en")
tw
score<-polarity(tw$text[1:2])
tw$text<-removePunctuation(tw$text)
score<-polarity(tw$text[1:2])
detach(package:rtweet, unload=TRUE)
detach(package:qdap, unload=TRUE)
detach(package:qdap, unload=TRUE)
`[[.qdap_hash` <- `[[.data.frame`
`[[.qdap_hash` <- `[[.data.frame`
score<-polarity(tw$text[1:2])
#solo ingles
library(qdap)
score<-polarity(tw$text[1:2])
tw$text[1]
polarity("LesleyAnnBrandt Period Love you both so much")
polarity("LesleyAnnBrandt Period Love you both so much")
detach(package:rtweet, unload=TRUE)
detach(package:dplyr, unload=TRUE)
detach(package:dplyr, unload=TRUE)
detach(package:dplyr, unload=TRUE)
polarity("LesleyAnnBrandt Period Love you both so much")
polarity("LesleyAnnBrandt")
polarity("LesleyAnnBrandt love")
library(syuzhet)
library(syuzhet)
library(rtweet)
tw<-search_tweets("coronavirus",n=1000,include_rts = F,lang="es")
tw$text[1:""]
tw$text[1:2]
tw$text[5:10]
ww<-get_sentiment_dictionary("nrc",language = "spanish")
View(ww)
aa<-get_nrc_sentiment(tw$text[1:5],language = "spanish")
aa
tw$text[5]
aa<-get_nrc_sentiment(tw$text,language = "spanish")
apply(aa,2,sum)
barplot(apply(aa,2,sum))
barplot(apply(aa,2,sum))
barplot(apply(aa,2,sum),horiz = T)
barplot(apply(aa,2,sum),horiz = T,las=1)
#ampliar el léxico
ww
table(ww$value)
#ampliar el léxico
rbind(ww,c("spanish","xxxx","negative","1")
#ampliar el léxico
rbind(ww,c("spanish","xxxx","negative","1"))
#ampliar el léxico
ww<-rbind(ww,c("spanish","xxxx","negative","1"))
tail(ww)
get_nrc_sentiment
lookup_users("evoespueblo")
a<-lookup_users("evoespueblo")
head(a)
tweets_data(a)
df<-tweets_data(a)
df
?tweets_data(a)
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
setwd("~/GitHub/EST-384/DataMining")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
70*28
library(foreign)
library(readr)
apropos("read")
getwd()
getwd()
setwd("C:\\Users\\ALVARO\\Downloads\\bd49 (1)\\Base EH2019")
dir()
?read.spss()
eh19p<-read.spss("EH2019_Vivienda.sav")
eh19p<-read.spss("EH2019_Vivienda.sav",to.data.frame = T)
eh19v<-read.spss("EH2019_Persona.sav",to.data.frame = T)
eh19v<-read.spss("EH2019_Vivienda.sav",to.data.frame = T)
eh19v<-read.spss("EH2019_Vivienda.sav",to.data.frame = T)
eh19p<-read.spss("EH2019_Persona.sav",to.data.frame = T)
object.size(eh19p)
object.size(eh19p)/10^6
library(foreign)
library(readr)
apropos("read")
getwd()
setwd("C:\\Users\\ALVARO\\Downloads\\bd49 (1)\\Base EH2019")
dir()
eh19v<-read.spss("EH2019_Vivienda.sav",to.data.frame = T)
eh19p<-read.spss("EH2019_Persona.sav",to.data.frame = T)
object.size(eh19p)/10^6
?load
?save()
#exportación de datos
setwd("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data")
setwd("~/GitHub/EST-384/data")
#exportación de datos
setwd("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data")
#exportación de datos
setwd("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data")
save(eh19p,eh19v,file="eh19.RData")
# cargando la base de datos que acabamos de guardar
rm(list=ls())
getwd()
load("eh19.RData")
load("oct20.RData")
# cargando la base de datos que acabamos de guardar
rm(list=ls())
# cargando desde github
rm(list=ls())
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/oct20.RData"))
library(readxl)
library(help=DBI)
# web scraping (API)
library(gtrendsR) # API
install.packages("gtrendsR")
install.packages("gtrendsR")
# web scraping (API)
library(gtrendsR) # API
# web scraping (API)
library(gtrendsR) # API
# web scraping (API)
library(help=gtrendsR) # API
?gtrends()
gg<-gtrends("data mining",time="today 12-m")
gg$interest_over_time
plot(gg)
gg<-gtrends(c("data mining","machine learning"),time="today 12-m")
gg$interest_over_time
plot(gg)
countries
countries[,countries$name=="BOLIVIA"]
View(countries)
countries[,countries$country_code=="BOLIVIA"]
countries[,countries$country_code=="BO"]
countries[countries$country_code=="BO",]
gg<-gtrends(c("data mining","machine learning"),time="today 12-m",geo="BO")
plot(gg)
View(gg)
gg[["interest_over_time"]]
std<-data.frame(name=c("ana","juan","carla"),math=c(86,43,80),stat=c(90,75,82))
std
