---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Clustering
El clustering es un método cuyo objetivo es el de crear grupos en base a las relaciones multivariantes que existen en los datos, este método es un método previo a las técnicas de clasificación que existen. La base del clustering es la definición de la similaridad entre las filas. Similaridad es definida como una función de distancia entre un par de filas.

Es importante distinguir la existencia de grupos naturales dentro de los datos, normalmente estos grupos son características naturales de las observaciones de interés. 

## Medidas de Disimilaridad
Dado el objetivo del clustering, el aspecto mas importante dentro de estos métodos es utilizar de forma correcta la medida de (di)similaridad entre un para de casos dentro de la base de datos.

La definición de las medidas de distancia es crucial para aplicar estos modelos. Funciones de distancia incorrecta pueden generar sesgos en los resultados y ser un problema para etapas posteriores de la mineria de datos. Debemos distinguir las funciones de distancia segun la naturaleza de las variables.

Sean las filas $x$ e $y$ dentro de una base de datos, estos vectores tienen una dimensión $p$, es decir, se observan  $p$ variables para las 2 observaciones.

### Distancia Euclideana: Variables numéricas

$$d(x,y)=\sqrt{\sum_{i=1}^p{(x_i-y_i)^2}}$$

Donde los $x_i$ y $_y_i$ son los valores para la variable $i$ de las observaciones $x$ e $y$.

### Distancia Manhattan: $p$ grande

$$d(x,y)=\sum_{i=1}^p{|x_i-y_i|}$$

### Distancia Minkowski 

$$d(x,y)=\left(\sum_{i=1}^p{|x_i-y_i|^d}\right)^{1/d}$$

```{r}
aux<-matrix(rnorm(100),nrow=5)
dist(aux) #euclideana
dist(aux, method="manhattan")
dist(aux, method="minkowski", p=3)
```

### programando

```{r}
minkowski<-function(x,y,d){
  dd<-(sum(abs(x-y)**d))**(1/d)
  return(dd)
}
minkowski(c(1,2,3),c(4,2,1),d=2)
x<-c(1,2,3)
y<-c(4,2,1)
sum(abs(x-y)) # manhattan
sqrt(sum(abs(x-y)**2)) # euclideana
# la funcion de distancia
distancia<-function(bd,d=2){
  nf<-dim(bd)[1]
  DD<-matrix(NA,nf-1,nf-1)
  colnames(DD)<-1:(nf-1)
  rownames(DD)<-2:nf
  for(i in 1:(nf-1)){
    for(j in (i+1):nf){
      DD[j-1,i]<-minkowski(bd[i,],bd[j,],d)
    }
  }
  return(DD)
}
distancia(aux,d=2)
dist(aux)
```

### Variables cualitativas
Para las variables cualitativas se debe considerar los casos cuando estas son nominales y ordinales, distinguir tambien los casos de variables binarias.

```{r,eval=F}
install.packages("vegan")
library(vegan)
```

### Datos mixtos 
Una de los mayores desafios es cuando las variables son mixtas, es decir cuantitativas y cualitativas.

```{r,eval=FALSE}
install.packages("cluster")
library(cluster)
```

## Métodos de clustering

* Partición (k-center)
* Jerárquicos (dendograma)
* Basados en densidad
* Basados en cuadrículas (grid)

## K-center Clustering (no jerárquicos)

Algoritmo

1. Partición de las observaciones en $k$ grupos, obtener el vector de promedios de cada grupo (centroides). Se puede trabajar con la media o la mediana.
2. Para cada observación calcular las distancia euclideana a los centroides y reasignar lo observación en base a la menor distancia, recalcular los centroides en base a la reasignación de cada observación
3. Repetir el paso 2 hasta que que ya no existan más reasignaciones

```{r}
bd<-data.frame(x=rnorm(100),y=rnorm(100))
kmeans(bd,2)
```

Ejemplo: 

* Implementar el algoritmo para el k-center, con la distancia de Minkowski y para la media y mediana.

```{r}
#Nota: La entrada de la funcion es un data frame
kcenter<-function(bd,k=3,d=2,tipo="media",seed=123456){
  nf<-dim(bd)[1]
  nc<-dim(bd)[2]
  #paso1: asignar las k (nf>=k)
  set.seed(seed)
  bd$k<-sample(1:k,nf,replace=T)
  centroide<-NULL
  for(i in 1:k){
    if(tipo=="media"){  
      centroide<-rbind(centroide,apply(bd[bd$k==i,],2, mean))
    } else if(tipo=="mediana"){
      centroide<-rbind(centroide,apply(bd[bd$k==i,],2, median))
    }
  }
  #paso2 (recalcular loo centroides al final)
  cc<-1
  while(cc!=0){ #paso3
    cc<-0
    for(i in 1:nf){
      auxd<-NULL
      for(j in 1:k){
        auxd<-c(auxd,minkowski(bd[i,1:nc],centroide[j,1:nc],d=d))
      }
      newk<-which(auxd==min(auxd))
        if(newk!=bd$k[i]){
          bd$k[i] <- newk
          cc<-cc+1
        }
    }
    centroide<-NULL
    for(i in 1:k){
      if(tipo=="media"){  
        centroide<-rbind(centroide,apply(bd[bd$k==i,],2, mean))
      } else if(tipo=="mediana"){
        centroide<-rbind(centroide,apply(bd[bd$k==i,],2, median))
      }
    }
  }
  return(bd)
}
```

* Pensar en un gráfico que permita ver como se asignaron los cluster 

```{r}
bd<-data.frame(x=rnorm(100),y=rnorm(100),z=rnorm(100))
kcenter(bd,k=4,d=1,tipo="mediana")
```


### Validación cluster

* La estructura de los cluster es aleatoria (¿funciona?)
* ¿Cómo definimos el valor de $K$? 

> Silhouette coefficient: 

1. Se obtiene para la observación $i$ el promedio de distancia a todos los objetos en el mismo cluster ($a_i$)
2. Se obtiene para la observación $i$ el promedio de distancia a todos los objetos de los otros clusters ($b_i$)
3. Se define a $s_i$ como el coeficiente, con un recorrido entre $[-1,1]$, para cada observación $i$

$$s_i=\frac{b_i-a_i}{max(a_i,b_i)}$$

Idealmente se espera que $a_i < b_i$ y los $a_i$ cercanos a $0$. 

```{r}
library(cluster)
kk<-kmeans(bd,3)
s <- silhouette(kk$cluster, dist(bd))
plot(s)
#sobre la base IRIS
data("iris")
aux<-kmeans(iris[,-5],3)
s <- silhouette(aux$cluster, dist(iris[,-5]))
plot(s)
```

> Medoide: es el punto de datos que es "menos diferente" de todos los otros puntos de datos. A diferencia del centroide, el medoide tiene que ser uno de los puntos originales.

```{r}
pam(bd,k=3)
kmeans(bd,3)
```

* ¿Cúal es el número óptimo de $k$? 

```{r}
library(fpc)# Flexible Procedures for Clustering
sol <- pamk(iris[,-5], krange=2:10, criterion="asw", usepam=TRUE)
sol
pamk(bd,krange=2:10,usepam = T)
```


### Distancias para variables nominales (todas nominales)
En este caso la mejor estrategia es llevar las variables con sus categorias a variables binarias. Existen múltiples medidas de distancia para variables binarias, muchas de estas medidas son aproximaciones a las medidas mas conocidas. Entre ellas:

Sean las filas $i$, $j$ que contienen los valores binarios de las variables de estudio. Sea $A$ el total de $1$ que existe en $i$, $B$ el total de $1$ que existe en $j$ y sea $J$ el total de casos en los que los $1$ ocurren simultaneamente en $i$ y $j$.

  * Euclideana
  
  $$d_{ij}=\sqrt{A+B-2J}$$
  * Manhattan
  
  $$d_{ij}=A+B-2J$$
  * Bray
  
  $$d_{ij}=\frac{A+B-2J}{A+B}$$

  * Binomial
  
  $$d_{ij}=log(2)(A+B-2J)$$

```{r}
aux<-rbind(c(0,0,0,0,1,1,1),c(1,0,1,0,0,1,1))
A<-sum(aux[1,])
B<-sum(aux[2,])
J<-sum(apply(aux, 2, sum)==2)
#euclideana
sqrt(A+B-2*J)
#manhathan
A+B-2*J
#bray
(A+B-2*J)/(A+B)
#binomial
log(2)*(A+B-2*J)
library(vegan)
vegdist(aux,binary = T)
vegdist(aux,binary = F)
#una base de datos mas grandes
set.seed(999)
aux1<-matrix(rbinom(200,1,0.4),nrow = 20)
vegdist(aux1,method = "binomial",binary = T)
dist(aux1)
```


### Distancias para variables mixtas (cuantitativas, nominales, ordinales)

```{r}
library(cluster)
data("flower")
str(flower)
dd<-daisy(flower,metric = "gower")
summary(dd)
```

Ejercicios

1. Busque funciones en R que permitan calcular los k-center para variables mixtas y medoides 
2. Crear una funcion k-center para variables mixtas y alternativas para incluir el medoide.

## Cluster Jerárquico

El objetivo es obtener una jerarquía de posibles soluciones que van desde un solo grupo a $n$ grupos, donde $n$ es el número de observaciones en el conjunto de datos.

### Algoritmo (Johnson)

1. Se inicia con $n$ grupos y se genera una matriz de $nxn$ de distancias, $D=\{d_{ik}\}$
2. 
3.
4.

```{r}
d <- dist(scale(iris[,-5]))
h <- hclust(d)
plot(h,hang=-0.1,labels=iris[["Species"]],cex=0.5)
clus3 <- cutree(h, 3)
plot(h,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(h,k=3)
```

* Single linkage
* Complete linkage
* Average linkage

## Ejercicios
1. Pensar en un gráfico que permita ver como se asignaron los cluster 
2. Pensar en optimizar el código empleado para el k-center
3. Hacer que la función desarrollada para el k-center retorne también los centroides
4. Utilizando la base de datos de las elecciones del 20 de octubre, crear una base de datos a nivel municipal, aplicar el método k-center con medoides para los resultados a nivel municipal y terminar el $k$ óptimo en un rango de $k=2:10$ (Usar datos relativos).