---
output: html_document
editor_options: 
  chunk_output_type: console
---
[comment]: <> (Machine Learning with R Cookbook, capitulo 4)
[comment]: <> (Torgo, 4.6)
# Regresión

$$y=f(x_1,x_2, \ldots)$$

  * $y$ Variable de resultado, dependiente, solo tenemos a una $y$. 
  * $x_1, x_2, \ldots$, variables de control, independientes.
  
 A partir de estas variables:
 
  * ¿Cuál es la relación de $x$ sobre $y$?
    + Lineal  

$$y_i=\beta_0+\beta_1 x_1+\beta_2x_2+\ldots+\epsilon_i$$
> Nota: Diferenciar que la regresión busca establacer relaciones basadas en los datos y no asi un proceso causal.

    + Polinomial
    + Etc; No lineal,
    
  * Conocer la naturaleza de $y$ y las variables $x$
    + $Y$ es cuanti (real), $X$ cuanti. 
    + $Y$ es cuanti (discreta >= 0), $X$ cuanti. 
    + $Y$ es cuali nominal binario, $X$ mixtas. 
    + $Y$ es cuali ordinal, $X$ mixtas. 

## Regresión lineal

### Paso 1: Base de datos

```{r}
library(dplyr)
load("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data\\eh18.Rdata")
bd<-eh18p %>% filter(s02a_03>=18 & s02a_05=="1.JEFE O JEFA DEL HOGAR" & ocupado=="Si") %>% select(s02a_02,s02a_03,aestudio,tothrs,ylab,ynolab,factor,estrato, upm,area,permanente,cob_op)
```

### Paso 2: Establecer la relación de interés.

  * $Y$ Ingreso laboral puede ser un buena opción
  * $X$ el resto, pueden ser basadas en un modelo teórico o buscadas a partir de un proceso de minería de datos

$$IngresoLaboral=f(edad,sexo,educación,...)$$

### Paso 3: Definir el modelo a utilizar

OLS, MCO. Modelos lineales

```{r}
m1<-lm(ylab~s02a_03,data=bd)# regresión lineal simple y=f(x)
m1
```

$$ylab_i=\beta_0+\beta_1edad_i+\epsilon_i$$

$$E[ylab_i]=4112.16-21.84*edad_i$$

```{r}
plot(bd$s02a_03,bd$ylab)
abline(m1,col="red",lwd=2)
summary(m1)
#en los betas
coefficients(m1)
confint(m1,level=0.99)
#mejorar el modelo
m2<-lm(ylab~s02a_03+aestudio+tothrs+ynolab+s02a_02+area+cob_op,data=bd)
summary(m2)
m3<-lm(log(ylab)~s02a_03+aestudio+tothrs+ynolab+s02a_02+area+cob_op,data=bd)
summary(m3)
m4<-lm(log(ylab)~s02a_03+factor(aestudio)+tothrs+ynolab+s02a_02+area+cob_op,data=bd)
summary(m4)
```

### Paso 4: Optimizar el modelo

```{r}
m5<-lm(log(ylab)~.,data=bd[,-c(7,8,9,11)])
summary(m5)
m6<-step(m5)
summary(m6)
#un ejemplo de laboratorio
bd2<-as.data.frame(matrix(rnorm(1000),ncol = 8))
names(bd2)[1]<-"y"
bd2$x<-bd2$y+runif(125)
plot(bd2$x,bd2$y)
plot(bd2)
p1<-lm(y~.,data=bd2)
summary(p1)
p2<-step(p1)
summary(p2)
```

Nota: Se debe tener en cuenta siempre la fuente de la información

```{r}
summary(m4)
summary(lm(ylab~factor(aestudio)+s02a_02+s02a_03,data=bd))
summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03,data=bd))
#estamos suponiendo una relación lineal
#las relaciones que encontramos son a nivel de la muestra
#para hacer inferencia el mejor camino es la libreria survey
summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03,data=bd))#MCO,OLS
summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03
           ,weights = factor ,data=bd))#MCP
```


### Paso 5: Validar el modelo

```{r}
#Supuestos del modelo
## los errores se distribuyen normal(media=0, varianza=constante)
## Independencia entre los X del modelo
# los errores se distribuyen normal
model<-lm(log(ylab)~s02a_03+aestudio+tothrs+ynolab+s02a_02+area+cob_op,data=bd)
summary(model)

par(mfrow=c(2,2))
plot(model)
dev.off()
#errores
ee<-residuals(model)
plot(density(ee))
#prueba de normalidad
#H0 x ~ normal()
library(normtest)
library(nortest)
ad.test(ee)
lillie.test(ee)
boxplot(bd$ylab)
#La normalidad de los errores
plot(density(ee))
curve(dnorm(x,mean(ee),sd(ee)),add=T,col="red")
#limitar los datos hasta el percentil 
punto<-c(0.01,0.90)
puntonl<-c(0.99)
z<-quantile(bd$ylab,punto,na.rm=T)
znl<-quantile(bd$ynolab,puntonl,na.rm=T)
znl
bd2<-bd %>% filter((ylab<z[2] & ylab>z[1]))
boxplot(bd2$ynolab)

#definiendo el modelo sin atípicos
model1<-lm(log(ylab)~s02a_03+aestudio+tothrs+s02a_02+area,data=bd2)
summary(model1)
boxplot(bd2$ylab)

ee1<-residuals(model1)
ad.test(ee1)
lillie.test(ee1)
ks.test(ee1,"pnorm",mean(ee1),sd(ee1))#kolmogorov Smirnofv

plot(density(ee1))
curve(dnorm(x,mean(ee1),sd(ee1)),add=T,col="red")
plot(model1)

# Distancia de Cook
plot(cooks.distance(model1))
cc<-cooks.distance(model1)
bd3<-bd2[cc<quantile(cc,0.90),]
model3<-lm(log(ylab)~s02a_03+aestudio+s02a_02+area,data=bd3)
summary(model3)
ad.test(residuals(model3))
lillie.test(residuals(model3))

plot(density(residuals(model3)))
curve(dnorm(x,mean(residuals(model3)),sd(residuals(model3))),add=T,col="red")
plot(model3)
plot(cooks.distance(model3))
#ajustando polinomios
bd3<-na.omit(bd3)
model4<-lm(log(ylab)~poly(s02a_03,2)+poly(aestudio,3)+s02a_02+area,data=bd3)
summary(model4)

ad.test(residuals(model4))
## interacciones entre variables
model5<-lm(log(ylab)~poly(s02a_03,2)+poly(aestudio,3)+s02a_02+area+s02a_02:aestudio+area:aestudio+exp(aestudio)+I(aestudio^4),data=bd3)
summary(model5)
ad.test(residuals(model5))

#trabajando sobre los valores atípicos desde R
library(MASS)
modela<-rlm(ylab~s02a_02+s02a_03+area,data=bd2)
modelb<-lm(ylab~s02a_02+s02a_03+area,data=bd2)
summary(modela)
summary(modelb)
ad.test(residuals(model))
# Colinealidad (X1=f(X2)
library(car)
vif(model3)
sqrt(vif(model3))>2 ##Variance Inflation Factors
# Verificar si la varianza es constante (homocedástico) o no (heterocedástico)
library(lmtest)
bptest(model3) # H0: Homocedsticidad https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test

#corrigiendo 
library(rms)
model1 = ols(ylab~s02a_02+s02a_03+area,data=bd3,x=T,y=T)
bptest(model1)
aux<-robcov(model1)
aux
# H0: Homocedasticidad, implica que los EE de B no son los correctos
```


### Paso 6: Predicir a partir del modelo

```{r}
test<-bd3
yest<-predict(model3,test)
plot(log(bd3$ylab),yest)
plot(bd3$ylab,exp(yest))
```

## Probit y Logit

Estrategia, llevar valores binarios a valores continuos. Mediante una función de enlace ($F(Y)$).

$$F(Y)=Y'=X \beta +\epsilon$$

Probit:

$$Y=\Phi (X \beta +\epsilon)$$
$$\phi^{-1}(Y)=X \beta +\epsilon$$
$$Y'=X \beta +\epsilon$$

El enlace $F(Y)=\Phi^{-1}(Y)$, es conocida como probit.


Logit:

$$logit(Y)=log(\frac{Y}{1-Y})=X\beta+\epsilon$$

$$Y=\frac{e^{X\beta+\epsilon}}{1+e^{X\beta+\epsilon}}$$

### En R:

```{r}
library(dplyr)
load("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data\\eh18.Rdata")
aux<-levels(eh18p$s04a_01a)
eh18p<-eh18p %>% mutate(diabetes=(s04a_01a==aux[1] | s04a_01b==aux[1]),corazon=(s04a_01a==aux[4] | s04a_01b==aux[4]),hiper=(s04a_01a==aux[10] | s04a_01b==aux[10]))
eh18p$diabetes<-(eh18p$diabetes==T); eh18p$diabetes[is.na(eh18p$diabetes)]<-F
eh18p$corazon<-(eh18p$corazon==T); eh18p$corazon[is.na(eh18p$corazon)]<-F
eh18p$hiper<-(eh18p$hiper==T); eh18p$hiper[is.na(eh18p$hiper)]<-F
eh18p$cronicas<-(eh18p$diabetes+eh18p$corazon+eh18p$hiper)
#modelo para las enfermedades crónicas
eh18p$cronicas<-(eh18p$cronicas!=0)
#probit logit
logit<-glm(cronicas~s02a_02+s02a_03,data=eh18p,family = binomial(link="logit"))
probit<-glm(cronicas~s02a_02+s02a_03,data=eh18p,family = binomial(link="probit"))
lineal<-lm(cronicas~s02a_02+s02a_03,data=eh18p)
#resumen
summary(logit)
summary(probit)
summary(lineal)
#score probabilidades
lres<-predict(logit,eh18p,type="response")
pres<-predict(probit,eh18p,type="response")
plot(density(lres))
points(density(pres),type = "l",col="red")
#efectos marginales
library(mfx)
probitmfx(cronicas~s02a_02+s02a_03,data=eh18p)
logitmfx(cronicas~s02a_02+s02a_03,data=eh18p)
#ajuste
library(DescTools)
PseudoR2(logit)
PseudoR2(probit)
summary(lineal)
#comparando
library(memisc)
mtable(logit,probit,lineal)
```

```{r}
summary(glm(cronicas~s02a_02+s02a_03,data=eh18p))
summary(lm(cronicas~s02a_02+s02a_03,data=eh18p))
```

