# Minería de Datos

## Motivación para la Minería de datos

  * (Variedad) Los métodos de recolección de datos han evolucionado muy rápidamente.
  * (Volumen) Las bases de datos han crecido exponencialmente
  * (Usuarios) Estos datos contienen información útil para las empresas, países, etc.
  * (Tecnología) El tamaño hace que la inspección manual sea casi imposible
  * (Método) Se requieren métodos de análisis de datos automáticos para optimizar el uso de estos enormes conjuntos de datos

## ¿Qué es la minería de datos?

Es el análisis de **conjuntos de datos** (a menudo grandes) para encontrar **relaciones insospechadas** (conocimiento) y resumir los datos de **formas novedosas** que sean **comprensibles y útiles** para el propietario/usuario de los datos.

<p style='text-align: right;'> Principles of Data Mining (Hand et.al. 2001) </p>

## Datos y conocimiento (Insumo/Resultado)

### Datos:

  * se refieren a instancias únicas y primitivas (single objetos, personas, eventos, puntos en el tiempo, etc.)
  * describir propiedades individuales
  * a menudo son fáciles de recolectar u obtener (por ejemplo, cajeros de escáner, internet, etc.)
  * no nos permiten hacer predicciones o pronósticos

Ejercicio:

Elija un sector/área y describa los potenciales datos que se tienen disponibles

  - (R) Salud (Hospital): Registros de pacientes, medicamentos, vacunas, fichas epidemiológicas, personal médico, consultorios (infraestructura), financiera
  - (R) (Seguridad) Feminicidios: Casos, lugar, entorno familiar, denuncias previas, sospechosos, información forense, Caracterizar al culpable  
  - (A) Banca: Prestamos; ingresos, laboral. Caja de ahorro; personal, movimientos. Cajas; Servicios, etc. Cajeros; Lugar, movimientos. Clientes; Prestamistas; Deuda, mora, juicio, ahorristas. Base de datos de patrimonio 
  - (B) Geografía: Clima; Lluvia, Minas; tipo, trabajadores, cultivos; tipo, temporada, tipo suelo, montañas, pluvial, temperatura. 

### Conocimiento:

  * (Características) se refiere a clases de instancias (conjuntos de ...)
  * (Forma) describe patrones generales, estructuras, leyes,
  * (Declaración) consta de la menor cantidad de declaraciones posibles
  * (Proceso) a menudo es difícil y lleva mucho tiempo encontrar u obtener
  * (Acciones) nos permite hacer predicciones y pronósticos

Ejercicio:

  * Agrupar la tipología de los pacientes; Mejorar servicio, acciones
  * Determinantes del feminicidio; SLIM, medidas 
  * Relación el monto del ingreso con el motivo de préstamo; Realizar ofertas, promocionar a clientes.
  * Relación entre el clima y los cultivos, dado el tipo de mineral se puede pensar en un mercado posterior

## Requerimientos

  * Disponibilidad para aprender 
  * Mucha paciencia
    - Interactúa con otras áreas
    - Preprocesamiento de datos 
  * Creatividad
  * Rigor, prueba y error

## knowledge discovery in databases (KDD)

![](images/f1.png)

## Preparación de los datos

### Recopilación

* Instituto de Estadística 
* UDAPE, ASFI 
* Ministerio Salud (SNIS), Ministerio de educación (SIE)
* APIs, Twitter, Facebook, etc.
* Kaggle
* Banco Mundial, UNICEF, FAO, BID (Open Data)

### Data Warehouse

![](images/dataware.png)

### Data Warehouse in R

![](images/rdata.png)

### Importación

```{r, echo=T,eval=F}
library(foreign)
library(readr)
apropos("read")
getwd()
setwd("C:\\Users\\ALVARO\\Downloads\\bd49 (1)\\Base EH2019")
dir()
eh19v<-read.spss("EH2019_Vivienda.sav",to.data.frame = T)
eh19p<-read.spss("EH2019_Persona.sav",to.data.frame = T)
object.size(eh19p)/10^6
#exportación de datos
setwd("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data")
save(eh19p,eh19v,file="eh19.RData")
# cargando la base de datos que acabamos de guardar
rm(list=ls())
load("eh19.RData")
load("oct20.RData")
# cargando desde github
rm(list=ls())
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/oct20.RData"))
```

### Recopilación

```{r,echo=T,eval=F}
read.table("clipboard",header = T)
library(readxl) # excel 
library(DBI)  # Bases de datos relacionales en el sistema
#library(help=DBI)  
library(RMySQL) # bases de datos en mysql
# web scraping (API)
library(gtrendsR) # API
gg<-gtrends(c("data mining","machine learning"),time="today 12-m")
gg$interest_over_time
plot(gg)
gg<-gtrends(c("data mining","machine learning"),time="today 12-m",geo="BO")
```

### Limpieza

```{r,echo=T}
std<-data.frame(name=c("ana","juan","carla"),math=c(86,43,80),stat=c(90,75,82))
std
```

```{r,echo=T}
library(tidyr)
bd<-gather(std,materia,nota,math:stat)
bd
# otra opción más relacionada a bases de datos con información de tiempo, 
# es el comando reshape
```

### Ejercicio (reshape)

http://www.udape.gob.bo/portales_html/dossierweb2019/htms/CAP07/C070311.xls

###  Limpieza (fechas)

```{r,echo=T}
library(lubridate)
date()
today()
ymd("20151021")
ymd("2015/11/30")
myd("11.2012.3")
dmy_hms("2/12/2013 14:05:01")
mdy("120112")
d1<-dmy("15032020")
class(d1)
#ts()
```

### Limpieza (String)

```{r,echo=TRUE}

toupper("hola")
tolower("HOLA")

abc<-letters[1:10]
toupper(abc)

tolower("Juan")
# Extraer partes de un texto
substr("hola como estan",1,3)
substr("hola como estan",3,7)

# contar la cantidad de caracteres
nchar("hola")

nchar(c("hola","chau","LA                         paz"))

x<-c("LA-.paz","La Paz", "La pas", "La    paz","lapaz","la 78 paz")

x<-toupper(x)

x<-gsub("PAS","PAZ",x)

library(tm)
x<-removeNumbers(x)
x<-removePunctuation(x)
x<-gsub("LAPAZ","LA PAZ",x)
x<-stripWhitespace(x)

nchar(x)

nchar(gsub("  "," ",x))

gsub("a","x","hola como estas")
grepl("a",c("hola","como"))
grepl("o",c("hola","como"))

#otra alternativa
x<-c("LA-.paz","La Paz", "La pas", "La    paz","lapaz","la 78 paz")
x<-toupper(x)
x[grepl("PAZ",x)]<-"LA PAZ"
x<-gsub("PAS","PAZ",x)

# para llevar a ascii
utf8ToInt("la paz")
utf8ToInt("@")

library(stringi)
```

Ejemplo de web scraping sobre la página https://www.worldometers.info/

```{r}
library(rvest)
url<-"https://www.worldometers.info/coronavirus/"
covid<-read_html(url)
bdcov<-html_table(covid)
bdnow<-bdcov[[1]]
str(bdnow)
```

Tarea: limpiar la base de datos

  * Convertir las variables necesarias a numéricas
  * Debe ser una base de solo países
  
### Transformación

```{r}
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))
names(eh19p)
```

* Estandarizar variables

```{r}
summary(eh19p$s02a_03) #edad
summary(eh19p$tothrs) # total de horas de trabajo semanal
summary(eh19p$ylab)  # ingreso laboral mensual

sd(eh19p$s02a_03)
sd(eh19p$tothrs,na.rm = T)
sd(eh19p$ylab,na.rm = T)

x1<-scale(eh19p$s02a_03)
x2<-scale(eh19p$tothrs)
x3<-scale(eh19p$ylab)

sd(x1);sd(x2,na.rm = T);sd(x3,na.rm = T)
par(mfrow=c(2,3))
boxplot(eh19p$s02a_03,ylim=c(0,25000))
boxplot(eh19p$tothrs,ylim=c(0,25000))
boxplot(eh19p$ylab,ylim=c(0,25000))

boxplot(x1,ylim=c(-3,3))
boxplot(x2,ylim=c(-3,3))
boxplot(x3,ylim=c(-3,3))

par(mfrow=c(2,3))
plot(density(eh19p$s02a_03))
plot(density(eh19p$tothrs,na.rm=T))
plot(density(eh19p$ylab,na.rm=T))

plot(density(x1))
plot(density(x2,na.rm=T))
plot(density(x3,na.rm=T))

mean(eh19p$ylab,na.rm=T)
median(eh19p$ylab,na.rm=T)
```


* Función logarítmo

```{r}
dev.off()
curve(log,xlim=c(10,30000))

x1<-log(eh19p$s02a_03)
x2<-log(eh19p$tothrs)
x3<-log(eh19p$ylab)

par(mfrow=c(2,3))
plot(density(eh19p$s02a_03))
plot(density(eh19p$tothrs,na.rm=T))
plot(density(eh19p$ylab,na.rm=T))

plot(density(x1))
plot(density(x2,na.rm=T))
plot(density(x3,na.rm=T))
dev.off()
```


* Creación de variables

```{r}
eh19p$log_ylab <-log(eh19p$ylab)
eh19p$scale_ylab <-scale(eh19p$ylab)
names(eh19p)

#install.packages("dplyr")
library(dplyr) # filtrado, selección, creación de variables, resumen
#nota: dplyr se enfoca en el encadenamiento de comandos, con una lógica similar al SQL

# %>% # operador pipe: ctr + mayus + m 

eh19p <- eh19p %>% mutate(x1=ylab^2,llylab=log(ylab),
                 tothrs_mensual=tothrs*4.35,mujer=s02a_02=="2.Mujer")

#cut() # crear clases

# grandes grupos de edad
eh19p<-eh19p %>% mutate(gedad=cut(s02a_03,c(-1,18,60,max(s02a_03)),labels = c("<=18","19 a 60",">60") ))

eh19p %>% select(1,folio,s02a_03,gedad) %>% head()

table(eh19p$gedad)

eh19p %>% select(gedad) %>% table()

barplot(table(eh19p$gedad))
```


* Recodificar variables

```{r,eval=FALSE}
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))
library(dplyr)

a<-c(1:10)
recode(a,`1` = 20L,`2` = 20L,`4` = 30L)

eh19p$sexo<-recode(eh19p$s02a_02,"1.Hombre"="H","2.Mujer"="M")

table(eh19p$sexo)

eh19p<-eh19p %>% mutate(sexo2=recode(s02a_02,"1.Hombre"="M","2.Mujer"="F"))
eh19p %>% select(sexo2) %>% table()

# binarias
unique(eh19p$depto)
# se quiere crear una nueva variable, llamada región: 
#    Altiplano: LP, OR, PT
#    Valle: CB, CH, TR
#    Llano: SC, BN, PD
# tarea
#if_else() # trabaja con spark, para crear binarios
#case_when() # múltiple categorías basadas en reglas

v1<-c("La Paz","Oruro","Potosí")
v2<-c("Chuquisaca","Cochabamba","Tarija")
v3<-c("Santa Cruz","Beni","Pando")

eh19p %>% mutate(altiplano = depto %in% v1 ) %>% select(altiplano) %>% table()
  
eh19p <- eh19p %>% mutate(altiplano = depto %in% v1 , valle = depto %in% v2,llano = depto %in% v3)
names(eh19p)

```


### Definir diseño de encuesta por muestreo

Si tenemos una base de datos proveniente de una encuesta por muestreo, debemos tener conocimiento de las características del diseño muestral empleado en la encuesta, ya que este diseño afecta de forma directa el proceso de estimación y tiene un error de muestreo. Principalmente lo siguiente:

  * Si el diseño tiene etapas (o mono etápico con conglomerados), la variable de conglomeración y de estratificación son muy relevantes. Estas afectan directamente a los errores de las estimaciones.
  * Si el esquema de selección de las unidades muestrales a sido autoponderada (MAS, todas las unidades tenían la **misma probabilidad** de ser seleccionadas) o no. Si no es autponderada se requiere la variable conocida como el factor de expansión (inverso de la probabilidad de selección)
  
Hay tres variables relevantes: conglomerados (principalmente primera etapa), estratificación (principalmente primera etapa) y factor de expansión. 

Idealmente en un muestreo de varias etapas y estratificado el factor por finitud es necesario ya que permite mejorar la aproximación a la varianza.


```{r,eval=FALSE}
#install.packages("survey")
library(survey) # no trabaja con el concepto de dplyr, no permite el uso de "%>%" 
library(srvyr) # permite el uso del operador %>% 
names(eh19p)
#survey
sd_eh19p<-svydesign(ids = ~upm + folio, strata = ~estrato, weights = ~factor,data = eh19p)
str(sd_eh19p)

table(eh19p$p0)
prop.table(table(eh19p$p0))*100 # pobreza moderada en la muestra 

svytable(~p0 ,design = sd_eh19p)
prop.table(svytable(~p0 ,design = sd_eh19p))*100

summary(svytable(~p0 ,design = sd_eh19p))

t1<-svymean(~ylab,design = sd_eh19p,na.rm=T,deff=T)
cv(t1)
confint(t1)

t2<-svyby(~ylab,by=~depto+area,design = sd_eh19p,FUN = svymean,na.rm=T)
cv(t2)
confint(t2)

summary(svytable(~depto+p0 ,design = sd_eh19p)) # revisar

# departamento
prop.table(table(eh19p$depto,eh19p$p0),1)*100
prop.table(svytable(~depto+p0 ,design = sd_eh19p),1)*100

svydesign(ids=~1,data=bd)#mas
svydesign(ids=~1,strata = estrato,data=bd)#mas estraficado
svydesign(ids=~1,strata = ~estrato,weights = ~ponderador,data=bd)#pps estraficado

svymean(~p0, design=sd_eh19p,na.rm=T)
svytotal(~p0, design=sd_eh19p,na.rm=T)

#
svytable(~p0,design=sd_eh19p) # tablas de contigencias y hacer pruebas sobre estas

t1<-svymean(~p0,design=sd_eh19p,na.rm=T) # proporciones / medias
t2<-svytotal(~p0,design=sd_eh19p,na.rm=T) # totales clase / total

cv(t1)
cv(t2)

confint(t1)
confint(t2)

sd2_eh19p<-svydesign(ids = ~1, weights = ~factor,data = eh19p) # pps

t3<-svymean(~p0,design=sd2_eh19p,na.rm=T) # proporciones / medias

t1
t3
cv(t1)*100
cv(t3)*100

summary(lm(ylab~aestudio,data=eh19p)) # ingreso= B0+B1*aestudio+e OLS
summary(lm(ylab~aestudio,data=eh19p,weights = factor )) # Minimos cuadrados ponderados

m1<-svyglm(ylab~aestudio,design=sd_eh19p)
summary(m1)
install.packages("jtools")

library(jtools)
summ(m1)

psrsq(m1)

# srvyr 

library(srvyr)
sd3_eh19p<-as_survey_design(sd_eh19p)

sd_eh19p %>% select(p0) %>% svymean()

sd3_eh19p %>% summarise(survey_mean(s02a_03))

sd3_eh19p %>% group_by(depto,area,s02a_02) %>% summarise(m_edad=survey_mean(s02a_03))

sd3_eh19p %>% group_by(depto,area,s02a_02) %>% summarise(m_edad=survey_mean(s02a_03),m_ylab=survey_mean(ylab,na.rm=T))

# p0
sd3_eh19p %>% group_by(depto) %>% survey_tally()

sd3_eh19p %>% group_by(depto) %>% survey_count()

sd3_eh19p %>% mutate(pobreza=p0=="Pobre") %>% summarise(p0=survey_mean(pobreza,na.rm=T))

sd3_eh19p %>% mutate(pobreza=p0=="Pobre") %>% group_by(depto,area) %>%  summarise(p0=survey_mean(pobreza,na.rm=T),N=survey_total(pobreza,na.rm=T))

sd4_eh19p<- eh19p %>% as_survey_design(ids=c(upm,folio),strata=estrato,weights=factor)

sd4_eh19p<- eh19p %>% as_survey_design(ids=upm,strata=estrato,weights=factor)

sd4_eh19p %>% mutate(pobreza=p0=="Pobre") %>% group_by(depto,area) %>%  summarise(p0=survey_mean(pobreza,na.rm=T,vartype = c("se", "ci", "var", "cv"),deff=T))

sd4_eh19p %>% mutate(pobreza=p0=="Pobre") %>%  summarise(p0=survey_mean(pobreza,na.rm=T,vartype = c("se", "ci", "var", "cv"),deff=T))
```


## Imputación de variables 

**We should be suspicious of any dataset (large or small) which appears perfect.**

— David J. Hand

### La falta de información es información

* MCAR missing completely at random 
* MAR missing at random 
* MNAR missing not at random 

### Aproximación formal
Sea $Y$ una matriz de datos con $n$ observaciones y $p$ variables. Sea $R$ una matriz de respuesta binaria, tal que si $y_{ij}$ es observada, entonces $r_{ij}=1$.

Los valores observados son colectados en $Y_{obs}$, las observaciones perdidas en $Y_{mis}$. Así, $Y=(Y_{obs},Y_{mis})$.

La distribución de $R$ depende de $Y=(Y_{obs},Y_{mis})$. Sea $\psi$ que contiene los parámetros del modelo de los datos perdidos, así la expresión del modelo de los datos perdidos es $\Pr(R|Y_\mathrm{obs},Y_\mathrm{mis},\psi)$

### MCAR, MAR, MNAR

MCAR (missing completely at random )
$$
\Pr(R=0|{\mbox{$Y_\mathrm{obs}$}},{\mbox{$Y_\mathrm{mis}$}},\psi) = \Pr(R=0|\psi)
$$

MAR (missing at random )
$$
\Pr(R=0|{\mbox{$Y_\mathrm{obs}$}},{\mbox{$Y_\mathrm{mis}$}},\psi) = \Pr(R=0|{\mbox{$Y_\mathrm{obs}$}},\psi) 
$$

MNAR (missing not at random )
$$
\Pr(R=0|{\mbox{$Y_\mathrm{obs}$}},{\mbox{$Y_\mathrm{mis}$}},\psi)
$$

### Alternativas para trabajar con los Missings (Ad-hoc)

* Listwise deletion 
* Pairwise deletion
* Mean imputation
* Regression imputation
* Stochastic regression imputation
* Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) 
* Indicator method

### Imputación Multiple

![](images/impu.png){width=800px}


### Patrones en datos multivariados

![](images/impu2.png){width=800px}

### Influx and outflux

$$
I_j = \frac{\sum_j^p\sum_k^p\sum_i^n (1-r_{ij})r_{ik}}{\sum_k^p\sum_i^n r_{ik}}
$$

La variable con mayor influx está mejor conectada a los datos observados y, por lo tanto, podría ser más fácil de imputar.

$$
O_j = \frac{\sum_j^p\sum_k^p\sum_i^n r_{ij}(1-r_{ik})}{\sum_k^p\sum_i^n 1-r_{ij}}
$$
La variable con mayor outflux está mejor conectada a los datos faltantes, por lo tanto, es potencialmente más útil para imputar otras variables.

### Imputación de datos monótonos

![](images/algoritmo1.png)


### Multivariate Imputation by Chained Equations (mice)
(Imputación multivariante por ecuaciones encadenadas)

![](images/algoritmo1.png)

### En R

#### Métodos ad-hoc

* Listwise deletion (trabajar solo con casos completos)

```{r}
table(is.na(airquality$Ozone))

R<-(!is.na(airquality))*1

mean(airquality$Ozone)
#listwise
x<-na.omit(airquality$Ozone)
mean(x)

bd<-airquality
bd2<-na.omit(bd)

na.action(x) 
na.action(bd2) 

naprint(na.action(x))
naprint(na.action(bd2))

table(complete.cases(bd))

ii<-complete.cases(bd)

bd[ii,]

Y_obs<-na.omit(bd)
Y_mis<-mice::ic(bd)

bd$Ozone[bd$Ozone=="**"]<-NA
#gsub recode

table(R[,1],R[,2])
chisq.test(table(R[,1],R[,2]))

apply(R,2,mean)

table(R[,1],R[,3])
chisq.test(table(R[,1],R[,3]))

table(R[,1],R[,4])
chisq.test(table(R[,1],R[,4]))


R<-as.data.frame(R)
m1<-glm(Ozone~Solar.R,data = R,family = "binomial") # logit

summary(m1)

summary(lm(Ozone~Solar.R,data=bd2))
```

* Pairwise deletion (casos parciales)

```{r}
dim(na.omit(airquality[,1:2]))

dim(na.omit(airquality[,c(1,3)]))

dim(na.omit(airquality[,c(2,3)]))

cor(na.omit(airquality[,c(2,3)]))
```


* Mean imputation (MCAR)

```{r}
bd<-airquality
m1<-mean(bd$Ozone,na.rm=T)
bd$Ozone[is.na(bd$Ozone)]<-m1
mean(bd$Ozone)
par(mfrow=c(1,2))
hist(airquality$Ozone)
hist(bd$Ozone)

par(mfrow=c(1,2))
plot(density(airquality$Ozone,na.rm=T))
plot(density(bd$Ozone,na.rm=T))


library(mice)
imp <- mice(airquality, method = "mean", m = 1, maxit = 1)
bdi<-complete(imp)

cor(na.omit(airquality[,1:2]))
cor(bd[,1:2])

plot(airquality$Ozone,airquality$Solar.R)
plot(bdi$Ozone,bdi$Solar.R)
```


* Regression imputation (MAR)

$$y_i=\beta_0+\beta_1 x_i+\epsilon_i$$

$$E[y/x]=\beta_0+\beta_1 x$$

```{r}
fit <- lm(Ozone ~ Solar.R, data = airquality)
summary(fit)

pred <- predict(fit, newdata = ic(airquality))

#para el caso 10

18.599+0.127*194

data <- airquality[, c("Ozone", "Solar.R")]

imp <- mice(data, method = "norm.predict", seed = 1,
           m = 1, print = FALSE)

complete(imp)

plot(airquality[,1:2])
plot(complete(imp)[,1:2])

cor(na.omit(airquality[,1:2]))
cor(complete(imp)[,1:2])
```

* Stochastic regression imputation (MAR)

$$y_i=\hat{\beta_0}+\hat{\beta_1} x_i+\hat{\epsilon_i}$$

```{r}
data <- airquality[, c("Ozone", "Solar.R")]
imp <- mice(data, method = "norm.nob", m = 1, maxit = 1,
            seed = 1, print = FALSE)
complete(imp)

plot(airquality[,1:2])
plot(complete(imp)[,1:2])

cor(na.omit(airquality[,1:2]))
cor(complete(imp)[,1:2])

plot(density(airquality$Ozone,na.rm=T))
plot(density(complete(imp)[,1],na.rm=T))

plot(density(airquality$Solar.R,na.rm=T))
plot(density(complete(imp)[,2],na.rm=T))

imp<-mice(airquality, method = "norm.nob", m = 1, maxit = 1,
            seed = 1, print = FALSE)
complete(imp)
```

```{r}
md.pattern(airquality, plot = T)
flux(airquality)[,1:3]
```