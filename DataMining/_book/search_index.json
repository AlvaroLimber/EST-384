[
["index.html", "Minería de datos con R EST-384 Prefacio Audiencia Estructura del libro Software y acuerdos Datos Agradecimiento", " Minería de datos con R EST-384 Alvaro Chirino Gutierrez 2020-06-03 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Programación Estadística II, una materia optativa del pregrado de la carrera de Estadística de la Universidad Mayor de San Andrés. Este documento representa un primer acercamiento a los estudiantes de estadistica al software R y al mundo de la minería de datos. Estructura del libro El libro inluye 5 capitulos, estos son: Introducción a R Preparación de los datos Modelado en Minería de datos Minería de Texto Machine Learning Software y acuerdos sessionInfo() ## R version 4.0.0 (2020-04-24) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Spanish_Bolivia.1252 ## [2] LC_CTYPE=Spanish_Bolivia.1252 ## [3] LC_MONETARY=Spanish_Bolivia.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=Spanish_Bolivia.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods ## [7] base ## ## other attached packages: ## [1] vegan_2.5-6 permute_0.9-5 fpc_2.2-5 ## [4] cluster_2.1.0 ca_0.71.1 TeachingDemos_2.12 ## [7] ggridges_0.5.2 GGally_1.5.0 Hmisc_4.4-0 ## [10] ggplot2_3.3.0 Formula_1.2-3 survival_3.1-12 ## [13] lattice_0.20-41 dplyr_0.8.5 DMwR2_0.0.2 ## [16] stringi_1.4.6 lubridate_1.7.8 tidyr_1.0.2 ## [19] readr_1.3.1 foreign_0.8-78 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-147 xts_0.12-0 RColorBrewer_1.1-2 ## [4] progress_1.2.2 prabclus_2.3-2 tools_4.0.0 ## [7] backports_1.1.6 utf8_1.1.4 R6_2.4.1 ## [10] rpart_4.1-15 DBI_1.1.0 mgcv_1.8-31 ## [13] colorspace_1.4-1 nnet_7.3-13 withr_2.2.0 ## [16] tidyselect_1.0.0 gridExtra_2.3 prettyunits_1.1.1 ## [19] curl_4.3 compiler_4.0.0 cli_2.0.2 ## [22] htmlTable_1.13.3 labeling_0.3 bookdown_0.18 ## [25] diptest_0.75-7 scales_1.1.0 checkmate_2.0.0 ## [28] DEoptimR_1.0-8 robustbase_0.93-6 stringr_1.4.0 ## [31] digest_0.6.25 rmarkdown_2.1 base64enc_0.1-3 ## [34] jpeg_0.1-8.1 pkgconfig_2.0.3 htmltools_0.4.0 ## [37] htmlwidgets_1.5.1 rlang_0.4.5 TTR_0.23-6 ## [40] rstudioapi_0.11 quantmod_0.4.17 farver_2.0.3 ## [43] generics_0.0.2 zoo_1.8-8 mclust_5.4.6 ## [46] acepack_1.4.1 magrittr_1.5 modeltools_0.2-23 ## [49] Matrix_1.2-18 Rcpp_1.0.4.6 munsell_0.5.0 ## [52] fansi_0.4.1 lifecycle_0.2.0 yaml_2.2.1 ## [55] MASS_7.3-51.5 flexmix_2.3-15 plyr_1.8.6 ## [58] grid_4.0.0 parallel_4.0.0 crayon_1.3.4 ## [61] splines_4.0.0 hms_0.5.3 knitr_1.28 ## [64] pillar_1.4.3 stats4_4.0.0 glue_1.4.1 ## [67] packrat_0.5.0 evaluate_0.14 latticeExtra_0.6-29 ## [70] data.table_1.12.8 png_0.1-7 vctrs_0.2.4 ## [73] gtable_0.3.0 purrr_0.3.4 reshape_0.8.8 ## [76] kernlab_0.9-29 assertthat_0.2.1 xfun_0.13 ## [79] class_7.3-16 tibble_3.0.1 ellipsis_0.3.0 Datos Agradecimiento Peeta… "],
["acerca-del-autor.html", "Acerca del autor Bibliografía", " Acerca del autor Bibliografía Torgo, L. (2016). Data mining with R: Learning with case studies, second edition. Hernandez, J. (2004). Introducción a la Minería de Datos Step, I., &amp; Blueprint, S. (2017). MACHINE LEARNING Intuitive Step by Step. "],
["minería-de-datos.html", "1 Minería de Datos 1.1 Motivación para la Mineria de datos 1.2 ¿Qué es la minería de datos? 1.3 Datos y conocimiento 1.4 Requerimientos 1.5 knowledge discovery in databases (KDD) 1.6 Preparación de los datos 1.7 Imputación de variables", " 1 Minería de Datos 1.1 Motivación para la Mineria de datos Los métodos de recoleccion de datos han evolucionado muy rápidamente. Las bases de datos han crecido exponencialmente Estos datos contienen información útil para las empresas, paises, etc.. El tamaño hace que la inspección manual sea casi imposible Se requieren métodos de análisis de datos automáticos para optimizar el uso de estos enormes conjuntos de datos 1.2 ¿Qué es la minería de datos? Es el análisis de conjuntos de datos (a menudo grandes) para encontrar relaciones insospechadas (conocimiento) y resumir los datos de formas novedosas que sean comprensibles y útiles para el propietario/usuario de los datos. Principles of Data Mining (Hand et.al. 2001) 1.3 Datos y conocimiento 1.3.1 Datos: se refieren a instancias únicas y primitivas (single objetos, personas, eventos, puntos en el tiempo, etc.) describir propiedades individuales a menudo son fáciles de recolectar u obtener (por ejemplo, cajeros de escáner, internet, etc.) no nos permiten hacer predicciones o pronósticos 1.3.2 Conocimiento: se refiere a clases de instancias (conjuntos de …) describe patrones generales, estructuras, leyes, consta de la menor cantidad de declaraciones posibles a menudo es difícil y lleva mucho tiempo encontrar u obtener nos permite hacer predicciones y pronósticos 1.4 Requerimientos Disponibilidad para aprender Mucha paciencia Interactúa con otras áreas Preprocesamiento de datos Creatividad Rigor, prueba y error 1.5 knowledge discovery in databases (KDD) 1.6 Preparación de los datos 1.6.1 Recopilación Instituto de Estadística UDAPE, ASFI Ministerio Salud (SNIS), Ministerio de educación (SIE) APIs, Twitter, Facebook, etc. Kaggle Banco Mundial, UNICEF, FAO, BID (Open Data) 1.6.2 Data Warehouse 1.6.3 Data Warehouse in R 1.6.4 Importación library(foreign) library(readr) apropos(&quot;read&quot;) ## [1] &quot;.rs.api.readPreference&quot; ## [2] &quot;.rs.connectionReadDSN&quot; ## [3] &quot;.rs.connectionReadInstallers&quot; ## [4] &quot;.rs.connectionReadOdbc&quot; ## [5] &quot;.rs.connectionReadOdbcEntry&quot; ## [6] &quot;.rs.connectionReadPackageInstallers&quot; ## [7] &quot;.rs.connectionReadPackages&quot; ## [8] &quot;.rs.connectionReadSnippets&quot; ## [9] &quot;.rs.connectionReadWindowsRegistry&quot; ## [10] &quot;.rs.isREADME&quot; ## [11] &quot;.rs.odbcBundleReadIni&quot; ## [12] &quot;.rs.onAvailablePackagesReady&quot; ## [13] &quot;.rs.readAliases&quot; ## [14] &quot;.rs.readDataCapture&quot; ## [15] &quot;.rs.readFile&quot; ## [16] &quot;.rs.readIniFile&quot; ## [17] &quot;.rs.readLines&quot; ## [18] &quot;.rs.readPackageDescription&quot; ## [19] &quot;.rs.readRnbCache&quot; ## [20] &quot;.rs.readShinytestResultRds&quot; ## [21] &quot;.rs.readSourceDocument&quot; ## [22] &quot;.rs.readUiPref&quot; ## [23] &quot;.rs.rnb.readConsoleData&quot; ## [24] &quot;read.arff&quot; ## [25] &quot;read.cep&quot; ## [26] &quot;read.csv&quot; ## [27] &quot;read.csv2&quot; ## [28] &quot;read.dbf&quot; ## [29] &quot;read.dcf&quot; ## [30] &quot;read.delim&quot; ## [31] &quot;read.delim2&quot; ## [32] &quot;read.DIF&quot; ## [33] &quot;read.dta&quot; ## [34] &quot;read.epiinfo&quot; ## [35] &quot;read.fortran&quot; ## [36] &quot;read.ftable&quot; ## [37] &quot;read.fwf&quot; ## [38] &quot;read.mtp&quot; ## [39] &quot;read.octave&quot; ## [40] &quot;read.S&quot; ## [41] &quot;read.socket&quot; ## [42] &quot;read.spss&quot; ## [43] &quot;read.ssd&quot; ## [44] &quot;read.systat&quot; ## [45] &quot;read.table&quot; ## [46] &quot;read.xport&quot; ## [47] &quot;read.xportDataload&quot; ## [48] &quot;read_csv&quot; ## [49] &quot;read_csv_chunked&quot; ## [50] &quot;read_csv2&quot; ## [51] &quot;read_csv2_chunked&quot; ## [52] &quot;read_delim&quot; ## [53] &quot;read_delim_chunked&quot; ## [54] &quot;read_file&quot; ## [55] &quot;read_file_raw&quot; ## [56] &quot;read_fwf&quot; ## [57] &quot;read_lines&quot; ## [58] &quot;read_lines_chunked&quot; ## [59] &quot;read_lines_raw&quot; ## [60] &quot;read_lines_raw_chunked&quot; ## [61] &quot;read_log&quot; ## [62] &quot;read_rds&quot; ## [63] &quot;read_table&quot; ## [64] &quot;read_table2&quot; ## [65] &quot;read_tsv&quot; ## [66] &quot;read_tsv_chunked&quot; ## [67] &quot;readBin&quot; ## [68] &quot;readChar&quot; ## [69] &quot;readCitationFile&quot; ## [70] &quot;readClipboard&quot; ## [71] &quot;readline&quot; ## [72] &quot;readLines&quot; ## [73] &quot;readr_example&quot; ## [74] &quot;readRDS&quot; ## [75] &quot;readRegistry&quot; ## [76] &quot;readRenviron&quot; ## [77] &quot;readSAScsv&quot; ## [78] &quot;RsquareAdj&quot; ## [79] &quot;spread&quot; ## [80] &quot;spread.labs&quot; ## [81] &quot;spread_&quot; ## [82] &quot;stri_read_lines&quot; ## [83] &quot;stri_read_raw&quot; ## [84] &quot;Sys.readlink&quot; 1.6.5 Recopilación read.table(&quot;clipboard&quot;,header = T) library(readxl) library(dplyr) library(DBI) library(RMySQL) library(gtrendsR) # API 1.6.6 Limpieza std&lt;-data.frame(name=c(&quot;ana&quot;,&quot;juan&quot;,&quot;carla&quot;),math=c(86,43,80),stat=c(90,75,82)) std ## name math stat ## 1 ana 86 90 ## 2 juan 43 75 ## 3 carla 80 82 library(tidyr) bd&lt;-gather(std,materia,nota,math:stat) bd ## name materia nota ## 1 ana math 86 ## 2 juan math 43 ## 3 carla math 80 ## 4 ana stat 90 ## 5 juan stat 75 ## 6 carla stat 82 1.6.7 Ejercicio (reshape) http://www.udape.gob.bo/portales_html/dossierweb2019/htms/CAP07/C070311.xls 1.6.8 Limpieza (fechas) library(lubridate) ymd(&quot;20151021&quot;) ## [1] &quot;2015-10-21&quot; ymd(&quot;2015/11/30&quot;) ## [1] &quot;2015-11-30&quot; myd(&quot;11.2012.3&quot;) ## [1] &quot;2012-11-03&quot; dmy_hms(&quot;2/12/2013 14:05:01&quot;) ## [1] &quot;2013-12-02 14:05:01 UTC&quot; mdy(&quot;120112&quot;) ## [1] &quot;2012-12-01&quot; 1.6.9 Limpieza (String) toupper(&quot;hola&quot;) ## [1] &quot;HOLA&quot; abc&lt;-letters[1:10] toupper(abc) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; tolower(&quot;HOLA&quot;) ## [1] &quot;hola&quot; tolower(&quot;Juan&quot;) ## [1] &quot;juan&quot; substr(&quot;hola como estan&quot;,1,3) ## [1] &quot;hol&quot; substr(&quot;hola como estan&quot;,3,7) ## [1] &quot;la co&quot; nchar(&quot;hola&quot;) ## [1] 4 gsub(&quot;a&quot;,&quot;x&quot;,&quot;hola como estas&quot;) ## [1] &quot;holx como estxs&quot; grepl(&quot;a&quot;,c(&quot;hola&quot;,&quot;como&quot;)) ## [1] TRUE FALSE grepl(&quot;o&quot;,c(&quot;hola&quot;,&quot;como&quot;)) ## [1] TRUE TRUE library(stringi) 1.6.10 Transfomración Estandarizar variables Función logarítmo Creación de variables Recodificar variables 1.7 Imputación de variables We should be suspicious of any dataset (large or small) which appears perfect. — David J. Hand 1.7.1 La falta de información es información MCAR missing completely at random MAR missing at random MNAR missing not at random 1.7.2 Aproximación formal Sea \\(Y\\) una matriz de datos con \\(n\\) observaciones y \\(p\\) variables. Sea \\(R\\) una matriz de respuesta binaria, tal que si \\(y_{ij}\\) es observada, entonces \\(r_{ij}=1\\). Los valores observados son colectados en \\(Y_{obs}\\), las observaciones perdidas en \\(Y_{mis}\\). Así, \\(Y=(Y_{obs},Y_{mis})\\). La distribución de \\(R\\) depende de \\(Y=(Y_{obs},Y_{mis})\\). Sea \\(\\psi\\) que contiene los parametros del modelos de los datos perdidos, asi la expresion del modelo de los datos perdidos es \\(\\Pr(R|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi)\\) 1.7.3 MCAR, MAR, MNAR MCAR (missing completely at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) = \\Pr(R=0|\\psi) \\] MAR (missing at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) = \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},\\psi) \\] MNAR (missing not at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) \\] 1.7.4 Alternativas para trabajar con los Missings (Ad-hoc) Listwise deletion Pairwise deletion Mean imputation Regression imputation Stochastic regression imputation Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) Indicator method 1.7.5 Imputación Multiple 1.7.6 Patrones en datos multivariados 1.7.7 Influx and outflux \\[ I_j = \\frac{\\sum_j^p\\sum_k^p\\sum_i^n (1-r_{ij})r_{ik}}{\\sum_k^p\\sum_i^n r_{ik}} \\] La variable con mayor influx está mejor conectada a los datos observados y, por lo tanto, podría ser más fácil de imputar. \\[ O_j = \\frac{\\sum_j^p\\sum_k^p\\sum_i^n r_{ij}(1-r_{ik})}{\\sum_k^p\\sum_i^n 1-r_{ij}} \\] La variable con mayor outflux está mejor conectada a los datos faltantes, por lo tanto, es potencialmente más útil para imputar otras variables. 1.7.8 Imputación de datos monótonos 1.7.9 Multivariate Imputation by Chained Equations (mice) (Imputación multivariante por ecuaciones encadenadas) "],
["modelado-en-minería-de-datos.html", "2 Modelado en Minería de datos 2.1 Explorando los datos 2.2 Componentes Principales 2.3 Análisis de correspondencia", " 2 Modelado en Minería de datos 2.1 Explorando los datos Existen dos aproximaciones para empezar a explorar la información existente en una base de datos: Resumen de los datos Visualización de los datos 2.1.1 Resumen de los datos Dado el tamaño de las bases de datos resulta imposible o muy dificil conocer todas sus propiedad, el resumen de los datos intenta brindar propiedades claves de los datos, estas propiedades podrian ser: Cual es el valor mas comun Cuan variable o dispersa esta la informacion Existen valores extraños o inesparedaps en la base de datos Los datos siguen alguna distribucion A continuación se emplea la encuesta a hogares 2018 para ir respondiendo estas preguntas. En cuanto a los valores mas comunes, la media y la mediana de los datos son suficientes para las variables cuantitativas, mientra que para variables cualitativas, las categorias mas frecuentes son una buena opción. load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/eh18.Rdata&quot;)) # media de edad mean(eh18p$s02a_03) ## [1] 29.59059 #mediana de edad median(eh18p$s02a_03) ## [1] 26 # para las categorias mas frecuentes library(DMwR2) library(dplyr) #para el sexo centralValue(eh18p$s02a_02) ## [1] &quot;2.Mujer&quot; #para el departamento centralValue(eh18p$depto) ## [1] &quot;La Paz&quot; A veces es mejor ver los resultados por grupos, por ejemplo, podemos verlo por departamento y area. eh18p %&gt;% group_by(depto,area) %&gt;% summarise(mean(s02a_03),median(s02a_03),centralValue(s02a_02),n()) ## # A tibble: 18 x 6 ## # Groups: depto [9] ## depto area `mean(s02a_03)` `median(s02a_03~ `centralValue(s~ ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Chuq~ Urba~ 28.7 24 2.Mujer ## 2 Chuq~ Rural 31.0 27 2.Mujer ## 3 La P~ Urba~ 30.4 27 2.Mujer ## 4 La P~ Rural 34.3 31 2.Mujer ## 5 Coch~ Urba~ 30.4 27 2.Mujer ## 6 Coch~ Rural 31.1 27.5 1.Hombre ## 7 Oruro Urba~ 28.7 26 2.Mujer ## 8 Oruro Rural 36.4 34 2.Mujer ## 9 Poto~ Urba~ 26.1 23 2.Mujer ## 10 Poto~ Rural 35.6 34.5 2.Mujer ## 11 Tari~ Urba~ 29.4 27 2.Mujer ## 12 Tari~ Rural 29.7 27 2.Mujer ## 13 Sant~ Urba~ 27.8 25 2.Mujer ## 14 Sant~ Rural 29.9 26 1.Hombre ## 15 Beni Urba~ 26.8 22 2.Mujer ## 16 Beni Rural 26.0 19 1.Hombre ## 17 Pando Urba~ 25.5 23 1.Hombre ## 18 Pando Rural 22.6 18 1.Hombre ## # ... with 1 more variable: `n()` &lt;int&gt; La función n() realiza un proceso de conteo. De forma similar podemos hacer para las medidas de variabilidad, las mas comunes la desviacion estandar, el rango, el rango intercuartil y los cuantiles. Usando la eh18 para la edad y el ingreso laboral. ##EDAD # Desviacion estandar sd(eh18p$s02a_03) ## [1] 20.97181 # Rango range(eh18p$s02a_03) ## [1] 0 98 # Rango intercuartil IQR(eh18p$s02a_03) ## [1] 32 # Quantiles quantile(eh18p$s02a_03) ## 0% 25% 50% 75% 100% ## 0 12 26 44 98 quantile(eh18p$s02a_03,c(0.10,0.90)) ## 10% 90% ## 5 61 ##INGRESO LABORAL # Desviacion estandar sd(eh18p$ylab,na.rm = T) ## [1] 2242.593 # Rango range(eh18p$ylab,na.rm = T) ## [1] 4.166667 36196.667969 # Rango intercuartil IQR(eh18p$ylab,na.rm = T) ## [1] 2397 # Quantiles quantile(eh18p$ylab,na.rm = T) ## 0% 25% 50% 75% ## 4.166667 1500.000000 2554.699951 3897.000000 ## 100% ## 36196.667969 quantile(eh18p$s02a_03,probs=c(0.10,0.90),na.rm = T) ## 10% 90% ## 5 61 ##por departamento y area tapply(eh18p$ylab,list(eh18p$depto,eh18p$area),sd,na.rm=T)#opcion1 ## Urbana Rural ## Chuquisaca 2586.058 1735.306 ## La Paz 2154.421 1703.794 ## Cochabamba 2313.400 1654.140 ## Oruro 2425.280 1622.623 ## Potosí 2308.412 1534.399 ## Tarija 2287.537 2142.638 ## Santa Cruz 2156.938 2134.006 ## Beni 2419.952 1809.108 ## Pando 1778.112 2050.242 tapply(eh18p$ylab,list(eh18p$depto,eh18p$area),quantile,na.rm=T)#con problemas ## Urbana Rural ## Chuquisaca Numeric,5 Numeric,5 ## La Paz Numeric,5 Numeric,5 ## Cochabamba Numeric,5 Numeric,5 ## Oruro Numeric,5 Numeric,5 ## Potosí Numeric,5 Numeric,5 ## Tarija Numeric,5 Numeric,5 ## Santa Cruz Numeric,5 Numeric,5 ## Beni Numeric,5 Numeric,5 ## Pando Numeric,5 Numeric,5 aggregate(eh18p$ylab,list(depto=eh18p$depto,area=eh18p$area),quantile,na.rm=T) ## depto area x.0% x.25% x.50% ## 1 Chuquisaca Urbana 39.583336 1317.083344 2598.000000 ## 2 La Paz Urbana 6.666667 1732.000000 2650.000000 ## 3 Cochabamba Urbana 80.000000 1850.833374 2700.000000 ## 4 Oruro Urbana 15.000000 1608.679932 2598.000000 ## 5 Potosí Urbana 160.000000 1724.750000 3064.550049 ## 6 Tarija Urbana 86.599998 1794.533325 2814.500000 ## 7 Santa Cruz Urbana 80.000000 2121.699951 2976.875000 ## 8 Beni Urbana 50.000000 1729.834961 2500.000000 ## 9 Pando Urbana 250.000000 2475.000000 3291.666748 ## 10 Chuquisaca Rural 36.666668 519.599976 1012.489990 ## 11 La Paz Rural 12.500000 300.000000 825.000000 ## 12 Cochabamba Rural 23.333334 523.649994 1590.000061 ## 13 Oruro Rural 19.166668 286.458336 653.916687 ## 14 Potosí Rural 25.833334 225.000000 516.250000 ## 15 Tarija Rural 4.166667 995.883316 1967.666687 ## 16 Santa Cruz Rural 66.666672 1230.000000 2262.250000 ## 17 Beni Rural 100.000000 705.000000 1800.000000 ## 18 Pando Rural 80.000000 1207.500000 2167.500000 ## x.75% x.100% ## 1 4369.449707 16123.333008 ## 2 3897.000000 23437.484375 ## 3 4000.000000 36196.667969 ## 4 4330.000000 16166.666992 ## 5 4500.000000 14072.500000 ## 6 4330.000000 21833.000000 ## 7 4156.799805 35668.664062 ## 8 4105.924927 22700.000000 ## 9 4538.041748 11367.666016 ## 10 2078.399902 10120.000000 ## 11 2249.166748 15433.333008 ## 12 2598.000000 10825.000000 ## 13 2051.324982 10175.500000 ## 14 1435.208374 7333.333496 ## 15 3366.749939 16730.000000 ## 16 3167.366516 22465.667969 ## 17 3500.000000 7577.500000 ## 18 3449.133362 19750.000000 Finalmente, para explorar a fondo las variables la funcion describe es bastante útil, tambien, el comando summary. #analizando las 5 primeras variables de la base de datos library(Hmisc) describe(eh18p[,1:5]) ## eh18p[, 1:5] ## ## 5 Variables 37517 Observations ## ---------------------------------------------------------------- ## folio ## n missing distinct ## 37517 0 11195 ## ## lowest : 111-00419704629-A-0011 111-00419704629-A-0021 111-00419704629-A-0041 111-00419704629-A-0051 111-00419704629-A-0071 ## highest: 953-11761951198-D-0081 953-11761951198-D-0091 953-11761951198-D-0101 953-11761951198-D-0111 953-11761951198-D-0121 ## ---------------------------------------------------------------- ## nro ## n missing distinct Info Mean Gmd .05 ## 37517 0 13 0.948 2.639 1.712 1 ## .10 .25 .50 .75 .90 .95 ## 1 1 2 4 5 6 ## ## lowest : 1 2 3 4 5, highest: 9 10 11 12 13 ## ## Value 1 2 3 4 5 6 7 8 ## Frequency 11195 9399 7215 4892 2654 1230 536 231 ## Proportion 0.298 0.251 0.192 0.130 0.071 0.033 0.014 0.006 ## ## Value 9 10 11 12 13 ## Frequency 110 39 13 2 1 ## Proportion 0.003 0.001 0.000 0.000 0.000 ## ---------------------------------------------------------------- ## depto ## n missing distinct ## 37517 0 9 ## ## lowest : Chuquisaca La Paz Cochabamba Oruro Potosí ## highest: Potosí Tarija Santa Cruz Beni Pando ## ## Chuquisaca (2117, 0.056), La Paz (9970, 0.266), Cochabamba ## (7578, 0.202), Oruro (2188, 0.058), Potosí (1855, 0.049), ## Tarija (3088, 0.082), Santa Cruz (6561, 0.175), Beni (2516, ## 0.067), Pando (1644, 0.044) ## ---------------------------------------------------------------- ## area ## n missing distinct ## 37517 0 2 ## ## Value Urbana Rural ## Frequency 29212 8305 ## Proportion 0.779 0.221 ## ---------------------------------------------------------------- ## s02a_02 ## n missing distinct ## 37517 0 2 ## ## Value 1.Hombre 2.Mujer ## Frequency 18419 19098 ## Proportion 0.491 0.509 ## ---------------------------------------------------------------- summary(eh18p$ylab) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 4.17 1500.00 2554.70 2959.39 3897.00 36196.67 22852 by(eh18p[,c(&quot;ylab&quot;,&quot;p0&quot;,&quot;s02a_03&quot;)],eh18p$area,summary) ## eh18p$area: Urbana ## ylab p0 s02a_03 ## Min. : 6.67 No Pobre:21225 Min. : 0.00 ## 1st Qu.: 1839.08 Pobre : 7971 1st Qu.:13.00 ## Median : 2788.52 NA&#39;s : 16 Median :26.00 ## Mean : 3238.21 Mean :29.17 ## 3rd Qu.: 4082.50 3rd Qu.:43.00 ## Max. :36196.67 Max. :98.00 ## NA&#39;s :17573 ## ------------------------------------------------ ## eh18p$area: Rural ## ylab p0 s02a_03 ## Min. : 4.167 No Pobre:4049 Min. : 0.00 ## 1st Qu.: 476.392 Pobre :4254 1st Qu.:11.00 ## Median : 1300.000 NA&#39;s : 2 Median :27.00 ## Mean : 1886.938 Mean :31.06 ## 3rd Qu.: 2657.667 3rd Qu.:49.00 ## Max. :22465.668 Max. :98.00 ## NA&#39;s :5279 2.1.2 Visualización La visualizacion es una herramienta importante para explorar y entender la base de datos. Los seres humanos son excelentes para capturar patrones visuales, y la visualización de datos intenta capitalizar en estas habilidades. Es util diferenciar las visualizaciones por: Una sola varibles Dos variables Multiples variables Los aspetor vinculados al uso de graficos de origen de R y la libreria ggplot pueden verse en el texto guia de EST-183 “BigData”. A continuación se introducen de forma directa funciones en R orientadas a la visualización univariante, bivariante y multivariante. Usando al EH-2018, para variables cualitativas. #Graficos de origen de R barplot(table(eh18p$s03a_01a),main=&quot;Dónde vivia hace 5 años?&quot;) #GGPLOT library(ggplot2) ggplot(eh18p,aes(x=s03a_01a))+geom_bar()+ggtitle(&quot;Dónde vivia hace 5 años?&quot;) Para variables del tipo cuantitativas par(mfrow=c(1,2)) boxplot(eh18p$ylab) hist(eh18p$ylab) dev.off() ## null device ## 1 ggplot(eh18p,aes(ylab))+geom_boxplot() ## Warning: Removed 22852 rows containing non-finite values ## (stat_boxplot). ggplot(eh18p,aes(ylab))+geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. ## Warning: Removed 22852 rows containing non-finite values ## (stat_bin). Si ahora queremos comparar usar ambas variables cuanti y cuali boxplot(eh18p$ylab~eh18p$s03a_01a) ggplot(eh18p,aes(x=s03a_01a,y=ylab))+geom_boxplot() ## Warning: Removed 22852 rows containing non-finite values ## (stat_boxplot). ggplot(eh18p,aes(x=s03a_01a,y=ylab))+geom_violin() ## Warning: Removed 22852 rows containing non-finite values ## (stat_ydensity). Usando ambas variables cuantitativas plot(eh18p$tothrs,eh18p$ylab) plot(eh18p[,c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)]) #pairs(eh18p[,c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)]) similar al anterior library(GGally) ggpairs(eh18p,columns = c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)) library(ggridges) # basic example ggplot(eh18p[eh18p$s02a_03&gt;=15,], aes(x = ylab, y = s02a_02, fill = s02a_02)) + geom_density_ridges() + theme_ridges() + theme(legend.position = &quot;none&quot;)+ ggtitle(&quot;Ingreso laboral por sexo, personas de 15 años o más&quot;) Ahora si combinamos variables cuanti y cuali con ggpairs. ggpairs(eh18p,columns = c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;s03a_01a&quot;,&quot;area&quot;)) Alternativas Multivariantes, #trabajando a partir de una muestra de 100 individuos s&lt;-sample(1:dim(eh18p)[1],100) i&lt;-match(c(&quot;s02a_03&quot;,&quot;aestudio&quot;,&quot;ylab&quot;,&quot;tothrs&quot;),names(eh18p)) ggparcoord(eh18p[s,],columns = i,groupColumn = &quot;area&quot;,boxplot=T) library(&quot;TeachingDemos&quot;) faces(na.omit(eh18p[s,i])) 2.2 Componentes Principales El método de Análisis de Componentes Principales se ocupa de explicar la estructura de varianza y covarianza de un grupo de variables a través de unas pocas combinaciones lineales de este grupo de variables. En general sus objetivos son (1) la reducción de los datos y (2) la interpretación. Algebráicamente, los componentes principales son combinaciones lineales de \\(p\\) variables aleatorias \\(X_1\\), \\(X_2\\), , \\(X_p\\). Geométricamente, estas combinaciones lineales representan la selección de un nuevo sistema de coordenadas obtenido por rotación de del sistema original con \\(X_1\\), \\(X_2\\), , \\(X_p\\) como los ejes de coordenadas. Los nuevos ejes representan la dirección con la máxima variabilidad y provee una simple y más parsimoniosa descripción de la estructura de la covarianza. Los componentes principales dependen únicamente de la matriz de covarianza \\(\\Sigma\\) o la matriz de correlaciones \\(\\rho\\) (Matriz estandarizada de \\(\\Sigma\\)) de \\(X_1\\), \\(X_2\\), , \\(X_p\\). Su desarrollo no requiere de ningún supuesto de normalidad multivariada, sin embargo, componentes principales derivados de poblaciones normales multivariantes tienen un gran uso en la interpretación en términos de elipsoide de densidad constante. Sea la matriz \\(\\mathbf{X}\\) compuesta de \\(p\\) vectores aleatorios \\(\\mathbf{X}=[X_1, X_2, \\ldots, X_p ]\\) que tiene la matriz de covarianza \\(\\Sigma\\) con valores propios \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_p \\geq 0\\). Considere la combinación lineal: \\[\\begin{equation} \\begin{array}{rrr} Y_1 = &amp; a_1^{&#39;} \\mathbf{X} = &amp; a_{11} X_1 + a_{12} X_2 + \\ldots a_{1p} X_p \\\\ Y_2 = &amp; a_2^{&#39;} \\mathbf{X} = &amp; a_{21} X_1 + a_{22} X_2 + \\ldots a_{2p} X_p\\\\ \\vdots = &amp; \\vdots &amp; \\vdots \\\\ Y_p = &amp; a_p^{&#39;} \\mathbf{X} = &amp; a_{p1} X_1 + a_{p2} X_2 + \\ldots a_{pp} X_p\\\\ \\end{array} \\label{cp1} \\end{equation}\\] Equivalente a: \\[\\begin{equation} \\mathbf{Y}= \\left[ \\begin{array}{c} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_p\\\\ \\end{array} \\right] = \\left[ \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1p} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{21} &amp; a_{p2} &amp; \\ldots &amp; a_{pp} \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} X_1\\\\ X_2\\\\ \\vdots\\\\ X_p\\\\ \\end{array} \\right] = \\mathbf{A X} \\label{cp2} \\end{equation}\\] La combinación lineal \\(\\mathbf{Y}=\\mathbf{AX}\\) tiene: \\[\\begin{equation} \\mu_y=E(\\mathbf{Y})=E(\\mathbf{AX})=A \\mu_x \\label{cp3} \\end{equation}\\] \\[\\begin{equation} \\Sigma_y=Cov(\\mathbf{Y})=Cov(\\mathbf{AX})=A \\Sigma A^{&#39;} \\label{cp4} \\end{equation}\\] En base a , se obtiene: \\[\\begin{equation} Var(Y_i)=a_i^{&#39;} \\Sigma a_i \\quad i=1, 2, \\ldots, p \\label{cp5} \\end{equation}\\] \\[\\begin{equation} Cov(Y_i,Y_k)=a_i^{&#39;} \\Sigma a_k \\quad i,k=1, 2, \\ldots, p \\label{cp6} \\end{equation}\\] Los componentes principales son combinaciones lineales incorrelacionadas, tal que es lo más grande posible. El primer componente principal es la combinación lineal con máxima varianza. Entonces se debe maximizar \\(Var(Y_1)=a_1^{&#39;} \\Sigma a_1\\). Es claro que \\(Var(Y_1)\\) puede ser incrementada multiplicando a \\(a_1\\) por alguna constante. Para eliminar esta indeterminación, es conveniente restringir los coeficientes del vector. Por lo tanto se define. \\[ \\begin{array}{rcl} \\text{Primer componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_1^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_1^{&#39;}X) \\quad \\text{sujeto a} \\quad a_1^{&#39;} a_1=1\\\\ \\text{Segundo componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_2^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_2^{&#39;}X) \\quad \\text{sujeto a} \\quad a_2^{&#39;} a_2=1 \\quad y \\\\ &amp; &amp; Cov(a_1^{&#39;}X,a_2^{&#39;}X)=0 \\end{array} \\] Para el \\(i-esimo\\) paso: \\[ \\begin{array}{rcl} i-esimo \\text{ componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_i^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_i^{&#39;}X) \\quad \\text{sujeto a} \\quad a_i^{&#39;} a_i=1 \\quad y \\\\ &amp; &amp; Cov(a_i^{&#39;}X,a_k^{&#39;}X)=0 \\quad para \\quad k&lt;i \\end{array} \\] Los pasos sugeridos para iniciar el analisis de componentes principales son: Identificar las variables de interés dentro de la matriz de datos, si las variables tienen las mismas escalas se recomienda emplear la matriz de covarianza, si las escalas son diferentes, se recomienda trabajar con la matriz de correlaciones. Obtener los componentes principales, los eigen valores y la matriz de eigen vectores Eliminar las variables rebundantes, se sugiere identificar las variables del conjunto de datos correlacionadas con los últimos componentes Calcular nuevamente los componentes principales expluyendo las variables identificadas en el paso previo Elegir el número de componentes a retener (scree plot, tamaño de los eigen valores, etc) Analizar los resultados #1. Seleccione una base de datos de interés del repositorio load(url(&quot;https://github.com/AlvaroLimber/EST-384/blob/master/data/oct20.RData?raw=true&quot;)) #2. Seleccione las variables para el PCA (Según la motivación) vv&lt;-c(14:22,24,25) #2A TRANFORMAR vval&lt;-apply(computo[,vv],1,sum) aux&lt;-computo[,vv]/vval #2B LIMPIEZA #3. Calcule el PCA aux1&lt;-na.omit(aux) cp1&lt;-eigen(cov(aux1)) cp2&lt;-eigen(cor(aux1)) #4. Identifique el número de CPs que explican hasta el 90% de la varianza cumsum(cp1$values)/sum(cp1$values) ## [1] 0.6437488 0.8650934 0.9435882 0.9722772 0.9839750 0.9903314 ## [7] 0.9952079 0.9977119 0.9991579 1.0000000 1.0000000 cumsum(cp2$values)/sum(cp2$values) ## [1] 0.2060107 0.3791103 0.4915437 0.5920446 0.6774866 0.7601857 ## [7] 0.8319256 0.8991550 0.9562490 1.0000000 1.0000000 #5. Identifique las variables correlacionadas con la cantidad de #componentes fuera del 90% del paso anterior cp11cov&lt;-as.matrix(aux1)%*%cp1$vectors[,11] cp11cor&lt;-as.matrix(aux1)%*%cp2$vectors[,11] cor(cbind(aux1,cp11cov,cp11cor))[1:11,12:13] ## cp11cov cp11cor ## CC 0.53979440 -0.30842297 ## FPV -0.55376059 0.21633850 ## MTS 0.13333915 0.40515623 ## UCS -0.34677314 0.35428285 ## MAS - IPSP -0.21854796 -0.41971569 ## 21F -0.38433159 0.53263368 ## PDC -0.12174043 -0.01216537 ## MNR -0.21628303 0.41542663 ## PAN-BOL 0.11710134 0.09577694 ## Blancos -0.07666553 0.50453525 ## Nulos -0.11039942 0.18712933 cp11&lt;-eigen(cov(aux1[,-2])) cumsum(cp11$values)/sum(cp11$values) ## [1] 0.6443329 0.8658697 0.9444240 0.9730984 0.9847943 0.9911498 ## [7] 0.9960295 0.9984949 0.9999206 1.0000000 cp10cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,10] #6. Calcule nuevamente el componente principal eliminando las variables rebundantes #7. Determine la cantidad de componentes principales a retener cp1cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,1] cp2cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,2] plot(cp1cov,cp2cov) #8. Defina un indicador a partir de estos cor(cbind(aux1,cp1cov,cp2cov))[1:11,12:13] ## cp1cov cp2cov ## CC 0.92113465 0.35759774 ## FPV -0.08074887 -0.10802564 ## MTS -0.16786500 -0.30855895 ## UCS 0.14139049 -0.22216012 ## MAS - IPSP -0.92337643 0.36777758 ## 21F 0.32455320 -0.47209833 ## PDC -0.09883288 0.43937850 ## MNR 0.19561852 -0.25724983 ## PAN-BOL 0.04776390 -0.01901819 ## Blancos -0.17330606 -0.88168972 ## Nulos -0.08874645 0.06391096 bd&lt;-cbind(aux1,cp1cov,cp2cov) names(bd)[5]&lt;-&quot;MAS&quot; #9. Modele un modelo lineal empleando los CPs retenidos. summary(lm(MAS~cp1cov,data=bd)) ## ## Call: ## lm(formula = MAS ~ cp1cov, data = bd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.39544 -0.05881 0.01417 0.06187 0.16168 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3407735 0.0003141 1085.0 &lt;2e-16 *** ## cp1cov -0.7053686 0.0011240 -627.5 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07868 on 68069 degrees of freedom ## Multiple R-squared: 0.8526, Adjusted R-squared: 0.8526 ## F-statistic: 3.938e+05 on 1 and 68069 DF, p-value: &lt; 2.2e-16 2.3 Análisis de correspondencia El analisis de correspondencia esta orientado a encontrar relaciones entre las categorias de variables cualitativas. Esta técnica es un método visual que va más alla del test de independencia Chi-cuadrado. Para resumir la teoría, primero divida la matriz de datos \\(I × J\\), denotada por \\(N\\), por su gran total \\(n\\) para obtener la llamada matriz de correspondencia \\(P = N / n\\). Deje que los totales marginales de fila y columna de \\(P\\) sean los vectores \\(r\\) y \\(c\\) respectivamente, es decir, los vectores de masas de fila y columna, y \\(Dr\\) y \\(Dc\\) sean las matrices diagonales de estas matrices. El algoritmo computacional para obtener coordenadas de los perfiles de fila y columna con respecto a los ejes principales, usando el SVD, es el siguiente: Calcular la matriz de residuos estadarizados: \\(S=D_r^{-1/2}(P-rc^t)D_c^{-1/2}\\) Calcular la descomposición SVD de \\(S\\): \\(S=UD_{\\alpha}V^t\\), donde \\(U^T U=V^T V=I\\) Coordenadas principales de filas: \\(F=D_r^{-1/2} U D_{\\alpha}\\) Coordenadas principales de columnas: \\(G=D_c^{-1/2} V D_{\\alpha}\\) Coordenadas estándar de filas: \\(X=D_r^{-1/2} U\\) Coordenadas estándar de columnas: \\(Y=D_c^{-1/2} V\\) Calcular la Inercia: \\[\\phi^2=\\sum_i^I\\sum_j^J{\\frac{(p_{ij}-r_i c_j)^2}{r_i c_j}}\\] Graficar las coordenadas de F y G según la la inercia contenida en la matriz \\(D_{\\alpha}\\) En R, existe la libreria ca que permite acceder a las coordenadas del método de correspondencia. #ejemplo CA #install.packages(&quot;ca&quot;) library(dplyr) library(ca) data(&quot;smoke&quot;) model&lt;-ca(smoke) plot(model) names(model) ## [1] &quot;sv&quot; &quot;nd&quot; &quot;rownames&quot; &quot;rowmass&quot; ## [5] &quot;rowdist&quot; &quot;rowinertia&quot; &quot;rowcoord&quot; &quot;rowsup&quot; ## [9] &quot;colnames&quot; &quot;colmass&quot; &quot;coldist&quot; &quot;colinertia&quot; ## [13] &quot;colcoord&quot; &quot;colsup&quot; &quot;N&quot; &quot;call&quot; summary(model) ## ## Principal inertias (eigenvalues): ## ## dim value % cum% scree plot ## 1 0.074759 87.8 87.8 ********************** ## 2 0.010017 11.8 99.5 *** ## 3 0.000414 0.5 100.0 ## -------- ----- ## Total: 0.085190 100.0 ## ## ## Rows: ## name mass qlt inr k=1 cor ctr k=2 cor ctr ## 1 | SM | 57 893 31 | -66 92 3 | -194 800 214 | ## 2 | JM | 93 991 139 | 259 526 84 | -243 465 551 | ## 3 | SE | 264 1000 450 | -381 999 512 | -11 1 3 | ## 4 | JE | 456 1000 308 | 233 942 331 | 58 58 152 | ## 5 | SC | 130 999 71 | -201 865 70 | 79 133 81 | ## ## Columns: ## name mass qlt inr k=1 cor ctr k=2 cor ctr ## 1 | none | 316 1000 577 | -393 994 654 | -30 6 29 | ## 2 | lght | 233 984 83 | 99 327 31 | 141 657 463 | ## 3 | medm | 321 983 148 | 196 982 166 | 7 1 2 | ## 4 | hevy | 130 995 192 | 294 684 150 | -198 310 506 | #ejemplo ENDSA load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/endsa.RData&quot;)) ll&lt;-attributes(endsa) ll$var.labels ## [1] &quot;Cluster&quot; ## [2] &quot;Departamento&quot; ## [3] &quot;Área&quot; ## [4] &quot;Año&quot; ## [5] &quot;Edad&quot; ## [6] &quot;Sexo&quot; ## [7] &quot;Nivel de educación&quot; ## [8] &quot;Años de educación en el ciclo&quot; ## [9] &quot;Total de años de educación&quot; ## [10] &quot;Alfabetismo&quot; ## [11] &quot;Lee periodicos o revistas en la semana?&quot; ## [12] &quot;Escucha radio en la semana&quot; ## [13] &quot;Mira televisión en la semana&quot; ## [14] &quot;Índice de riqueza&quot; ## [15] &quot;Fuma&quot; ## [16] &quot;Conocimiento de algun método anticonceptivo&quot; ## [17] &quot;Uso de algún método anticonceptivo&quot; ## [18] &quot;Conocimiento del periodo fertil&quot; ## [19] &quot;Método comunmente usado&quot; ## [20] &quot;Edad al momento de la esterilización&quot; ## [21] &quot;A oido del sida?&quot; ## [22] &quot;Conoce maneras de evitar el VIH SIDA&quot; ## [23] &quot;Estado civil actual&quot; ## [24] &quot;Edad al primer matrimonio&quot; ## [25] &quot;Edad a la primera relación sexual&quot; ## [26] &quot;Número de uniones&quot; ## [27] &quot;Total de niños nacidos&quot; ## [28] &quot;Hijos en casa&quot; ## [29] &quot;Hijas en casa&quot; ## [30] &quot;Hijos en otro lugar&quot; ## [31] &quot;Hijas en otro lugar&quot; ## [32] &quot;Hijos que han muerto&quot; ## [33] &quot;Hijas que han muerto&quot; ## [34] &quot;Edad al primer nacimiento&quot; ## [35] &quot;A oido hablar de la tuberculosis o tb&quot; ## [36] &quot;Mantendría en secreto si alguien de su familia tuviera tb&quot; ## [37] &quot;Número ideal de niños&quot; ## [38] &quot;Número ideal de hijos&quot; ## [39] &quot;Número ideal de hijas&quot; ## [40] &quot;Nivel de educación de la pareja&quot; ## [41] &quot;Años de educación en el ciclo (de la pareja)&quot; ## [42] &quot;Ocupación de la pareja&quot; ## [43] &quot;Actualmente trabaja?&quot; ## [44] &quot;Educación en años simples de la pareja&quot; ## [45] &quot;Ocupación&quot; ## [46] &quot;Trabajó en los ultimos 12 meses&quot; ## [47] &quot;Edad de la pareja&quot; ## [48] &quot;Regularidad del trabajo&quot; ## [49] &quot;Quién decide como gastar el dinero?&quot; ## [50] &quot;(Psi) Acusan de ser infiel&quot; ## [51] &quot;(Psi) Limitan contacto con su familia&quot; ## [52] &quot;(PSI) Expresiones ofensivas&quot; ## [53] &quot;(PSI) Amenaza con irse &quot; ## [54] &quot;(PSI) Amenaza con no cumplir responsabilidad economica&quot; ## [55] &quot;(Fis) Empujado jaloneado&quot; ## [56] &quot;(Fis) Golpeado con mano o pie&quot; ## [57] &quot;(Fis) Golpeado con objeto duro&quot; ## [58] &quot;(Fis) Tratado de estrangular&quot; ## [59] &quot;(Fis) Forzado a tener relaciones&quot; ## [60] &quot;&quot; ## [61] &quot;Factor de expansión&quot; t1&lt;-endsa %&gt;% filter(year==2008) %&gt;% select(7,14) %&gt;% table() t1&lt;-t1[1:4,] #test chi2 chisq.test(t1) ## ## Pearson&#39;s Chi-squared test ## ## data: t1 ## X-squared = 8367.8, df = 12, p-value &lt; 2.2e-16 model&lt;-ca(t1) model ## ## Principal inertias (eigenvalues): ## 1 2 3 ## Value 0.323708 0.038805 0.001415 ## Percentage 88.95% 10.66% 0.39% ## ## ## Rows: ## Sin educación Con educación primaria ## Mass 0.036794 0.397860 ## ChiDist 0.999518 0.564015 ## Inertia 0.036758 0.126564 ## Dim. 1 -1.589674 -0.986443 ## Dim. 2 1.986439 0.251096 ## Con educación secundaria Con educación superior ## Mass 0.366807 0.198539 ## ChiDist 0.326543 0.901890 ## Inertia 0.039113 0.161492 ## Dim. 1 0.406081 1.521130 ## Dim. 2 -1.168372 1.287294 ## ## ## Columns: ## Más pobre Pobre Clase media Rico Más rico ## Mass 0.168964 0.177837 0.198713 0.218066 0.236420 ## ChiDist 0.869550 0.471835 0.232894 0.323342 0.830339 ## Inertia 0.127757 0.039591 0.010778 0.022799 0.163003 ## Dim. 1 -1.460754 -0.818013 -0.185957 0.436418 1.413047 ## Dim. 2 1.281961 -0.168562 -1.053040 -1.037739 1.052869 plot(model) #programando el ca lcol&lt;-colnames(t1) lrow&lt;-rownames(t1) P&lt;-prop.table(t1) r&lt;-margin.table(P,1) c&lt;-margin.table(P,2) Dr&lt;-diag(r) Dc&lt;-diag(c) ##Paso 1 P-r%*%t(c) ## ae08 ## ae01 Más pobre Pobre ## Sin educación 0.012441006 0.002372467 ## Con educación primaria 0.057509375 0.035321601 ## Con educación secundaria -0.038100566 -0.011824251 ## Con educación superior -0.031849815 -0.025869816 ## ae08 ## ae01 Clase media Rico ## Sin educación -0.001657500 -0.005240036 ## Con educación primaria 0.003921918 -0.027698479 ## Con educación secundaria 0.014659135 0.028348987 ## Con educación superior -0.016923553 0.004589529 ## ae08 ## ae01 Más rico ## Sin educación -0.007915937 ## Con educación primaria -0.069054414 ## Con educación secundaria 0.006916695 ## Con educación superior 0.070053656 #error en las matriz diagonales Dr^(-0.5)%*%(P-r%*%t(c))%*% Dc^(-0.5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] NaN NaN NaN NaN NaN ## [2,] NaN NaN NaN NaN NaN ## [3,] NaN NaN NaN NaN NaN ## [4,] NaN NaN NaN NaN NaN # corrigiendo el problema S&lt;-diag(r^(-0.5))%*%(P-r%*%t(c))%*% diag(c^(-0.5)) # 2 descomposición SVD svd(S) ## $d ## [1] 5.689533e-01 1.969900e-01 3.761651e-02 8.641056e-17 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] -0.3049268 0.3810330 0.8514926 0.1918171 ## [2,] -0.6222107 0.1583814 -0.4357855 0.6307616 ## [3,] 0.2459415 -0.7076197 0.2682905 0.6056462 ## [4,] 0.6777804 0.5735882 -0.1143309 0.4455768 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] -0.60044732 0.52695380 0.43904427 -0.4084045 ## [2,] -0.34496166 -0.07108381 -0.78620333 -0.3924945 ## [3,] -0.08289454 -0.46941592 0.04584658 -0.5165641 ## [4,] 0.20379664 -0.48459901 0.41307289 -0.4090435 ## [5,] 0.68706615 0.51193680 -0.12803642 -0.4949736 U&lt;-svd(S)$u V&lt;-svd(S)$v Da&lt;-diag(svd(S)$d) #verificando las propiedades U %*% t(U) ## [,1] [,2] [,3] [,4] ## [1,] 1.000000e+00 1.942890e-16 -8.326673e-17 -5.551115e-17 ## [2,] 1.942890e-16 1.000000e+00 -1.110223e-16 5.551115e-17 ## [3,] -8.326673e-17 -1.110223e-16 1.000000e+00 1.665335e-16 ## [4,] -5.551115e-17 5.551115e-17 1.665335e-16 1.000000e+00 t(V) %*% V ## [,1] [,2] [,3] [,4] ## [1,] 1.000000e+00 2.220446e-16 2.081668e-16 5.551115e-17 ## [2,] 2.220446e-16 1.000000e+00 2.914335e-16 5.551115e-17 ## [3,] 2.081668e-16 2.914335e-16 1.000000e+00 8.326673e-17 ## [4,] 5.551115e-17 5.551115e-17 8.326673e-17 1.000000e+00 U %*% Da %*% t(V) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.1577867 0.02932932 -0.01938445 -0.05849955 -0.08487368 ## [2,] 0.2218072 0.13278970 0.01394827 -0.09403647 -0.22515639 ## [3,] -0.1530435 -0.04629605 0.05429710 0.10023611 0.02348755 ## [4,] -0.1738948 -0.13767657 -0.08520326 0.02205724 0.32334513 S ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.1577867 0.02932932 -0.01938445 -0.05849955 -0.08487368 ## [2,] 0.2218072 0.13278970 0.01394827 -0.09403647 -0.22515639 ## [3,] -0.1530435 -0.04629605 0.05429710 0.10023611 0.02348755 ## [4,] -0.1738948 -0.13767657 -0.08520326 0.02205724 0.32334513 # 3 Coordenadas principales filas FF&lt;- diag(r^(-0.5)) %*% U %*% Da # 4 Coordenadas principales columnas G&lt;- diag(c^(-0.5)) %*% V %*% Da # 5 Coordenadas estandar filas X&lt;- diag(r^(-0.5)) %*% U # 6 Coordenadas estandar columnas Y&lt;- diag(c^(-0.5)) %*% V # 7 inercia sum(((P-r%*%t(c))**2)/(r%*%t(c))) ## [1] 0.3639279 #graficando xmin&lt;-min(c(FF[,1],G[,1])) xmax&lt;-max(c(FF[,1],G[,1])) ymin&lt;-min(c(FF[,2],G[,2])) ymax&lt;-max(c(FF[,2],G[,2])) plot(FF[,1],FF[,2],col=&quot;red&quot;,xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1) points(G[,1],G[,2],col=&quot;blue&quot;) abline(h=0,v=0,lty=2) #incluyendo el texto plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = &quot;n&quot;) text(FF[,1],FF[,2],labels = lrow,col=&quot;red&quot;,cex=0.7) text(G[,1],G[,2],labels = lcol,col=&quot;blue&quot;,cex=0.7) abline(h=0,v=0,lty=2) #incluyendo mas información plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = &quot;n&quot;) text(FF[,1],FF[,2],labels = lrow,col=&quot;red&quot;,cex=0.5+r*2) text(G[,1],G[,2],labels = lcol,col=&quot;blue&quot;,cex=0.5+c*2) abline(h=0,v=0,lty=2) #viendo solo una dimensión plot(rep(0,dim(FF)[1]),FF[,1],type = &quot;n&quot;, axes = F) axis(2) text(rep(0,dim(FF)[1]),FF[,1],labels = lrow,col=&quot;red&quot;,cex=0.5+r*2) text(rep(0,dim(G)[1]),G[,1],labels = lcol,col=&quot;blue&quot;,cex=0.5+c*2) abline(h=0,v=0,lty=2) "],
["clustering.html", "3 Clustering 3.1 Medidas de Disimilaridad 3.2 Métodos de clustering 3.3 K-center Clustering (no jerárquicos) 3.4 Cluster Jerárquico 3.5 Ejercicios", " 3 Clustering El clustering es un método cuyo objetivo es el de crear grupos en base a las relaciones multivariantes que existen en los datos, este método es un método previo a las técnicas de clasificación que existen. La base del clustering es la definición de la similaridad entre las filas. Similaridad es definida como una función de distancia entre un par de filas. Es importante distinguir la existencia de grupos naturales dentro de los datos, normalmente estos grupos son características naturales de las observaciones de interés. 3.1 Medidas de Disimilaridad Dado el objetivo del clustering, el aspecto mas importante dentro de estos métodos es utilizar de forma correcta la medida de (di)similaridad entre un para de casos dentro de la base de datos. La definición de las medidas de distancia es crucial para aplicar estos modelos. Funciones de distancia incorrecta pueden generar sesgos en los resultados y ser un problema para etapas posteriores de la mineria de datos. Debemos distinguir las funciones de distancia segun la naturaleza de las variables. Sean las filas \\(x\\) e \\(y\\) dentro de una base de datos, estos vectores tienen una dimensión \\(p\\), es decir, se observan \\(p\\) variables para las 2 observaciones. 3.1.1 Distancia Euclideana: Variables numéricas \\[d(x,y)=\\sqrt{\\sum_{i=1}^p{(x_i-y_i)^2}}\\] Donde los \\(x_i\\) y \\(_y_i\\) son los valores para la variable \\(i\\) de las observaciones \\(x\\) e \\(y\\). 3.1.2 Distancia Manhattan: \\(p\\) grande \\[d(x,y)=\\sum_{i=1}^p{|x_i-y_i|}\\] 3.1.3 Distancia Minkowski \\[d(x,y)=\\left(\\sum_{i=1}^p{|x_i-y_i|^d}\\right)^{1/d}\\] aux&lt;-matrix(rnorm(100),nrow=5) dist(aux) #euclideana ## 1 2 3 4 ## 2 5.966749 ## 3 6.566555 5.536907 ## 4 6.498146 6.322441 5.733616 ## 5 5.916694 6.096577 6.773793 6.143012 dist(aux, method=&quot;manhattan&quot;) ## 1 2 3 4 ## 2 23.46720 ## 3 22.08197 22.72832 ## 4 24.53411 20.48697 21.46772 ## 5 20.50470 20.27724 22.05167 22.53056 dist(aux, method=&quot;minkowski&quot;, p=3) ## 1 2 3 4 ## 2 3.979295 ## 3 4.799798 3.624081 ## 4 4.355454 4.592381 3.951028 ## 5 4.315126 4.416634 4.954648 4.194482 3.1.4 programando minkowski&lt;-function(x,y,d){ dd&lt;-(sum(abs(x-y)**d))**(1/d) return(dd) } minkowski(c(1,2,3),c(4,2,1),d=2) ## [1] 3.605551 x&lt;-c(1,2,3) y&lt;-c(4,2,1) sum(abs(x-y)) # manhattan ## [1] 5 sqrt(sum(abs(x-y)**2)) # euclideana ## [1] 3.605551 # la funcion de distancia distancia&lt;-function(bd,d=2){ nf&lt;-dim(bd)[1] DD&lt;-matrix(NA,nf-1,nf-1) colnames(DD)&lt;-1:(nf-1) rownames(DD)&lt;-2:nf for(i in 1:(nf-1)){ for(j in (i+1):nf){ DD[j-1,i]&lt;-minkowski(bd[i,],bd[j,],d) } } return(DD) } distancia(aux,d=2) ## 1 2 3 4 ## 2 5.966749 NA NA NA ## 3 6.566555 5.536907 NA NA ## 4 6.498146 6.322441 5.733616 NA ## 5 5.916694 6.096577 6.773793 6.143012 dist(aux) ## 1 2 3 4 ## 2 5.966749 ## 3 6.566555 5.536907 ## 4 6.498146 6.322441 5.733616 ## 5 5.916694 6.096577 6.773793 6.143012 3.1.5 Variables cualitativas Para las variables cualitativas se debe considerar los casos cuando estas son nominales y ordinales, distinguir tambien los casos de variables binarias. install.packages(&quot;vegan&quot;) library(vegan) 3.1.6 Datos mixtos Una de los mayores desafios es cuando las variables son mixtas, es decir cuantitativas y cualitativas. install.packages(&quot;cluster&quot;) library(cluster) 3.2 Métodos de clustering Partición (k-center) Jerárquicos (dendograma) Basados en densidad Basados en cuadrículas (grid) 3.3 K-center Clustering (no jerárquicos) Algoritmo Partición de las observaciones en \\(k\\) grupos, obtener el vector de promedios de cada grupo (centroides). Se puede trabajar con la media o la mediana. Para cada observación calcular las distancia euclideana a los centroides y reasignar lo observación en base a la menor distancia, recalcular los centroides en base a la reasignación de cada observación Repetir el paso 2 hasta que que ya no existan más reasignaciones bd&lt;-data.frame(x=rnorm(100),y=rnorm(100)) kmeans(bd,2) ## K-means clustering with 2 clusters of sizes 38, 62 ## ## Cluster means: ## x y ## 1 0.9397322 -0.08140439 ## 2 -0.7152533 0.16054516 ## ## Clustering vector: ## [1] 1 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 1 2 2 2 2 1 1 2 1 1 ## [30] 1 2 1 1 2 1 1 1 2 1 2 2 2 2 2 1 2 2 1 2 2 2 1 2 2 2 2 2 1 ## [59] 2 2 2 2 2 2 1 1 1 1 2 2 1 2 1 2 2 2 2 2 2 2 2 1 2 1 1 2 2 ## [88] 1 2 1 1 1 2 1 2 1 2 2 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 48.80637 69.30803 ## (between_SS / total_SS = 35.8 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; Ejemplo: Implementar el algoritmo para el k-center, con la distancia de Minkowski y para la media y mediana. #Nota: La entrada de la funcion es un data frame kcenter&lt;-function(bd,k=3,d=2,tipo=&quot;media&quot;,seed=123456){ nf&lt;-dim(bd)[1] nc&lt;-dim(bd)[2] #paso1: asignar las k (nf&gt;=k) set.seed(seed) bd$k&lt;-sample(1:k,nf,replace=T) centroide&lt;-NULL for(i in 1:k){ if(tipo==&quot;media&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, mean)) } else if(tipo==&quot;mediana&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, median)) } } #paso2 (recalcular loo centroides al final) cc&lt;-1 while(cc!=0){ #paso3 cc&lt;-0 for(i in 1:nf){ auxd&lt;-NULL for(j in 1:k){ auxd&lt;-c(auxd,minkowski(bd[i,1:nc],centroide[j,1:nc],d=d)) } newk&lt;-which(auxd==min(auxd)) if(newk!=bd$k[i]){ bd$k[i] &lt;- newk cc&lt;-cc+1 } } centroide&lt;-NULL for(i in 1:k){ if(tipo==&quot;media&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, mean)) } else if(tipo==&quot;mediana&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, median)) } } } return(bd) } Pensar en un gráfico que permita ver como se asignaron los cluster bd&lt;-data.frame(x=rnorm(100),y=rnorm(100),z=rnorm(100)) kcenter(bd,k=4,d=1,tipo=&quot;mediana&quot;) ## x y z k ## 1 0.381802373 -0.053229948 -1.33069666 1 ## 2 1.088203468 0.651068275 -0.51990562 1 ## 3 -0.249899370 0.577879654 -0.53684245 1 ## 4 -0.287061038 1.068886636 0.56202515 4 ## 5 0.152321073 -0.541126321 1.57207945 4 ## 6 -1.447260563 -1.389559097 -0.18504876 3 ## 7 0.647898062 0.788820556 -0.37365674 1 ## 8 -0.264433035 -0.905845713 2.08211187 4 ## 9 -0.860485191 -0.630972133 -0.36169358 3 ## 10 -1.453193429 -0.833252152 -1.26987888 3 ## 11 -0.291926101 0.095854154 0.20827789 4 ## 12 1.571771949 2.633014802 -2.13405302 1 ## 13 0.620970684 -1.202029022 0.34157750 2 ## 14 -0.331493636 1.701489170 -0.23685776 1 ## 15 -1.020997153 1.855017145 -1.68995248 3 ## 16 0.733647119 0.928409612 -0.33299127 1 ## 17 -1.620931403 -0.207714201 -1.97237487 3 ## 18 0.272319590 -1.084278021 0.32347897 2 ## 19 -1.089094834 -1.432574060 -1.18747007 3 ## 20 -0.555439460 0.080514448 -0.04146018 3 ## 21 -0.005664276 -1.672591724 -0.37752704 2 ## 22 -0.498191835 -0.906426539 -0.48704394 3 ## 23 -0.149332062 -0.381132108 0.74999059 4 ## 24 0.603328319 -0.355708201 -0.52016779 1 ## 25 0.343017111 -0.670644603 -0.92679810 2 ## 26 0.261599616 1.197750875 -1.63198178 1 ## 27 1.256879504 1.096018207 -0.63572820 1 ## 28 -1.319411050 1.152219723 0.03437943 3 ## 29 -1.145994689 -0.137986482 1.19705215 4 ## 30 0.827723674 -1.318264630 0.85542603 2 ## 31 1.761442961 -1.072779960 -0.43011669 2 ## 32 0.226232884 0.010988954 0.90888514 4 ## 33 -0.919313593 0.127986535 -0.14275284 3 ## 34 0.973750600 0.920828413 -0.34264321 1 ## 35 -1.681186318 -0.001203942 0.75500698 4 ## 36 2.469876944 -1.685660550 0.63603701 2 ## 37 -0.273045676 -1.635627846 -0.83658014 2 ## 38 0.099297412 1.721431702 0.76140670 4 ## 39 0.458142870 -0.097750991 0.51271860 4 ## 40 -0.272781845 0.337879191 -0.25923560 3 ## 41 -2.514060537 -0.611279040 -2.03943542 3 ## 42 -0.011615951 -0.191042266 -0.65238465 3 ## 43 -1.056791014 0.069294449 -1.02857875 3 ## 44 -1.163943943 0.096350840 -1.39774048 3 ## 45 1.028535736 0.170543515 -0.62301238 1 ## 46 1.028717005 -0.934816606 0.62404250 2 ## 47 -0.816360618 0.146111527 0.56478733 4 ## 48 0.481664743 -0.709814901 0.51989056 2 ## 49 0.012558653 0.267408675 0.13340814 4 ## 50 0.692538746 1.196092579 -0.14898525 1 ## 51 0.649157835 -0.340278834 0.51721618 4 ## 52 -0.680769186 -1.590346377 -1.68011128 3 ## 53 0.116653727 -1.071994714 0.56733084 2 ## 54 -0.372278115 -1.548392289 0.21420746 2 ## 55 -0.413571375 1.168114323 -0.78359451 1 ## 56 -0.494128546 -1.798526974 -0.45631447 2 ## 57 1.270755567 -1.728124050 -0.52140704 2 ## 58 -0.833783722 -1.019661471 0.11366508 2 ## 59 -0.289369516 -0.056861531 0.68224459 4 ## 60 0.537101717 -1.786962210 -0.91982598 2 ## 61 -0.353533354 -1.128421724 0.14242973 2 ## 62 0.361128946 0.272279950 0.56966183 4 ## 63 -0.367373993 0.918913397 0.56443210 4 ## 64 -1.308071034 -0.709064757 0.09931917 3 ## 65 0.363781771 0.600300041 1.25084267 4 ## 66 -1.157268744 -1.528309342 0.65069301 2 ## 67 -0.387238163 -0.243560040 1.97783106 4 ## 68 -0.175815026 -0.493380607 0.38743242 4 ## 69 -0.215994716 -0.187486470 1.21239434 4 ## 70 0.125822750 1.014336464 -1.50830620 1 ## 71 0.043898465 -0.694894128 2.91079266 4 ## 72 0.591877823 -0.514743854 0.52220198 2 ## 73 0.670617685 -1.136412823 0.98490951 2 ## 74 0.566821647 0.355308190 -1.08324875 1 ## 75 -0.405243539 -0.377361921 0.63447511 4 ## 76 -0.261515340 -0.034568298 -0.55808822 3 ## 77 1.230273869 0.108360598 -0.70533844 1 ## 78 -0.487005607 0.589729025 1.12477853 4 ## 79 1.569759428 0.944430422 -0.17498957 1 ## 80 0.623485499 0.851742583 2.08937933 1 ## 81 -1.027310211 0.237917530 -0.29944143 3 ## 82 1.827820076 -0.215036705 1.90868499 4 ## 83 -1.085703931 1.130120053 -1.18413495 3 ## 84 0.408097339 0.448767196 -0.04021405 1 ## 85 -0.600843813 0.719044310 -1.26301128 3 ## 86 -1.050820973 -0.966729201 -0.46877698 3 ## 87 0.357982742 1.029322627 -2.61219453 1 ## 88 -1.532520166 0.991392277 -1.89446736 3 ## 89 -0.717781466 1.414925161 0.04768555 3 ## 90 -0.314442059 -0.193111983 -0.05937934 4 ## 91 -2.687489452 0.443963044 -0.50247137 3 ## 92 -1.211978026 -1.293910693 1.60953208 4 ## 93 0.334622052 -0.095044801 1.02239392 4 ## 94 2.166708581 -1.053012004 -0.70030691 2 ## 95 0.363738464 -0.759141691 0.47094467 2 ## 96 0.781041314 0.931473607 0.94789466 1 ## 97 0.001213169 1.213996356 1.06862004 4 ## 98 -0.072951257 1.162093838 1.35342437 4 ## 99 0.363119167 0.709899211 -0.35868988 1 ## 100 0.398491938 -0.790806636 -0.26957218 2 3.3.1 Validación cluster La estructura de los cluster es aleatoria (¿funciona?) ¿Cómo definimos el valor de \\(K\\)? Silhouette coefficient: Se obtiene para la observación \\(i\\) el promedio de distancia a todos los objetos en el mismo cluster (\\(a_i\\)) Se obtiene para la observación \\(i\\) el promedio de distancia a todos los objetos de los otros clusters (\\(b_i\\)) Se define a \\(s_i\\) como el coeficiente, con un recorrido entre \\([-1,1]\\), para cada observación \\(i\\) \\[s_i=\\frac{b_i-a_i}{max(a_i,b_i)}\\] Idealmente se espera que \\(a_i &lt; b_i\\) y los \\(a_i\\) cercanos a \\(0\\). library(cluster) kk&lt;-kmeans(bd,3) s &lt;- silhouette(kk$cluster, dist(bd)) plot(s) #sobre la base IRIS data(&quot;iris&quot;) aux&lt;-kmeans(iris[,-5],3) s &lt;- silhouette(aux$cluster, dist(iris[,-5])) plot(s) Medoide: es el punto de datos que es “menos diferente” de todos los otros puntos de datos. A diferencia del centroide, el medoide tiene que ser uno de los puntos originales. pam(bd,k=3) ## Medoids: ## ID x y z ## [1,] 99 0.3631192 0.7098992 -0.3586899 ## [2,] 48 0.4816647 -0.7098149 0.5198906 ## [3,] 9 -0.8604852 -0.6309721 -0.3616936 ## Clustering vector: ## [1] 1 1 1 1 2 3 1 2 3 3 1 1 2 1 1 1 3 2 3 3 3 3 2 1 3 1 1 1 3 ## [30] 2 2 2 3 1 3 2 3 1 2 1 3 3 3 3 1 2 3 2 1 1 2 3 2 3 1 3 2 3 ## [59] 2 2 3 2 1 3 2 3 2 2 2 1 2 2 2 1 2 3 1 1 1 2 3 2 1 1 1 3 1 ## [88] 3 1 3 3 2 2 2 2 1 1 1 1 2 ## Objective function: ## build swap ## 1.200771 1.158822 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; ## [5] &quot;isolation&quot; &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; ## [9] &quot;call&quot; &quot;data&quot; kmeans(bd,3) ## K-means clustering with 3 clusters of sizes 29, 37, 34 ## ## Cluster means: ## x y z ## 1 -0.07374988 0.1476459 1.0845637 ## 2 -0.16499758 0.6810607 -0.8366864 ## 3 0.06344588 -1.0985924 -0.1236598 ## ## Clustering vector: ## [1] 2 2 2 1 1 3 2 1 3 3 1 2 3 2 2 2 2 3 3 2 3 3 1 3 3 2 2 2 1 ## [30] 3 3 1 2 2 1 3 3 1 1 2 2 2 2 2 2 3 1 3 1 2 1 3 3 3 2 3 3 3 ## [59] 1 3 3 1 1 3 1 3 1 3 1 2 1 3 3 2 1 2 2 1 2 1 2 1 2 2 2 3 2 ## [88] 2 2 3 2 1 1 3 3 1 1 1 2 3 ## ## Within cluster sum of squares by cluster: ## [1] 37.71076 74.70348 52.91934 ## (between_SS / total_SS = 42.0 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; ¿Cúal es el número óptimo de \\(k\\)? library(fpc)# Flexible Procedures for Clustering sol &lt;- pamk(iris[,-5], krange=2:10, criterion=&quot;asw&quot;, usepam=TRUE) sol ## $pamobject ## Medoids: ## ID Sepal.Length Sepal.Width Petal.Length Petal.Width ## [1,] 8 5.0 3.4 1.5 0.2 ## [2,] 127 6.2 2.8 4.8 1.8 ## Clustering vector: ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [30] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 ## [59] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [88] 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [117] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [146] 2 2 2 2 2 ## Objective function: ## build swap ## 0.9901187 0.8622026 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; ## [5] &quot;isolation&quot; &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; ## [9] &quot;call&quot; &quot;data&quot; ## ## $nc ## [1] 2 ## ## $crit ## [1] 0.0000000 0.6857882 0.5528190 0.4896972 0.4867481 0.4703951 ## [7] 0.3390116 0.3318516 0.2918520 0.2918482 pamk(bd,krange=2:10,usepam = T) ## $pamobject ## Medoids: ## ID x y z ## [1,] 70 0.125822750 1.01433646 -1.50830620 ## [2,] 7 0.647898062 0.78882056 -0.37365674 ## [3,] 20 -0.555439460 0.08051445 -0.04146018 ## [4,] 97 0.001213169 1.21399636 1.06862004 ## [5,] 69 -0.215994716 -0.18748647 1.21239434 ## [6,] 58 -0.833783722 -1.01966147 0.11366508 ## [7,] 44 -1.163943943 0.09635084 -1.39774048 ## [8,] 48 0.481664743 -0.70981490 0.51989056 ## [9,] 37 -0.273045676 -1.63562785 -0.83658014 ## [10,] 31 1.761442961 -1.07277996 -0.43011669 ## Clustering vector: ## [1] 1 2 3 4 5 6 2 5 6 7 3 1 8 2 1 2 7 8 9 ## [20] 3 9 6 5 8 9 1 2 3 5 8 10 5 3 2 3 10 9 4 ## [39] 8 3 7 3 7 7 2 8 3 8 3 2 8 9 8 6 1 9 10 ## [58] 6 5 9 6 5 4 6 4 6 5 8 5 1 5 8 8 2 5 3 ## [77] 2 4 2 4 3 8 7 2 1 6 1 7 4 3 7 5 5 10 8 ## [96] 4 4 4 2 8 ## Objective function: ## build swap ## 0.7278826 0.7202182 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; ## [5] &quot;isolation&quot; &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; ## [9] &quot;call&quot; &quot;data&quot; ## ## $nc ## [1] 10 ## ## $crit ## [1] 0.0000000 0.2315081 0.2403894 0.2323086 0.2456012 0.2337017 ## [7] 0.2313436 0.2491545 0.2692471 0.2714183 3.3.2 Distancias para variables nominales (todas nominales) En este caso la mejor estrategia es llevar las variables con sus categorias a variables binarias. Existen múltiples medidas de distancia para variables binarias, muchas de estas medidas son aproximaciones a las medidas mas conocidas. Entre ellas: Sean las filas \\(i\\), \\(j\\) que contienen los valores binarios de las variables de estudio. Sea \\(A\\) el total de \\(1\\) que existe en \\(i\\), \\(B\\) el total de \\(1\\) que existe en \\(j\\) y sea \\(J\\) el total de casos en los que los \\(1\\) ocurren simultaneamente en \\(i\\) y \\(j\\). Euclideana \\[d_{ij}=\\sqrt{A+B-2J}\\] * Manhattan \\[d_{ij}=A+B-2J\\] * Bray \\[d_{ij}=\\frac{A+B-2J}{A+B}\\] Binomial \\[d_{ij}=log(2)(A+B-2J)\\] aux&lt;-rbind(c(0,0,0,0,1,1,1),c(1,0,1,0,0,1,1)) A&lt;-sum(aux[1,]) B&lt;-sum(aux[2,]) J&lt;-sum(apply(aux, 2, sum)==2) #euclideana sqrt(A+B-2*J) ## [1] 1.732051 #manhathan A+B-2*J ## [1] 3 #bray (A+B-2*J)/(A+B) ## [1] 0.4285714 #binomial log(2)*(A+B-2*J) ## [1] 2.079442 library(vegan) vegdist(aux,binary = T) ## 1 ## 2 0.4285714 vegdist(aux,binary = F) ## 1 ## 2 0.4285714 #una base de datos mas grandes set.seed(999) aux1&lt;-matrix(rbinom(200,1,0.4),nrow = 20) vegdist(aux1,method = &quot;binomial&quot;,binary = T) ## 1 2 3 4 5 6 ## 2 3.4657359 ## 3 3.4657359 2.7725887 ## 4 2.7725887 4.8520303 4.8520303 ## 5 4.1588831 3.4657359 4.8520303 2.7725887 ## 6 2.7725887 3.4657359 3.4657359 4.1588831 1.3862944 ## 7 3.4657359 2.7725887 2.7725887 2.0794415 3.4657359 4.8520303 ## 8 2.0794415 1.3862944 2.7725887 4.8520303 3.4657359 2.0794415 ## 9 2.7725887 3.4657359 4.8520303 4.1588831 2.7725887 1.3862944 ## 10 2.7725887 4.8520303 3.4657359 2.7725887 2.7725887 2.7725887 ## 11 3.4657359 2.7725887 4.1588831 3.4657359 4.8520303 4.8520303 ## 12 2.7725887 4.8520303 3.4657359 2.7725887 4.1588831 2.7725887 ## 13 4.1588831 4.8520303 3.4657359 2.7725887 2.7725887 2.7725887 ## 14 4.8520303 4.1588831 4.1588831 3.4657359 3.4657359 3.4657359 ## 15 1.3862944 3.4657359 2.0794415 2.7725887 4.1588831 2.7725887 ## 16 3.4657359 4.1588831 5.5451774 2.0794415 0.6931472 2.0794415 ## 17 3.4657359 2.7725887 4.1588831 3.4657359 3.4657359 3.4657359 ## 18 2.7725887 4.8520303 3.4657359 4.1588831 4.1588831 4.1588831 ## 19 3.4657359 4.1588831 1.3862944 4.8520303 3.4657359 2.0794415 ## 20 4.8520303 2.7725887 4.1588831 3.4657359 3.4657359 3.4657359 ## 7 8 9 10 11 12 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 4.1588831 ## 9 6.2383246 2.0794415 ## 10 3.4657359 3.4657359 2.7725887 ## 11 2.7725887 4.1588831 3.4657359 3.4657359 ## 12 3.4657359 4.8520303 2.7725887 2.7725887 2.0794415 ## 13 4.8520303 4.8520303 2.7725887 2.7725887 3.4657359 2.7725887 ## 14 4.1588831 4.1588831 2.0794415 2.0794415 2.7725887 2.0794415 ## 15 3.4657359 2.0794415 2.7725887 1.3862944 3.4657359 2.7725887 ## 16 4.1588831 4.1588831 2.0794415 2.0794415 4.1588831 3.4657359 ## 17 5.5451774 2.7725887 2.0794415 4.8520303 4.1588831 4.8520303 ## 18 3.4657359 4.8520303 4.1588831 2.7725887 3.4657359 2.7725887 ## 19 4.1588831 2.7725887 3.4657359 2.0794415 4.1588831 3.4657359 ## 20 4.1588831 4.1588831 2.0794415 3.4657359 1.3862944 2.0794415 ## 13 14 15 16 17 18 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 3.4657359 ## 15 2.7725887 3.4657359 ## 16 2.0794415 2.7725887 3.4657359 ## 17 2.0794415 4.1588831 3.4657359 2.7725887 ## 18 4.1588831 3.4657359 4.1588831 3.4657359 4.8520303 ## 19 2.0794415 4.1588831 2.0794415 4.1588831 4.1588831 3.4657359 ## 20 2.0794415 1.3862944 3.4657359 2.7725887 2.7725887 4.8520303 ## 19 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## 17 ## 18 ## 19 ## 20 4.1588831 dist(aux1) ## 1 2 3 4 5 6 ## 2 2.236068 ## 3 2.236068 2.000000 ## 4 2.000000 2.645751 2.645751 ## 5 2.449490 2.236068 2.645751 2.000000 ## 6 2.000000 2.236068 2.236068 2.449490 1.414214 ## 7 2.236068 2.000000 2.000000 1.732051 2.236068 2.645751 ## 8 1.732051 1.414214 2.000000 2.645751 2.236068 1.732051 ## 9 2.000000 2.236068 2.645751 2.449490 2.000000 1.414214 ## 10 2.000000 2.645751 2.236068 2.000000 2.000000 2.000000 ## 11 2.236068 2.000000 2.449490 2.236068 2.645751 2.645751 ## 12 2.000000 2.645751 2.236068 2.000000 2.449490 2.000000 ## 13 2.449490 2.645751 2.236068 2.000000 2.000000 2.000000 ## 14 2.645751 2.449490 2.449490 2.236068 2.236068 2.236068 ## 15 1.414214 2.236068 1.732051 2.000000 2.449490 2.000000 ## 16 2.236068 2.449490 2.828427 1.732051 1.000000 1.732051 ## 17 2.236068 2.000000 2.449490 2.236068 2.236068 2.236068 ## 18 2.000000 2.645751 2.236068 2.449490 2.449490 2.449490 ## 19 2.236068 2.449490 1.414214 2.645751 2.236068 1.732051 ## 20 2.645751 2.000000 2.449490 2.236068 2.236068 2.236068 ## 7 8 9 10 11 12 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 2.449490 ## 9 3.000000 1.732051 ## 10 2.236068 2.236068 2.000000 ## 11 2.000000 2.449490 2.236068 2.236068 ## 12 2.236068 2.645751 2.000000 2.000000 1.732051 ## 13 2.645751 2.645751 2.000000 2.000000 2.236068 2.000000 ## 14 2.449490 2.449490 1.732051 1.732051 2.000000 1.732051 ## 15 2.236068 1.732051 2.000000 1.414214 2.236068 2.000000 ## 16 2.449490 2.449490 1.732051 1.732051 2.449490 2.236068 ## 17 2.828427 2.000000 1.732051 2.645751 2.449490 2.645751 ## 18 2.236068 2.645751 2.449490 2.000000 2.236068 2.000000 ## 19 2.449490 2.000000 2.236068 1.732051 2.449490 2.236068 ## 20 2.449490 2.449490 1.732051 2.236068 1.414214 1.732051 ## 13 14 15 16 17 18 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 2.236068 ## 15 2.000000 2.236068 ## 16 1.732051 2.000000 2.236068 ## 17 1.732051 2.449490 2.236068 2.000000 ## 18 2.449490 2.236068 2.449490 2.236068 2.645751 ## 19 1.732051 2.449490 1.732051 2.449490 2.449490 2.236068 ## 20 1.732051 1.414214 2.236068 2.000000 2.000000 2.645751 ## 19 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## 17 ## 18 ## 19 ## 20 2.449490 3.3.3 Distancias para variables mixtas (cuantitativas, nominales, ordinales) library(cluster) data(&quot;flower&quot;) str(flower) ## &#39;data.frame&#39;: 18 obs. of 8 variables: ## $ V1: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 1 1 1 1 1 2 2 ... ## $ V2: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 1 2 2 1 1 2 2 ... ## $ V3: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 2 1 1 1 2 1 1 ... ## $ V4: Factor w/ 5 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 4 2 3 4 5 4 4 2 3 5 ... ## $ V5: Ord.factor w/ 3 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;: 3 1 3 2 2 3 3 2 1 2 ... ## $ V6: Ord.factor w/ 18 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 15 3 1 16 2 12 13 7 4 14 ... ## $ V7: num 25 150 150 125 20 50 40 100 25 100 ... ## $ V8: num 15 50 50 50 15 40 20 15 15 60 ... dd&lt;-daisy(flower,metric = &quot;gower&quot;) summary(dd) ## 153 dissimilarities, summarized : ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1418 0.3904 0.4829 0.4865 0.5865 0.8875 ## Metric : mixed ; Types = N, N, N, N, O, O, I, I ## Number of objects : 18 Ejercicios Busque funciones en R que permitan calcular los k-center para variables mixtas y medoides Crear una función k-center para variables mixtas y alternativas para incluir el medoide. 3.4 Cluster Jerárquico El objetivo es obtener una jerarquía de posibles soluciones que van desde un solo grupo a \\(n\\) grupos, donde \\(n\\) es el número de observaciones en el conjunto de datos. 3.4.1 Algoritmo (Johnson) Se inicia con \\(n\\) grupos y se genera una matriz de \\(nxn\\) de distancias, \\(D=\\{d_{ik}\\}\\) Buscar en la matriz de distancia los pares de cluster más cercanos entre ellos, “los cluster mas similares”, si defifinimos los clusters \\(V\\) y \\(U\\), estamos interesados en encontrar \\(d_{UV}\\) Unir los cluster \\(U\\) y \\(V\\), re etiquetar el nuevo cluster como \\(UV\\). Actualizar la matriz de distancias a) remover las filas y columnas correspondientes a \\(U\\) y \\(V\\) b) incluimos las nuevas filas y columnas para el nuevo cluster \\(UV\\). Repetimos el paso 2 y 3 un total de \\(n-1\\) veces. El momento de definir el cluster más cercano, se puede emplear los siguientes enlaces: Single linkage (Enlace simple): Se elige al cluster más cercano, con la regla de que las distincia individual entre las observaciones dentro de los clusters es la más corta Complete linkage (Enlace completo): Se elige el cluster mas cercano, con la regla que las distnaicas individuales entre las observaciones dentro de los cluster es la más larga Average linkage (Enlace promedio): Se elige el cluster mas cercano, considerando el promedio de las distancias entre los cluster. Nota: Se debe elegir la matriz de distancias acorde a la naturaleza de los datos, se recomienda: Todas Numéricas: Euclideana o Manhatan Todas nominales: Transformación a binarias y usar la distancia binomial Mixtas: Distancia de Gower d &lt;- dist(scale(iris[,-5]))#euclideana h &lt;- hclust(d) plot(h,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) #elegir la cantidad de grupos clus3 &lt;- cutree(h, 5) plot(h,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(h,k=5) Probando los tres tipos de enlaces para \\(k=3\\) hs &lt;- hclust(d,method = &quot;single&quot;) hc &lt;- hclust(d,method = &quot;complete&quot;) ha &lt;- hclust(d,method = &quot;average&quot;) cs&lt;-cutree(hs,3) cc&lt;-cutree(hc,3) ca&lt;-cutree(ha,3) table(cs,cc) ## cc ## cs 1 2 3 ## 1 49 0 0 ## 2 0 1 0 ## 3 0 23 77 table(cs,ca) ## ca ## cs 1 2 3 ## 1 49 0 0 ## 2 1 0 0 ## 3 0 97 3 table(cc,ca) ## ca ## cc 1 2 3 ## 1 49 0 0 ## 2 1 23 0 ## 3 0 74 3 par(mfrow=c(1,3)) plot(hs,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(hs,k=3) plot(hc,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(hc,k=3) plot(ha,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(ha,k=3) Nota: El dendograma es muy útil para ver las relaciones que existen basadas en las distancias y la creación de las jerarquias, a partir de estos se puede definir un \\(k\\) (de forma visual) Nota: El dendograma pierde su utilidad cuando la cantidad de observaciones es muy alta, Ejemplo, Usar los datos de las elecciones del 20 de octubre, agregar los resultados en términos relativos para los municipios y generar el dendograma para los tres tipos de enlaces, de forma visual sugerir un valor de \\(k\\) para cada tipo de enlace. library(dplyr) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\oct20.RData&quot;) #filtrar los casos aux&lt;-c(&quot;Número departamento&quot;,&quot;Departamento&quot; ,&quot;Número municipio&quot;,&quot;Municipio&quot; ,&quot;CC&quot;,&quot;FPV&quot;,&quot;MTS&quot;,&quot;UCS&quot;,&quot;MAS - IPSP&quot;,&quot;21F&quot;,&quot;PDC&quot;,&quot;MNR&quot;,&quot;PAN-BOL&quot;,&quot;Votos Válidos&quot;,&quot;Blancos&quot;,&quot;Nulos&quot;) names(computo)[1]&lt;-&quot;pais&quot; names(computo)[12]&lt;-&quot;eleccion&quot; bd&lt;-computo %&gt;% filter(pais==&quot;Bolivia&quot; &amp; eleccion==&quot;Presidente y Vicepresidente&quot;) %&gt;% select(aux) names(bd)[1:4]&lt;-c(&quot;idep&quot;,&quot;ddep&quot;,&quot;imun&quot;,&quot;dmun&quot;) bdmun&lt;-aggregate(bd[,5:16],bd[,1:4],sum) bdmun&lt;-bdmun[,-14] bdmun[,5:15]&lt;-prop.table(as.matrix(bdmun[,5:15]),1) #cluster jerarquico d&lt;-dist(bdmun[,5:15]) plot(hclust(d),hang=-0.1,label=bdmun$dmun,cex=0.5) # determinar el mejor k y el mejor enlace mm&lt;-c(&quot;single&quot;, &quot;complete&quot;, &quot;average&quot;) # método k&lt;-2:20 # cantidad de cluster d&lt;-dist(bdmun[,5:15]) # matriz de distancia # matriz de resultados res&lt;-matrix(NA, nrow = 19,ncol=3) colnames(res)&lt;-mm rownames(res)&lt;-k ####################### for(i in k){ for(j in 1:3){ h&lt;-hclust(d,method = mm[j]) c&lt;-cutree(h,i) s&lt;-silhouette(c,d) res[i-1,j]&lt;-median(s[,3]) } } #la mejor opción es k=2 con el método average h&lt;-hclust(d,method = &quot;average&quot;) c&lt;-cutree(h,2) plot(h,hang=-0.1,labels=bdmun$dmun,cex=0.4) rect.hclust(h,k=2) bdmun$cluster&lt;-c group_by(bdmun,cluster) %&gt;% summarise(mean(CC),mean(`MAS - IPSP`)) ## # A tibble: 2 x 3 ## cluster `mean(CC)` `mean(\\`MAS - IPSP\\`)` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0783 0.708 ## 2 2 0.317 0.429 3.5 Ejercicios Pensar en un gráfico que permita ver como se asignaron los cluster Pensar en optimizar el código empleado para el k-center Hacer que la función desarrollada para el k-center retorne también los centroides Utilizando la base de datos de las elecciones del 20 de octubre, crear una base de datos a nivel municipal, aplicar el método k-center con medoides para los resultados a nivel municipal y terminar el \\(k\\) óptimo en un rango de \\(k=2:10\\) (Usar datos relativos). "],
["regresión.html", "4 Regresión 4.1 Regresión lineal 4.2 Probit y Logit", " 4 Regresión 4.1 Regresión lineal 4.2 Probit y Logit "],
["clasificación.html", "5 Clasificación 5.1 Arboles de decision 5.2 Naive Bayes", " 5 Clasificación 5.1 Arboles de decision 5.2 Naive Bayes "],
["minería-de-texto.html", "6 Minería de Texto 6.1 Recolección de texto 6.2 Nubes de palabras 6.3 Análisis de sentimiento 6.4 n-gramas y correlaciones", " 6 Minería de Texto 6.1 Recolección de texto 6.2 Nubes de palabras 6.3 Análisis de sentimiento 6.4 n-gramas y correlaciones "]
]
