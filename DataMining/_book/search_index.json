[
["index.html", "Minería de datos con R EST-384 Prefacio Audiencia Estructura del libro Software y acuerdos Datos Agradecimiento", " Minería de datos con R EST-384 Alvaro Chirino Gutierrez 2020-07-09 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Programación Estadística II, una materia optativa del pregrado de la carrera de Estadística de la Universidad Mayor de San Andrés. Este documento representa un primer acercamiento a los estudiantes de estadistica al software R y al mundo de la minería de datos. Estructura del libro El libro inluye 5 capitulos, estos son: Introducción a R Preparación de los datos Modelado en Minería de datos Minería de Texto Machine Learning Software y acuerdos sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Spanish_Bolivia.1252 ## [2] LC_CTYPE=Spanish_Bolivia.1252 ## [3] LC_MONETARY=Spanish_Bolivia.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=Spanish_Bolivia.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets ## [6] methods base ## ## other attached packages: ## [1] vegan_2.5-6 permute_0.9-5 ## [3] fpc_2.2-5 cluster_2.1.0 ## [5] TeachingDemos_2.12 ggridges_0.5.2 ## [7] GGally_2.0.0 Hmisc_4.4-0 ## [9] ggplot2_3.3.2 Formula_1.2-3 ## [11] survival_3.1-12 lattice_0.20-41 ## [13] dplyr_1.0.0 DMwR2_0.0.2 ## [15] stringi_1.4.6 lubridate_1.7.9 ## [17] tidyr_1.0.2 readr_1.3.1 ## [19] foreign_0.8-80 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-148 xts_0.12-0 ## [3] RColorBrewer_1.1-2 progress_1.2.2 ## [5] prabclus_2.3-2 tools_4.0.2 ## [7] backports_1.1.6 utf8_1.1.4 ## [9] R6_2.4.1 rpart_4.1-15 ## [11] DBI_1.1.0 mgcv_1.8-31 ## [13] colorspace_1.4-1 nnet_7.3-14 ## [15] withr_2.2.0 tidyselect_1.1.0 ## [17] gridExtra_2.3 prettyunits_1.1.1 ## [19] curl_4.3 compiler_4.0.2 ## [21] cli_2.0.2 htmlTable_2.0.0 ## [23] labeling_0.3 bookdown_0.18 ## [25] diptest_0.75-7 scales_1.1.1 ## [27] checkmate_2.0.0 DEoptimR_1.0-8 ## [29] robustbase_0.93-6 stringr_1.4.0 ## [31] digest_0.6.25 rmarkdown_2.3 ## [33] base64enc_0.1-3 jpeg_0.1-8.1 ## [35] pkgconfig_2.0.3 htmltools_0.4.0 ## [37] htmlwidgets_1.5.1 rlang_0.4.6 ## [39] TTR_0.23-6 rstudioapi_0.11 ## [41] quantmod_0.4.17 farver_2.0.3 ## [43] generics_0.0.2 zoo_1.8-8 ## [45] mclust_5.4.6 acepack_1.4.1 ## [47] magrittr_1.5 modeltools_0.2-23 ## [49] Matrix_1.2-18 Rcpp_1.0.4.6 ## [51] munsell_0.5.0 fansi_0.4.1 ## [53] lifecycle_0.2.0 yaml_2.2.1 ## [55] MASS_7.3-51.6 flexmix_2.3-15 ## [57] plyr_1.8.6 grid_4.0.2 ## [59] parallel_4.0.2 crayon_1.3.4 ## [61] splines_4.0.2 hms_0.5.3 ## [63] knitr_1.28 pillar_1.4.4 ## [65] stats4_4.0.2 glue_1.4.1 ## [67] packrat_0.5.0 evaluate_0.14 ## [69] latticeExtra_0.6-29 data.table_1.12.8 ## [71] png_0.1-7 vctrs_0.3.1 ## [73] gtable_0.3.0 purrr_0.3.4 ## [75] reshape_0.8.8 kernlab_0.9-29 ## [77] assertthat_0.2.1 xfun_0.13 ## [79] class_7.3-17 tibble_3.0.1 ## [81] ellipsis_0.3.1 Datos Agradecimiento Peeta… "],
["acerca-del-autor.html", "Acerca del autor Bibliografía", " Acerca del autor Bibliografía Torgo, L. (2016). Data mining with R: Learning with case studies, second edition. Hernandez, J. (2004). Introducción a la Minería de Datos Step, I., &amp; Blueprint, S. (2017). MACHINE LEARNING Intuitive Step by Step. "],
["minería-de-datos.html", "1 Minería de Datos 1.1 Motivación para la Mineria de datos 1.2 ¿Qué es la minería de datos? 1.3 Datos y conocimiento 1.4 Requerimientos 1.5 knowledge discovery in databases (KDD) 1.6 Preparación de los datos 1.7 Imputación de variables", " 1 Minería de Datos 1.1 Motivación para la Mineria de datos Los métodos de recoleccion de datos han evolucionado muy rápidamente. Las bases de datos han crecido exponencialmente Estos datos contienen información útil para las empresas, paises, etc.. El tamaño hace que la inspección manual sea casi imposible Se requieren métodos de análisis de datos automáticos para optimizar el uso de estos enormes conjuntos de datos 1.2 ¿Qué es la minería de datos? Es el análisis de conjuntos de datos (a menudo grandes) para encontrar relaciones insospechadas (conocimiento) y resumir los datos de formas novedosas que sean comprensibles y útiles para el propietario/usuario de los datos. Principles of Data Mining (Hand et.al. 2001) 1.3 Datos y conocimiento 1.3.1 Datos: se refieren a instancias únicas y primitivas (single objetos, personas, eventos, puntos en el tiempo, etc.) describir propiedades individuales a menudo son fáciles de recolectar u obtener (por ejemplo, cajeros de escáner, internet, etc.) no nos permiten hacer predicciones o pronósticos 1.3.2 Conocimiento: se refiere a clases de instancias (conjuntos de …) describe patrones generales, estructuras, leyes, consta de la menor cantidad de declaraciones posibles a menudo es difícil y lleva mucho tiempo encontrar u obtener nos permite hacer predicciones y pronósticos 1.4 Requerimientos Disponibilidad para aprender Mucha paciencia Interactúa con otras áreas Preprocesamiento de datos Creatividad Rigor, prueba y error 1.5 knowledge discovery in databases (KDD) 1.6 Preparación de los datos 1.6.1 Recopilación Instituto de Estadística UDAPE, ASFI Ministerio Salud (SNIS), Ministerio de educación (SIE) APIs, Twitter, Facebook, etc. Kaggle Banco Mundial, UNICEF, FAO, BID (Open Data) 1.6.2 Data Warehouse 1.6.3 Data Warehouse in R 1.6.4 Importación library(foreign) library(readr) apropos(&quot;read&quot;) ## [1] &quot;.rs.api.readPreference&quot; ## [2] &quot;.rs.api.readRStudioPreference&quot; ## [3] &quot;.rs.connectionReadDSN&quot; ## [4] &quot;.rs.connectionReadInstallers&quot; ## [5] &quot;.rs.connectionReadOdbc&quot; ## [6] &quot;.rs.connectionReadOdbcEntry&quot; ## [7] &quot;.rs.connectionReadPackageInstallers&quot; ## [8] &quot;.rs.connectionReadPackages&quot; ## [9] &quot;.rs.connectionReadSnippets&quot; ## [10] &quot;.rs.connectionReadWindowsRegistry&quot; ## [11] &quot;.rs.isREADME&quot; ## [12] &quot;.rs.odbcBundleReadIni&quot; ## [13] &quot;.rs.onAvailablePackagesReady&quot; ## [14] &quot;.rs.readAliases&quot; ## [15] &quot;.rs.readApiPref&quot; ## [16] &quot;.rs.readDataCapture&quot; ## [17] &quot;.rs.readFile&quot; ## [18] &quot;.rs.readIniFile&quot; ## [19] &quot;.rs.readLines&quot; ## [20] &quot;.rs.readPackageDescription&quot; ## [21] &quot;.rs.readPrefInternal&quot; ## [22] &quot;.rs.readRnbCache&quot; ## [23] &quot;.rs.readShinytestResultRds&quot; ## [24] &quot;.rs.readSourceDocument&quot; ## [25] &quot;.rs.readUiPref&quot; ## [26] &quot;.rs.readUserPref&quot; ## [27] &quot;.rs.readUserState&quot; ## [28] &quot;.rs.renv.readLockfilePackages&quot; ## [29] &quot;.rs.rnb.readConsoleData&quot; ## [30] &quot;read.arff&quot; ## [31] &quot;read.cep&quot; ## [32] &quot;read.csv&quot; ## [33] &quot;read.csv2&quot; ## [34] &quot;read.dbf&quot; ## [35] &quot;read.dcf&quot; ## [36] &quot;read.delim&quot; ## [37] &quot;read.delim2&quot; ## [38] &quot;read.DIF&quot; ## [39] &quot;read.dta&quot; ## [40] &quot;read.epiinfo&quot; ## [41] &quot;read.fortran&quot; ## [42] &quot;read.ftable&quot; ## [43] &quot;read.fwf&quot; ## [44] &quot;read.mtp&quot; ## [45] &quot;read.octave&quot; ## [46] &quot;read.S&quot; ## [47] &quot;read.socket&quot; ## [48] &quot;read.spss&quot; ## [49] &quot;read.ssd&quot; ## [50] &quot;read.systat&quot; ## [51] &quot;read.table&quot; ## [52] &quot;read.xport&quot; ## [53] &quot;read.xportDataload&quot; ## [54] &quot;read_csv&quot; ## [55] &quot;read_csv_chunked&quot; ## [56] &quot;read_csv2&quot; ## [57] &quot;read_csv2_chunked&quot; ## [58] &quot;read_delim&quot; ## [59] &quot;read_delim_chunked&quot; ## [60] &quot;read_file&quot; ## [61] &quot;read_file_raw&quot; ## [62] &quot;read_fwf&quot; ## [63] &quot;read_lines&quot; ## [64] &quot;read_lines_chunked&quot; ## [65] &quot;read_lines_raw&quot; ## [66] &quot;read_lines_raw_chunked&quot; ## [67] &quot;read_log&quot; ## [68] &quot;read_rds&quot; ## [69] &quot;read_table&quot; ## [70] &quot;read_table2&quot; ## [71] &quot;read_tsv&quot; ## [72] &quot;read_tsv_chunked&quot; ## [73] &quot;readBin&quot; ## [74] &quot;readChar&quot; ## [75] &quot;readCitationFile&quot; ## [76] &quot;readClipboard&quot; ## [77] &quot;readline&quot; ## [78] &quot;readLines&quot; ## [79] &quot;readr_example&quot; ## [80] &quot;readRDS&quot; ## [81] &quot;readRegistry&quot; ## [82] &quot;readRenviron&quot; ## [83] &quot;readSAScsv&quot; ## [84] &quot;RsquareAdj&quot; ## [85] &quot;spread&quot; ## [86] &quot;spread.labs&quot; ## [87] &quot;spread_&quot; ## [88] &quot;stri_read_lines&quot; ## [89] &quot;stri_read_raw&quot; ## [90] &quot;Sys.readlink&quot; 1.6.5 Recopilación read.table(&quot;clipboard&quot;,header = T) library(readxl) library(dplyr) library(DBI) library(RMySQL) library(gtrendsR) # API 1.6.6 Limpieza std&lt;-data.frame(name=c(&quot;ana&quot;,&quot;juan&quot;,&quot;carla&quot;),math=c(86,43,80),stat=c(90,75,82)) std ## name math stat ## 1 ana 86 90 ## 2 juan 43 75 ## 3 carla 80 82 library(tidyr) bd&lt;-gather(std,materia,nota,math:stat) bd ## name materia nota ## 1 ana math 86 ## 2 juan math 43 ## 3 carla math 80 ## 4 ana stat 90 ## 5 juan stat 75 ## 6 carla stat 82 1.6.7 Ejercicio (reshape) http://www.udape.gob.bo/portales_html/dossierweb2019/htms/CAP07/C070311.xls 1.6.8 Limpieza (fechas) library(lubridate) ymd(&quot;20151021&quot;) ## [1] &quot;2015-10-21&quot; ymd(&quot;2015/11/30&quot;) ## [1] &quot;2015-11-30&quot; myd(&quot;11.2012.3&quot;) ## [1] &quot;2012-11-03&quot; dmy_hms(&quot;2/12/2013 14:05:01&quot;) ## [1] &quot;2013-12-02 14:05:01 UTC&quot; mdy(&quot;120112&quot;) ## [1] &quot;2012-12-01&quot; 1.6.9 Limpieza (String) toupper(&quot;hola&quot;) ## [1] &quot;HOLA&quot; abc&lt;-letters[1:10] toupper(abc) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; tolower(&quot;HOLA&quot;) ## [1] &quot;hola&quot; tolower(&quot;Juan&quot;) ## [1] &quot;juan&quot; substr(&quot;hola como estan&quot;,1,3) ## [1] &quot;hol&quot; substr(&quot;hola como estan&quot;,3,7) ## [1] &quot;la co&quot; nchar(&quot;hola&quot;) ## [1] 4 gsub(&quot;a&quot;,&quot;x&quot;,&quot;hola como estas&quot;) ## [1] &quot;holx como estxs&quot; grepl(&quot;a&quot;,c(&quot;hola&quot;,&quot;como&quot;)) ## [1] TRUE FALSE grepl(&quot;o&quot;,c(&quot;hola&quot;,&quot;como&quot;)) ## [1] TRUE TRUE library(stringi) 1.6.10 Transfomración Estandarizar variables Función logarítmo Creación de variables Recodificar variables 1.7 Imputación de variables We should be suspicious of any dataset (large or small) which appears perfect. — David J. Hand 1.7.1 La falta de información es información MCAR missing completely at random MAR missing at random MNAR missing not at random 1.7.2 Aproximación formal Sea \\(Y\\) una matriz de datos con \\(n\\) observaciones y \\(p\\) variables. Sea \\(R\\) una matriz de respuesta binaria, tal que si \\(y_{ij}\\) es observada, entonces \\(r_{ij}=1\\). Los valores observados son colectados en \\(Y_{obs}\\), las observaciones perdidas en \\(Y_{mis}\\). Así, \\(Y=(Y_{obs},Y_{mis})\\). La distribución de \\(R\\) depende de \\(Y=(Y_{obs},Y_{mis})\\). Sea \\(\\psi\\) que contiene los parametros del modelos de los datos perdidos, asi la expresion del modelo de los datos perdidos es \\(\\Pr(R|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi)\\) 1.7.3 MCAR, MAR, MNAR MCAR (missing completely at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) = \\Pr(R=0|\\psi) \\] MAR (missing at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) = \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},\\psi) \\] MNAR (missing not at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) \\] 1.7.4 Alternativas para trabajar con los Missings (Ad-hoc) Listwise deletion Pairwise deletion Mean imputation Regression imputation Stochastic regression imputation Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) Indicator method 1.7.5 Imputación Multiple 1.7.6 Patrones en datos multivariados 1.7.7 Influx and outflux \\[ I_j = \\frac{\\sum_j^p\\sum_k^p\\sum_i^n (1-r_{ij})r_{ik}}{\\sum_k^p\\sum_i^n r_{ik}} \\] La variable con mayor influx está mejor conectada a los datos observados y, por lo tanto, podría ser más fácil de imputar. \\[ O_j = \\frac{\\sum_j^p\\sum_k^p\\sum_i^n r_{ij}(1-r_{ik})}{\\sum_k^p\\sum_i^n 1-r_{ij}} \\] La variable con mayor outflux está mejor conectada a los datos faltantes, por lo tanto, es potencialmente más útil para imputar otras variables. 1.7.8 Imputación de datos monótonos 1.7.9 Multivariate Imputation by Chained Equations (mice) (Imputación multivariante por ecuaciones encadenadas) "],
["modelado-en-minería-de-datos.html", "2 Modelado en Minería de datos 2.1 Explorando los datos 2.2 Componentes Principales 2.3 Análisis de correspondencia", " 2 Modelado en Minería de datos 2.1 Explorando los datos Existen dos aproximaciones para empezar a explorar la información existente en una base de datos: Resumen de los datos Visualización de los datos 2.1.1 Resumen de los datos Dado el tamaño de las bases de datos resulta imposible o muy dificil conocer todas sus propiedad, el resumen de los datos intenta brindar propiedades claves de los datos, estas propiedades podrian ser: Cual es el valor mas comun Cuan variable o dispersa esta la informacion Existen valores extraños o inesparedaps en la base de datos Los datos siguen alguna distribucion A continuación se emplea la encuesta a hogares 2018 para ir respondiendo estas preguntas. En cuanto a los valores mas comunes, la media y la mediana de los datos son suficientes para las variables cuantitativas, mientra que para variables cualitativas, las categorias mas frecuentes son una buena opción. load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/eh18.Rdata&quot;)) # media de edad mean(eh18p$s02a_03) ## [1] 29.59059 #mediana de edad median(eh18p$s02a_03) ## [1] 26 # para las categorias mas frecuentes library(DMwR2) library(dplyr) #para el sexo centralValue(eh18p$s02a_02) ## [1] &quot;2.Mujer&quot; #para el departamento centralValue(eh18p$depto) ## [1] &quot;La Paz&quot; A veces es mejor ver los resultados por grupos, por ejemplo, podemos verlo por departamento y area. eh18p %&gt;% group_by(depto,area) %&gt;% summarise(mean(s02a_03),median(s02a_03),centralValue(s02a_02),n()) ## `summarise()` regrouping output by &#39;depto&#39; (override with `.groups` argument) ## # A tibble: 18 x 6 ## # Groups: depto [9] ## depto area `mean(s02a_03)` `median(s02a_03~ ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chuq~ Urba~ 28.7 24 ## 2 Chuq~ Rural 31.0 27 ## 3 La P~ Urba~ 30.4 27 ## 4 La P~ Rural 34.3 31 ## 5 Coch~ Urba~ 30.4 27 ## 6 Coch~ Rural 31.1 27.5 ## 7 Oruro Urba~ 28.7 26 ## 8 Oruro Rural 36.4 34 ## 9 Poto~ Urba~ 26.1 23 ## 10 Poto~ Rural 35.6 34.5 ## 11 Tari~ Urba~ 29.4 27 ## 12 Tari~ Rural 29.7 27 ## 13 Sant~ Urba~ 27.8 25 ## 14 Sant~ Rural 29.9 26 ## 15 Beni Urba~ 26.8 22 ## 16 Beni Rural 26.0 19 ## 17 Pando Urba~ 25.5 23 ## 18 Pando Rural 22.6 18 ## # ... with 2 more variables: ## # `centralValue(s02a_02)` &lt;chr&gt;, `n()` &lt;int&gt; La función n() realiza un proceso de conteo. De forma similar podemos hacer para las medidas de variabilidad, las mas comunes la desviacion estandar, el rango, el rango intercuartil y los cuantiles. Usando la eh18 para la edad y el ingreso laboral. ##EDAD # Desviacion estandar sd(eh18p$s02a_03) ## [1] 20.97181 # Rango range(eh18p$s02a_03) ## [1] 0 98 # Rango intercuartil IQR(eh18p$s02a_03) ## [1] 32 # Quantiles quantile(eh18p$s02a_03) ## 0% 25% 50% 75% 100% ## 0 12 26 44 98 quantile(eh18p$s02a_03,c(0.10,0.90)) ## 10% 90% ## 5 61 ##INGRESO LABORAL # Desviacion estandar sd(eh18p$ylab,na.rm = T) ## [1] 2242.593 # Rango range(eh18p$ylab,na.rm = T) ## [1] 4.166667 36196.667969 # Rango intercuartil IQR(eh18p$ylab,na.rm = T) ## [1] 2397 # Quantiles quantile(eh18p$ylab,na.rm = T) ## 0% 25% 50% 75% ## 4.166667 1500.000000 2554.699951 3897.000000 ## 100% ## 36196.667969 quantile(eh18p$s02a_03,probs=c(0.10,0.90),na.rm = T) ## 10% 90% ## 5 61 ##por departamento y area tapply(eh18p$ylab,list(eh18p$depto,eh18p$area),sd,na.rm=T)#opcion1 ## Urbana Rural ## Chuquisaca 2586.058 1735.306 ## La Paz 2154.421 1703.794 ## Cochabamba 2313.400 1654.140 ## Oruro 2425.280 1622.623 ## Potosí 2308.412 1534.399 ## Tarija 2287.537 2142.638 ## Santa Cruz 2156.938 2134.006 ## Beni 2419.952 1809.108 ## Pando 1778.112 2050.242 tapply(eh18p$ylab,list(eh18p$depto,eh18p$area),quantile,na.rm=T)#con problemas ## Urbana Rural ## Chuquisaca Numeric,5 Numeric,5 ## La Paz Numeric,5 Numeric,5 ## Cochabamba Numeric,5 Numeric,5 ## Oruro Numeric,5 Numeric,5 ## Potosí Numeric,5 Numeric,5 ## Tarija Numeric,5 Numeric,5 ## Santa Cruz Numeric,5 Numeric,5 ## Beni Numeric,5 Numeric,5 ## Pando Numeric,5 Numeric,5 aggregate(eh18p$ylab,list(depto=eh18p$depto,area=eh18p$area),quantile,na.rm=T) ## depto area x.0% x.25% ## 1 Chuquisaca Urbana 39.583336 1317.083344 ## 2 La Paz Urbana 6.666667 1732.000000 ## 3 Cochabamba Urbana 80.000000 1850.833374 ## 4 Oruro Urbana 15.000000 1608.679932 ## 5 Potosí Urbana 160.000000 1724.750000 ## 6 Tarija Urbana 86.599998 1794.533325 ## 7 Santa Cruz Urbana 80.000000 2121.699951 ## 8 Beni Urbana 50.000000 1729.834961 ## 9 Pando Urbana 250.000000 2475.000000 ## 10 Chuquisaca Rural 36.666668 519.599976 ## 11 La Paz Rural 12.500000 300.000000 ## 12 Cochabamba Rural 23.333334 523.649994 ## 13 Oruro Rural 19.166668 286.458336 ## 14 Potosí Rural 25.833334 225.000000 ## 15 Tarija Rural 4.166667 995.883316 ## 16 Santa Cruz Rural 66.666672 1230.000000 ## 17 Beni Rural 100.000000 705.000000 ## 18 Pando Rural 80.000000 1207.500000 ## x.50% x.75% x.100% ## 1 2598.000000 4369.449707 16123.333008 ## 2 2650.000000 3897.000000 23437.484375 ## 3 2700.000000 4000.000000 36196.667969 ## 4 2598.000000 4330.000000 16166.666992 ## 5 3064.550049 4500.000000 14072.500000 ## 6 2814.500000 4330.000000 21833.000000 ## 7 2976.875000 4156.799805 35668.664062 ## 8 2500.000000 4105.924927 22700.000000 ## 9 3291.666748 4538.041748 11367.666016 ## 10 1012.489990 2078.399902 10120.000000 ## 11 825.000000 2249.166748 15433.333008 ## 12 1590.000061 2598.000000 10825.000000 ## 13 653.916687 2051.324982 10175.500000 ## 14 516.250000 1435.208374 7333.333496 ## 15 1967.666687 3366.749939 16730.000000 ## 16 2262.250000 3167.366516 22465.667969 ## 17 1800.000000 3500.000000 7577.500000 ## 18 2167.500000 3449.133362 19750.000000 Finalmente, para explorar a fondo las variables la funcion describe es bastante útil, tambien, el comando summary. #analizando las 5 primeras variables de la base de datos library(Hmisc) describe(eh18p[,1:5]) ## eh18p[, 1:5] ## ## 5 Variables 37517 Observations ## ------------------------------------------------------- ## folio ## n missing distinct ## 37517 0 11195 ## ## lowest : 111-00419704629-A-0011 111-00419704629-A-0021 111-00419704629-A-0041 111-00419704629-A-0051 111-00419704629-A-0071 ## highest: 953-11761951198-D-0081 953-11761951198-D-0091 953-11761951198-D-0101 953-11761951198-D-0111 953-11761951198-D-0121 ## ------------------------------------------------------- ## nro ## n missing distinct Info Mean Gmd ## 37517 0 13 0.948 2.639 1.712 ## .05 .10 .25 .50 .75 .90 ## 1 1 1 2 4 5 ## .95 ## 6 ## ## lowest : 1 2 3 4 5, highest: 9 10 11 12 13 ## ## Value 1 2 3 4 5 6 7 ## Frequency 11195 9399 7215 4892 2654 1230 536 ## Proportion 0.298 0.251 0.192 0.130 0.071 0.033 0.014 ## ## Value 8 9 10 11 12 13 ## Frequency 231 110 39 13 2 1 ## Proportion 0.006 0.003 0.001 0.000 0.000 0.000 ## ------------------------------------------------------- ## depto ## n missing distinct ## 37517 0 9 ## ## lowest : Chuquisaca La Paz Cochabamba Oruro Potosí ## highest: Potosí Tarija Santa Cruz Beni Pando ## ## Chuquisaca (2117, 0.056), La Paz (9970, 0.266), ## Cochabamba (7578, 0.202), Oruro (2188, 0.058), Potosí ## (1855, 0.049), Tarija (3088, 0.082), Santa Cruz (6561, ## 0.175), Beni (2516, 0.067), Pando (1644, 0.044) ## ------------------------------------------------------- ## area ## n missing distinct ## 37517 0 2 ## ## Value Urbana Rural ## Frequency 29212 8305 ## Proportion 0.779 0.221 ## ------------------------------------------------------- ## s02a_02 ## n missing distinct ## 37517 0 2 ## ## Value 1.Hombre 2.Mujer ## Frequency 18419 19098 ## Proportion 0.491 0.509 ## ------------------------------------------------------- summary(eh18p$ylab) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.17 1500.00 2554.70 2959.39 3897.00 36196.67 ## NA&#39;s ## 22852 by(eh18p[,c(&quot;ylab&quot;,&quot;p0&quot;,&quot;s02a_03&quot;)],eh18p$area,summary) ## eh18p$area: Urbana ## ylab p0 s02a_03 ## Min. : 6.67 No Pobre:21225 Min. : 0.00 ## 1st Qu.: 1839.08 Pobre : 7971 1st Qu.:13.00 ## Median : 2788.52 NA&#39;s : 16 Median :26.00 ## Mean : 3238.21 Mean :29.17 ## 3rd Qu.: 4082.50 3rd Qu.:43.00 ## Max. :36196.67 Max. :98.00 ## NA&#39;s :17573 ## ----------------------------------------- ## eh18p$area: Rural ## ylab p0 s02a_03 ## Min. : 4.167 No Pobre:4049 Min. : 0.00 ## 1st Qu.: 476.392 Pobre :4254 1st Qu.:11.00 ## Median : 1300.000 NA&#39;s : 2 Median :27.00 ## Mean : 1886.938 Mean :31.06 ## 3rd Qu.: 2657.667 3rd Qu.:49.00 ## Max. :22465.668 Max. :98.00 ## NA&#39;s :5279 2.1.2 Visualización La visualizacion es una herramienta importante para explorar y entender la base de datos. Los seres humanos son excelentes para capturar patrones visuales, y la visualización de datos intenta capitalizar en estas habilidades. Es util diferenciar las visualizaciones por: Una sola varibles Dos variables Multiples variables Los aspetor vinculados al uso de graficos de origen de R y la libreria ggplot pueden verse en el texto guia de EST-183 “BigData”. A continuación se introducen de forma directa funciones en R orientadas a la visualización univariante, bivariante y multivariante. Usando al EH-2018, para variables cualitativas. #Graficos de origen de R barplot(table(eh18p$s03a_01a),main=&quot;Dónde vivia hace 5 años?&quot;) #GGPLOT library(ggplot2) ggplot(eh18p,aes(x=s03a_01a))+geom_bar()+ggtitle(&quot;Dónde vivia hace 5 años?&quot;) Para variables del tipo cuantitativas par(mfrow=c(1,2)) boxplot(eh18p$ylab) hist(eh18p$ylab) dev.off() ## null device ## 1 ggplot(eh18p,aes(ylab))+geom_boxplot() ## Warning: Removed 22852 rows containing non-finite values ## (stat_boxplot). ggplot(eh18p,aes(ylab))+geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## Warning: Removed 22852 rows containing non-finite values ## (stat_bin). Si ahora queremos comparar usar ambas variables cuanti y cuali boxplot(eh18p$ylab~eh18p$s03a_01a) ggplot(eh18p,aes(x=s03a_01a,y=ylab))+geom_boxplot() ## Warning: Removed 22852 rows containing non-finite values ## (stat_boxplot). ggplot(eh18p,aes(x=s03a_01a,y=ylab))+geom_violin() ## Warning: Removed 22852 rows containing non-finite values ## (stat_ydensity). Usando ambas variables cuantitativas plot(eh18p$tothrs,eh18p$ylab) plot(eh18p[,c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)]) #pairs(eh18p[,c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)]) similar al anterior library(GGally) ggpairs(eh18p,columns = c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)) library(ggridges) # basic example ggplot(eh18p[eh18p$s02a_03&gt;=15,], aes(x = ylab, y = s02a_02, fill = s02a_02)) + geom_density_ridges() + theme_ridges() + theme(legend.position = &quot;none&quot;)+ ggtitle(&quot;Ingreso laboral por sexo, personas de 15 años o más&quot;) Ahora si combinamos variables cuanti y cuali con ggpairs. ggpairs(eh18p,columns = c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;s03a_01a&quot;,&quot;area&quot;)) Alternativas Multivariantes, #trabajando a partir de una muestra de 100 individuos s&lt;-sample(1:dim(eh18p)[1],100) i&lt;-match(c(&quot;s02a_03&quot;,&quot;aestudio&quot;,&quot;ylab&quot;,&quot;tothrs&quot;),names(eh18p)) ggparcoord(eh18p[s,],columns = i,groupColumn = &quot;area&quot;,boxplot=T) library(&quot;TeachingDemos&quot;) faces(na.omit(eh18p[s,i])) 2.2 Componentes Principales El método de Análisis de Componentes Principales se ocupa de explicar la estructura de varianza y covarianza de un grupo de variables a través de unas pocas combinaciones lineales de este grupo de variables. En general sus objetivos son (1) la reducción de los datos y (2) la interpretación. Algebráicamente, los componentes principales son combinaciones lineales de \\(p\\) variables aleatorias \\(X_1\\), \\(X_2\\), , \\(X_p\\). Geométricamente, estas combinaciones lineales representan la selección de un nuevo sistema de coordenadas obtenido por rotación de del sistema original con \\(X_1\\), \\(X_2\\), , \\(X_p\\) como los ejes de coordenadas. Los nuevos ejes representan la dirección con la máxima variabilidad y provee una simple y más parsimoniosa descripción de la estructura de la covarianza. Los componentes principales dependen únicamente de la matriz de covarianza \\(\\Sigma\\) o la matriz de correlaciones \\(\\rho\\) (Matriz estandarizada de \\(\\Sigma\\)) de \\(X_1\\), \\(X_2\\), , \\(X_p\\). Su desarrollo no requiere de ningún supuesto de normalidad multivariada, sin embargo, componentes principales derivados de poblaciones normales multivariantes tienen un gran uso en la interpretación en términos de elipsoide de densidad constante. Sea la matriz \\(\\mathbf{X}\\) compuesta de \\(p\\) vectores aleatorios \\(\\mathbf{X}=[X_1, X_2, \\ldots, X_p ]\\) que tiene la matriz de covarianza \\(\\Sigma\\) con valores propios \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_p \\geq 0\\). Considere la combinación lineal: \\[\\begin{equation} \\begin{array}{rrr} Y_1 = &amp; a_1^{&#39;} \\mathbf{X} = &amp; a_{11} X_1 + a_{12} X_2 + \\ldots a_{1p} X_p \\\\ Y_2 = &amp; a_2^{&#39;} \\mathbf{X} = &amp; a_{21} X_1 + a_{22} X_2 + \\ldots a_{2p} X_p\\\\ \\vdots = &amp; \\vdots &amp; \\vdots \\\\ Y_p = &amp; a_p^{&#39;} \\mathbf{X} = &amp; a_{p1} X_1 + a_{p2} X_2 + \\ldots a_{pp} X_p\\\\ \\end{array} \\label{cp1} \\end{equation}\\] Equivalente a: \\[\\begin{equation} \\mathbf{Y}= \\left[ \\begin{array}{c} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_p\\\\ \\end{array} \\right] = \\left[ \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1p} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{21} &amp; a_{p2} &amp; \\ldots &amp; a_{pp} \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} X_1\\\\ X_2\\\\ \\vdots\\\\ X_p\\\\ \\end{array} \\right] = \\mathbf{A X} \\label{cp2} \\end{equation}\\] La combinación lineal \\(\\mathbf{Y}=\\mathbf{AX}\\) tiene: \\[\\begin{equation} \\mu_y=E(\\mathbf{Y})=E(\\mathbf{AX})=A \\mu_x \\label{cp3} \\end{equation}\\] \\[\\begin{equation} \\Sigma_y=Cov(\\mathbf{Y})=Cov(\\mathbf{AX})=A \\Sigma A^{&#39;} \\label{cp4} \\end{equation}\\] En base a , se obtiene: \\[\\begin{equation} Var(Y_i)=a_i^{&#39;} \\Sigma a_i \\quad i=1, 2, \\ldots, p \\label{cp5} \\end{equation}\\] \\[\\begin{equation} Cov(Y_i,Y_k)=a_i^{&#39;} \\Sigma a_k \\quad i,k=1, 2, \\ldots, p \\label{cp6} \\end{equation}\\] Los componentes principales son combinaciones lineales incorrelacionadas, tal que es lo más grande posible. El primer componente principal es la combinación lineal con máxima varianza. Entonces se debe maximizar \\(Var(Y_1)=a_1^{&#39;} \\Sigma a_1\\). Es claro que \\(Var(Y_1)\\) puede ser incrementada multiplicando a \\(a_1\\) por alguna constante. Para eliminar esta indeterminación, es conveniente restringir los coeficientes del vector. Por lo tanto se define. \\[ \\begin{array}{rcl} \\text{Primer componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_1^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_1^{&#39;}X) \\quad \\text{sujeto a} \\quad a_1^{&#39;} a_1=1\\\\ \\text{Segundo componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_2^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_2^{&#39;}X) \\quad \\text{sujeto a} \\quad a_2^{&#39;} a_2=1 \\quad y \\\\ &amp; &amp; Cov(a_1^{&#39;}X,a_2^{&#39;}X)=0 \\end{array} \\] Para el \\(i-esimo\\) paso: \\[ \\begin{array}{rcl} i-esimo \\text{ componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_i^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_i^{&#39;}X) \\quad \\text{sujeto a} \\quad a_i^{&#39;} a_i=1 \\quad y \\\\ &amp; &amp; Cov(a_i^{&#39;}X,a_k^{&#39;}X)=0 \\quad para \\quad k&lt;i \\end{array} \\] Los pasos sugeridos para iniciar el analisis de componentes principales son: Identificar las variables de interés dentro de la matriz de datos, si las variables tienen las mismas escalas se recomienda emplear la matriz de covarianza, si las escalas son diferentes, se recomienda trabajar con la matriz de correlaciones. Obtener los componentes principales, los eigen valores y la matriz de eigen vectores Eliminar las variables rebundantes, se sugiere identificar las variables del conjunto de datos correlacionadas con los últimos componentes Calcular nuevamente los componentes principales expluyendo las variables identificadas en el paso previo Elegir el número de componentes a retener (scree plot, tamaño de los eigen valores, etc) Analizar los resultados #1. Seleccione una base de datos de interés del repositorio load(url(&quot;https://github.com/AlvaroLimber/EST-384/blob/master/data/oct20.RData?raw=true&quot;)) #2. Seleccione las variables para el PCA (Según la motivación) vv&lt;-c(14:22,24,25) #2A TRANFORMAR vval&lt;-apply(computo[,vv],1,sum) aux&lt;-computo[,vv]/vval #2B LIMPIEZA #3. Calcule el PCA aux1&lt;-na.omit(aux) cp1&lt;-eigen(cov(aux1)) cp2&lt;-eigen(cor(aux1)) #4. Identifique el número de CPs que explican hasta el 90% de la varianza cumsum(cp1$values)/sum(cp1$values) ## [1] 0.6437488 0.8650934 0.9435882 0.9722772 0.9839750 ## [6] 0.9903314 0.9952079 0.9977119 0.9991579 1.0000000 ## [11] 1.0000000 cumsum(cp2$values)/sum(cp2$values) ## [1] 0.2060107 0.3791103 0.4915437 0.5920446 0.6774866 ## [6] 0.7601857 0.8319256 0.8991550 0.9562490 1.0000000 ## [11] 1.0000000 #5. Identifique las variables correlacionadas con la cantidad de #componentes fuera del 90% del paso anterior cp11cov&lt;-as.matrix(aux1)%*%cp1$vectors[,11] cp11cor&lt;-as.matrix(aux1)%*%cp2$vectors[,11] cor(cbind(aux1,cp11cov,cp11cor))[1:11,12:13] ## cp11cov cp11cor ## CC 0.53979440 -0.30842297 ## FPV -0.55376059 0.21633850 ## MTS 0.13333915 0.40515623 ## UCS -0.34677314 0.35428285 ## MAS - IPSP -0.21854796 -0.41971569 ## 21F -0.38433159 0.53263368 ## PDC -0.12174043 -0.01216537 ## MNR -0.21628303 0.41542663 ## PAN-BOL 0.11710134 0.09577694 ## Blancos -0.07666553 0.50453525 ## Nulos -0.11039942 0.18712933 cp11&lt;-eigen(cov(aux1[,-2])) cumsum(cp11$values)/sum(cp11$values) ## [1] 0.6443329 0.8658697 0.9444240 0.9730984 0.9847943 ## [6] 0.9911498 0.9960295 0.9984949 0.9999206 1.0000000 cp10cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,10] #6. Calcule nuevamente el componente principal eliminando las variables rebundantes #7. Determine la cantidad de componentes principales a retener cp1cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,1] cp2cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,2] plot(cp1cov,cp2cov) #8. Defina un indicador a partir de estos cor(cbind(aux1,cp1cov,cp2cov))[1:11,12:13] ## cp1cov cp2cov ## CC 0.92113465 0.35759774 ## FPV -0.08074887 -0.10802564 ## MTS -0.16786500 -0.30855895 ## UCS 0.14139049 -0.22216012 ## MAS - IPSP -0.92337643 0.36777758 ## 21F 0.32455320 -0.47209833 ## PDC -0.09883288 0.43937850 ## MNR 0.19561852 -0.25724983 ## PAN-BOL 0.04776390 -0.01901819 ## Blancos -0.17330606 -0.88168972 ## Nulos -0.08874645 0.06391096 bd&lt;-cbind(aux1,cp1cov,cp2cov) names(bd)[5]&lt;-&quot;MAS&quot; #9. Modele un modelo lineal empleando los CPs retenidos. summary(lm(MAS~cp1cov,data=bd)) ## ## Call: ## lm(formula = MAS ~ cp1cov, data = bd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.39544 -0.05881 0.01417 0.06187 0.16168 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3407735 0.0003141 1085.0 &lt;2e-16 *** ## cp1cov -0.7053686 0.0011240 -627.5 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07868 on 68069 degrees of freedom ## Multiple R-squared: 0.8526, Adjusted R-squared: 0.8526 ## F-statistic: 3.938e+05 on 1 and 68069 DF, p-value: &lt; 2.2e-16 2.3 Análisis de correspondencia El analisis de correspondencia esta orientado a encontrar relaciones entre las categorias de variables cualitativas. Esta técnica es un método visual que va más alla del test de independencia Chi-cuadrado. Para resumir la teoría, primero divida la matriz de datos \\(I × J\\), denotada por \\(N\\), por su gran total \\(n\\) para obtener la llamada matriz de correspondencia \\(P = N / n\\). Deje que los totales marginales de fila y columna de \\(P\\) sean los vectores \\(r\\) y \\(c\\) respectivamente, es decir, los vectores de masas de fila y columna, y \\(Dr\\) y \\(Dc\\) sean las matrices diagonales de estas matrices. El algoritmo computacional para obtener coordenadas de los perfiles de fila y columna con respecto a los ejes principales, usando el SVD, es el siguiente: Calcular la matriz de residuos estadarizados: \\(S=D_r^{-1/2}(P-rc^t)D_c^{-1/2}\\) Calcular la descomposición SVD de \\(S\\): \\(S=UD_{\\alpha}V^t\\), donde \\(U^T U=V^T V=I\\) Coordenadas principales de filas: \\(F=D_r^{-1/2} U D_{\\alpha}\\) Coordenadas principales de columnas: \\(G=D_c^{-1/2} V D_{\\alpha}\\) Coordenadas estándar de filas: \\(X=D_r^{-1/2} U\\) Coordenadas estándar de columnas: \\(Y=D_c^{-1/2} V\\) Calcular la Inercia: \\[\\phi^2=\\sum_i^I\\sum_j^J{\\frac{(p_{ij}-r_i c_j)^2}{r_i c_j}}\\] Graficar las coordenadas de F y G según la la inercia contenida en la matriz \\(D_{\\alpha}\\) En R, existe la libreria ca que permite acceder a las coordenadas del método de correspondencia. #ejemplo CA #install.packages(&quot;ca&quot;) library(dplyr) library(ca) data(&quot;smoke&quot;) model&lt;-ca(smoke) plot(model) names(model) summary(model) #ejemplo ENDSA load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/endsa.RData&quot;)) ll&lt;-attributes(endsa) ll$var.labels t1&lt;-endsa %&gt;% filter(year==2008) %&gt;% select(7,14) %&gt;% table() t1&lt;-t1[1:4,] #test chi2 chisq.test(t1) model&lt;-ca(t1) model plot(model) #programando el ca lcol&lt;-colnames(t1) lrow&lt;-rownames(t1) P&lt;-prop.table(t1) r&lt;-margin.table(P,1) c&lt;-margin.table(P,2) Dr&lt;-diag(r) Dc&lt;-diag(c) ##Paso 1 P-r%*%t(c) #error en las matriz diagonales Dr^(-0.5)%*%(P-r%*%t(c))%*% Dc^(-0.5) # corrigiendo el problema S&lt;-diag(r^(-0.5))%*%(P-r%*%t(c))%*% diag(c^(-0.5)) # 2 descomposición SVD svd(S) U&lt;-svd(S)$u V&lt;-svd(S)$v Da&lt;-diag(svd(S)$d) #verificando las propiedades U %*% t(U) t(V) %*% V U %*% Da %*% t(V) S # 3 Coordenadas principales filas FF&lt;- diag(r^(-0.5)) %*% U %*% Da # 4 Coordenadas principales columnas G&lt;- diag(c^(-0.5)) %*% V %*% Da # 5 Coordenadas estandar filas X&lt;- diag(r^(-0.5)) %*% U # 6 Coordenadas estandar columnas Y&lt;- diag(c^(-0.5)) %*% V # 7 inercia sum(((P-r%*%t(c))**2)/(r%*%t(c))) #graficando xmin&lt;-min(c(FF[,1],G[,1])) xmax&lt;-max(c(FF[,1],G[,1])) ymin&lt;-min(c(FF[,2],G[,2])) ymax&lt;-max(c(FF[,2],G[,2])) plot(FF[,1],FF[,2],col=&quot;red&quot;,xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1) points(G[,1],G[,2],col=&quot;blue&quot;) abline(h=0,v=0,lty=2) #incluyendo el texto plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = &quot;n&quot;) text(FF[,1],FF[,2],labels = lrow,col=&quot;red&quot;,cex=0.7) text(G[,1],G[,2],labels = lcol,col=&quot;blue&quot;,cex=0.7) abline(h=0,v=0,lty=2) #incluyendo mas información plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = &quot;n&quot;) text(FF[,1],FF[,2],labels = lrow,col=&quot;red&quot;,cex=0.5+r*2) text(G[,1],G[,2],labels = lcol,col=&quot;blue&quot;,cex=0.5+c*2) abline(h=0,v=0,lty=2) #viendo solo una dimensión plot(rep(0,dim(FF)[1]),FF[,1],type = &quot;n&quot;, axes = F) axis(2) text(rep(0,dim(FF)[1]),FF[,1],labels = lrow,col=&quot;red&quot;,cex=0.5+r*2) text(rep(0,dim(G)[1]),G[,1],labels = lcol,col=&quot;blue&quot;,cex=0.5+c*2) abline(h=0,v=0,lty=2) "],
["clustering.html", "3 Clustering 3.1 Medidas de Disimilaridad 3.2 Métodos de clustering 3.3 K-center Clustering (no jerárquicos) 3.4 Cluster Jerárquico 3.5 Ejercicios", " 3 Clustering El clustering es un método cuyo objetivo es el de crear grupos en base a las relaciones multivariantes que existen en los datos, este método es un método previo a las técnicas de clasificación que existen. La base del clustering es la definición de la similaridad entre las filas. Similaridad es definida como una función de distancia entre un par de filas. Es importante distinguir la existencia de grupos naturales dentro de los datos, normalmente estos grupos son características naturales de las observaciones de interés. 3.1 Medidas de Disimilaridad Dado el objetivo del clustering, el aspecto mas importante dentro de estos métodos es utilizar de forma correcta la medida de (di)similaridad entre un para de casos dentro de la base de datos. La definición de las medidas de distancia es crucial para aplicar estos modelos. Funciones de distancia incorrecta pueden generar sesgos en los resultados y ser un problema para etapas posteriores de la mineria de datos. Debemos distinguir las funciones de distancia segun la naturaleza de las variables. Sean las filas \\(x\\) e \\(y\\) dentro de una base de datos, estos vectores tienen una dimensión \\(p\\), es decir, se observan \\(p\\) variables para las 2 observaciones. 3.1.1 Distancia Euclideana: Variables numéricas \\[d(x,y)=\\sqrt{\\sum_{i=1}^p{(x_i-y_i)^2}}\\] Donde los \\(x_i\\) y \\(_y_i\\) son los valores para la variable \\(i\\) de las observaciones \\(x\\) e \\(y\\). 3.1.2 Distancia Manhattan: \\(p\\) grande \\[d(x,y)=\\sum_{i=1}^p{|x_i-y_i|}\\] 3.1.3 Distancia Minkowski \\[d(x,y)=\\left(\\sum_{i=1}^p{|x_i-y_i|^d}\\right)^{1/d}\\] aux&lt;-matrix(rnorm(100),nrow=5) dist(aux) #euclideana ## 1 2 3 4 ## 2 5.966749 ## 3 6.566555 5.536907 ## 4 6.498146 6.322441 5.733616 ## 5 5.916694 6.096577 6.773793 6.143012 dist(aux, method=&quot;manhattan&quot;) ## 1 2 3 4 ## 2 23.46720 ## 3 22.08197 22.72832 ## 4 24.53411 20.48697 21.46772 ## 5 20.50470 20.27724 22.05167 22.53056 dist(aux, method=&quot;minkowski&quot;, p=3) ## 1 2 3 4 ## 2 3.979295 ## 3 4.799798 3.624081 ## 4 4.355454 4.592381 3.951028 ## 5 4.315126 4.416634 4.954648 4.194482 3.1.4 programando minkowski&lt;-function(x,y,d){ dd&lt;-(sum(abs(x-y)**d))**(1/d) return(dd) } minkowski(c(1,2,3),c(4,2,1),d=2) ## [1] 3.605551 x&lt;-c(1,2,3) y&lt;-c(4,2,1) sum(abs(x-y)) # manhattan ## [1] 5 sqrt(sum(abs(x-y)**2)) # euclideana ## [1] 3.605551 # la funcion de distancia distancia&lt;-function(bd,d=2){ nf&lt;-dim(bd)[1] DD&lt;-matrix(NA,nf-1,nf-1) colnames(DD)&lt;-1:(nf-1) rownames(DD)&lt;-2:nf for(i in 1:(nf-1)){ for(j in (i+1):nf){ DD[j-1,i]&lt;-minkowski(bd[i,],bd[j,],d) } } return(DD) } distancia(aux,d=2) ## 1 2 3 4 ## 2 5.966749 NA NA NA ## 3 6.566555 5.536907 NA NA ## 4 6.498146 6.322441 5.733616 NA ## 5 5.916694 6.096577 6.773793 6.143012 dist(aux) ## 1 2 3 4 ## 2 5.966749 ## 3 6.566555 5.536907 ## 4 6.498146 6.322441 5.733616 ## 5 5.916694 6.096577 6.773793 6.143012 3.1.5 Variables cualitativas Para las variables cualitativas se debe considerar los casos cuando estas son nominales y ordinales, distinguir tambien los casos de variables binarias. install.packages(&quot;vegan&quot;) library(vegan) 3.1.6 Datos mixtos Una de los mayores desafios es cuando las variables son mixtas, es decir cuantitativas y cualitativas. install.packages(&quot;cluster&quot;) library(cluster) 3.2 Métodos de clustering Partición (k-center) Jerárquicos (dendograma) Basados en densidad Basados en cuadrículas (grid) 3.3 K-center Clustering (no jerárquicos) Algoritmo Partición de las observaciones en \\(k\\) grupos, obtener el vector de promedios de cada grupo (centroides). Se puede trabajar con la media o la mediana. Para cada observación calcular las distancia euclideana a los centroides y reasignar lo observación en base a la menor distancia, recalcular los centroides en base a la reasignación de cada observación Repetir el paso 2 hasta que que ya no existan más reasignaciones bd&lt;-data.frame(x=rnorm(100),y=rnorm(100)) kmeans(bd,2) ## K-means clustering with 2 clusters of sizes 38, 62 ## ## Cluster means: ## x y ## 1 0.9397322 -0.08140439 ## 2 -0.7152533 0.16054516 ## ## Clustering vector: ## [1] 1 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 1 2 2 2 2 1 ## [26] 1 2 1 1 1 2 1 1 2 1 1 1 2 1 2 2 2 2 2 1 2 2 1 2 2 ## [51] 2 1 2 2 2 2 2 1 2 2 2 2 2 2 1 1 1 1 2 2 1 2 1 2 2 ## [76] 2 2 2 2 2 2 1 2 1 1 2 2 1 2 1 1 1 2 1 2 1 2 2 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 48.80637 69.30803 ## (between_SS / total_SS = 35.8 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; ## [4] &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; ## [7] &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; Ejemplo: Implementar el algoritmo para el k-center, con la distancia de Minkowski y para la media y mediana. #Nota: La entrada de la funcion es un data frame kcenter&lt;-function(bd,k=3,d=2,tipo=&quot;media&quot;,seed=123456){ nf&lt;-dim(bd)[1] nc&lt;-dim(bd)[2] #paso1: asignar las k (nf&gt;=k) set.seed(seed) bd$k&lt;-sample(1:k,nf,replace=T) centroide&lt;-NULL for(i in 1:k){ if(tipo==&quot;media&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, mean)) } else if(tipo==&quot;mediana&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, median)) } } #paso2 (recalcular loo centroides al final) cc&lt;-1 while(cc!=0){ #paso3 cc&lt;-0 for(i in 1:nf){ auxd&lt;-NULL for(j in 1:k){ auxd&lt;-c(auxd,minkowski(bd[i,1:nc],centroide[j,1:nc],d=d)) } newk&lt;-which(auxd==min(auxd)) if(newk!=bd$k[i]){ bd$k[i] &lt;- newk cc&lt;-cc+1 } } centroide&lt;-NULL for(i in 1:k){ if(tipo==&quot;media&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, mean)) } else if(tipo==&quot;mediana&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, median)) } } } return(bd) } Pensar en un gráfico que permita ver como se asignaron los cluster bd&lt;-data.frame(x=rnorm(100),y=rnorm(100),z=rnorm(100)) kcenter(bd,k=4,d=1,tipo=&quot;mediana&quot;) ## x y z k ## 1 0.381802373 -0.053229948 -1.33069666 1 ## 2 1.088203468 0.651068275 -0.51990562 1 ## 3 -0.249899370 0.577879654 -0.53684245 1 ## 4 -0.287061038 1.068886636 0.56202515 4 ## 5 0.152321073 -0.541126321 1.57207945 4 ## 6 -1.447260563 -1.389559097 -0.18504876 3 ## 7 0.647898062 0.788820556 -0.37365674 1 ## 8 -0.264433035 -0.905845713 2.08211187 4 ## 9 -0.860485191 -0.630972133 -0.36169358 3 ## 10 -1.453193429 -0.833252152 -1.26987888 3 ## 11 -0.291926101 0.095854154 0.20827789 4 ## 12 1.571771949 2.633014802 -2.13405302 1 ## 13 0.620970684 -1.202029022 0.34157750 2 ## 14 -0.331493636 1.701489170 -0.23685776 1 ## 15 -1.020997153 1.855017145 -1.68995248 3 ## 16 0.733647119 0.928409612 -0.33299127 1 ## 17 -1.620931403 -0.207714201 -1.97237487 3 ## 18 0.272319590 -1.084278021 0.32347897 2 ## 19 -1.089094834 -1.432574060 -1.18747007 3 ## 20 -0.555439460 0.080514448 -0.04146018 3 ## 21 -0.005664276 -1.672591724 -0.37752704 2 ## 22 -0.498191835 -0.906426539 -0.48704394 3 ## 23 -0.149332062 -0.381132108 0.74999059 4 ## 24 0.603328319 -0.355708201 -0.52016779 1 ## 25 0.343017111 -0.670644603 -0.92679810 2 ## 26 0.261599616 1.197750875 -1.63198178 1 ## 27 1.256879504 1.096018207 -0.63572820 1 ## 28 -1.319411050 1.152219723 0.03437943 3 ## 29 -1.145994689 -0.137986482 1.19705215 4 ## 30 0.827723674 -1.318264630 0.85542603 2 ## 31 1.761442961 -1.072779960 -0.43011669 2 ## 32 0.226232884 0.010988954 0.90888514 4 ## 33 -0.919313593 0.127986535 -0.14275284 3 ## 34 0.973750600 0.920828413 -0.34264321 1 ## 35 -1.681186318 -0.001203942 0.75500698 4 ## 36 2.469876944 -1.685660550 0.63603701 2 ## 37 -0.273045676 -1.635627846 -0.83658014 2 ## 38 0.099297412 1.721431702 0.76140670 4 ## 39 0.458142870 -0.097750991 0.51271860 4 ## 40 -0.272781845 0.337879191 -0.25923560 3 ## 41 -2.514060537 -0.611279040 -2.03943542 3 ## 42 -0.011615951 -0.191042266 -0.65238465 3 ## 43 -1.056791014 0.069294449 -1.02857875 3 ## 44 -1.163943943 0.096350840 -1.39774048 3 ## 45 1.028535736 0.170543515 -0.62301238 1 ## 46 1.028717005 -0.934816606 0.62404250 2 ## 47 -0.816360618 0.146111527 0.56478733 4 ## 48 0.481664743 -0.709814901 0.51989056 2 ## 49 0.012558653 0.267408675 0.13340814 4 ## 50 0.692538746 1.196092579 -0.14898525 1 ## 51 0.649157835 -0.340278834 0.51721618 4 ## 52 -0.680769186 -1.590346377 -1.68011128 3 ## 53 0.116653727 -1.071994714 0.56733084 2 ## 54 -0.372278115 -1.548392289 0.21420746 2 ## 55 -0.413571375 1.168114323 -0.78359451 1 ## 56 -0.494128546 -1.798526974 -0.45631447 2 ## 57 1.270755567 -1.728124050 -0.52140704 2 ## 58 -0.833783722 -1.019661471 0.11366508 2 ## 59 -0.289369516 -0.056861531 0.68224459 4 ## 60 0.537101717 -1.786962210 -0.91982598 2 ## 61 -0.353533354 -1.128421724 0.14242973 2 ## 62 0.361128946 0.272279950 0.56966183 4 ## 63 -0.367373993 0.918913397 0.56443210 4 ## 64 -1.308071034 -0.709064757 0.09931917 3 ## 65 0.363781771 0.600300041 1.25084267 4 ## 66 -1.157268744 -1.528309342 0.65069301 2 ## 67 -0.387238163 -0.243560040 1.97783106 4 ## 68 -0.175815026 -0.493380607 0.38743242 4 ## 69 -0.215994716 -0.187486470 1.21239434 4 ## 70 0.125822750 1.014336464 -1.50830620 1 ## 71 0.043898465 -0.694894128 2.91079266 4 ## 72 0.591877823 -0.514743854 0.52220198 2 ## 73 0.670617685 -1.136412823 0.98490951 2 ## 74 0.566821647 0.355308190 -1.08324875 1 ## 75 -0.405243539 -0.377361921 0.63447511 4 ## 76 -0.261515340 -0.034568298 -0.55808822 3 ## 77 1.230273869 0.108360598 -0.70533844 1 ## 78 -0.487005607 0.589729025 1.12477853 4 ## 79 1.569759428 0.944430422 -0.17498957 1 ## 80 0.623485499 0.851742583 2.08937933 1 ## 81 -1.027310211 0.237917530 -0.29944143 3 ## 82 1.827820076 -0.215036705 1.90868499 4 ## 83 -1.085703931 1.130120053 -1.18413495 3 ## 84 0.408097339 0.448767196 -0.04021405 1 ## 85 -0.600843813 0.719044310 -1.26301128 3 ## 86 -1.050820973 -0.966729201 -0.46877698 3 ## 87 0.357982742 1.029322627 -2.61219453 1 ## 88 -1.532520166 0.991392277 -1.89446736 3 ## 89 -0.717781466 1.414925161 0.04768555 3 ## 90 -0.314442059 -0.193111983 -0.05937934 4 ## 91 -2.687489452 0.443963044 -0.50247137 3 ## 92 -1.211978026 -1.293910693 1.60953208 4 ## 93 0.334622052 -0.095044801 1.02239392 4 ## 94 2.166708581 -1.053012004 -0.70030691 2 ## 95 0.363738464 -0.759141691 0.47094467 2 ## 96 0.781041314 0.931473607 0.94789466 1 ## 97 0.001213169 1.213996356 1.06862004 4 ## 98 -0.072951257 1.162093838 1.35342437 4 ## 99 0.363119167 0.709899211 -0.35868988 1 ## 100 0.398491938 -0.790806636 -0.26957218 2 3.3.1 Validación cluster La estructura de los cluster es aleatoria (¿funciona?) ¿Cómo definimos el valor de \\(K\\)? Silhouette coefficient: Se obtiene para la observación \\(i\\) el promedio de distancia a todos los objetos en el mismo cluster (\\(a_i\\)) Se obtiene para la observación \\(i\\) el promedio de distancia a todos los objetos de los otros clusters (\\(b_i\\)) Se define a \\(s_i\\) como el coeficiente, con un recorrido entre \\([-1,1]\\), para cada observación \\(i\\) \\[s_i=\\frac{b_i-a_i}{max(a_i,b_i)}\\] Idealmente se espera que \\(a_i &lt; b_i\\) y los \\(a_i\\) cercanos a \\(0\\). library(cluster) kk&lt;-kmeans(bd,3) s &lt;- silhouette(kk$cluster, dist(bd)) plot(s) #sobre la base IRIS data(&quot;iris&quot;) aux&lt;-kmeans(iris[,-5],3) s &lt;- silhouette(aux$cluster, dist(iris[,-5])) plot(s) Medoide: es el punto de datos que es “menos diferente” de todos los otros puntos de datos. A diferencia del centroide, el medoide tiene que ser uno de los puntos originales. pam(bd,k=3) ## Medoids: ## ID x y z ## [1,] 99 0.3631192 0.7098992 -0.3586899 ## [2,] 48 0.4816647 -0.7098149 0.5198906 ## [3,] 9 -0.8604852 -0.6309721 -0.3616936 ## Clustering vector: ## [1] 1 1 1 1 2 3 1 2 3 3 1 1 2 1 1 1 3 2 3 3 3 3 2 1 3 ## [26] 1 1 1 3 2 2 2 3 1 3 2 3 1 2 1 3 3 3 3 1 2 3 2 1 1 ## [51] 2 3 2 3 1 3 2 3 2 2 3 2 1 3 2 3 2 2 2 1 2 2 2 1 2 ## [76] 3 1 1 1 2 3 2 1 1 1 3 1 3 1 3 3 2 2 2 2 1 1 1 1 2 ## Objective function: ## build swap ## 1.200771 1.158822 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; ## [4] &quot;objective&quot; &quot;isolation&quot; &quot;clusinfo&quot; ## [7] &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; ## [10] &quot;data&quot; kmeans(bd,3) ## K-means clustering with 3 clusters of sizes 29, 37, 34 ## ## Cluster means: ## x y z ## 1 -0.07374988 0.1476459 1.0845637 ## 2 -0.16499758 0.6810607 -0.8366864 ## 3 0.06344588 -1.0985924 -0.1236598 ## ## Clustering vector: ## [1] 2 2 2 1 1 3 2 1 3 3 1 2 3 2 2 2 2 3 3 2 3 3 1 3 3 ## [26] 2 2 2 1 3 3 1 2 2 1 3 3 1 1 2 2 2 2 2 2 3 1 3 1 2 ## [51] 1 3 3 3 2 3 3 3 1 3 3 1 1 3 1 3 1 3 1 2 1 3 3 2 1 ## [76] 2 2 1 2 1 2 1 2 2 2 3 2 2 2 3 2 1 1 3 3 1 1 1 2 3 ## ## Within cluster sum of squares by cluster: ## [1] 37.71076 74.70348 52.91934 ## (between_SS / total_SS = 42.0 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; ## [4] &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; ## [7] &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; ¿Cúal es el número óptimo de \\(k\\)? library(fpc)# Flexible Procedures for Clustering sol &lt;- pamk(iris[,-5], krange=2:10, criterion=&quot;asw&quot;, usepam=TRUE) sol ## $pamobject ## Medoids: ## ID Sepal.Length Sepal.Width Petal.Length ## [1,] 8 5.0 3.4 1.5 ## [2,] 127 6.2 2.8 4.8 ## Petal.Width ## [1,] 0.2 ## [2,] 1.8 ## Clustering vector: ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [26] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [51] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [76] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 ## [101] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [126] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## Objective function: ## build swap ## 0.9901187 0.8622026 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; ## [4] &quot;objective&quot; &quot;isolation&quot; &quot;clusinfo&quot; ## [7] &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; ## [10] &quot;data&quot; ## ## $nc ## [1] 2 ## ## $crit ## [1] 0.0000000 0.6857882 0.5528190 0.4896972 0.4867481 ## [6] 0.4703951 0.3390116 0.3318516 0.2918520 0.2918482 pamk(bd,krange=2:10,usepam = T) ## $pamobject ## Medoids: ## ID x y z ## [1,] 70 0.125822750 1.01433646 -1.50830620 ## [2,] 7 0.647898062 0.78882056 -0.37365674 ## [3,] 20 -0.555439460 0.08051445 -0.04146018 ## [4,] 97 0.001213169 1.21399636 1.06862004 ## [5,] 69 -0.215994716 -0.18748647 1.21239434 ## [6,] 58 -0.833783722 -1.01966147 0.11366508 ## [7,] 44 -1.163943943 0.09635084 -1.39774048 ## [8,] 48 0.481664743 -0.70981490 0.51989056 ## [9,] 37 -0.273045676 -1.63562785 -0.83658014 ## [10,] 31 1.761442961 -1.07277996 -0.43011669 ## Clustering vector: ## [1] 1 2 3 4 5 6 2 5 6 7 3 1 8 2 1 2 ## [17] 7 8 9 3 9 6 5 8 9 1 2 3 5 8 10 5 ## [33] 3 2 3 10 9 4 8 3 7 3 7 7 2 8 3 8 ## [49] 3 2 8 9 8 6 1 9 10 6 5 9 6 5 4 6 ## [65] 4 6 5 8 5 1 5 8 8 2 5 3 2 4 2 4 ## [81] 3 8 7 2 1 6 1 7 4 3 7 5 5 10 8 4 ## [97] 4 4 2 8 ## Objective function: ## build swap ## 0.7278826 0.7202182 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; ## [4] &quot;objective&quot; &quot;isolation&quot; &quot;clusinfo&quot; ## [7] &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; ## [10] &quot;data&quot; ## ## $nc ## [1] 10 ## ## $crit ## [1] 0.0000000 0.2315081 0.2403894 0.2323086 0.2456012 ## [6] 0.2337017 0.2313436 0.2491545 0.2692471 0.2714183 3.3.2 Distancias para variables nominales (todas nominales) En este caso la mejor estrategia es llevar las variables con sus categorias a variables binarias. Existen múltiples medidas de distancia para variables binarias, muchas de estas medidas son aproximaciones a las medidas mas conocidas. Entre ellas: Sean las filas \\(i\\), \\(j\\) que contienen los valores binarios de las variables de estudio. Sea \\(A\\) el total de \\(1\\) que existe en \\(i\\), \\(B\\) el total de \\(1\\) que existe en \\(j\\) y sea \\(J\\) el total de casos en los que los \\(1\\) ocurren simultaneamente en \\(i\\) y \\(j\\). Euclideana \\[d_{ij}=\\sqrt{A+B-2J}\\] * Manhattan \\[d_{ij}=A+B-2J\\] * Bray \\[d_{ij}=\\frac{A+B-2J}{A+B}\\] Binomial \\[d_{ij}=log(2)(A+B-2J)\\] aux&lt;-rbind(c(0,0,0,0,1,1,1),c(1,0,1,0,0,1,1)) A&lt;-sum(aux[1,]) B&lt;-sum(aux[2,]) J&lt;-sum(apply(aux, 2, sum)==2) #euclideana sqrt(A+B-2*J) ## [1] 1.732051 #manhathan A+B-2*J ## [1] 3 #bray (A+B-2*J)/(A+B) ## [1] 0.4285714 #binomial log(2)*(A+B-2*J) ## [1] 2.079442 library(vegan) vegdist(aux,binary = T) ## 1 ## 2 0.4285714 vegdist(aux,binary = F) ## 1 ## 2 0.4285714 #una base de datos mas grandes set.seed(999) aux1&lt;-matrix(rbinom(200,1,0.4),nrow = 20) vegdist(aux1,method = &quot;binomial&quot;,binary = T) ## 1 2 3 4 5 ## 2 3.4657359 ## 3 3.4657359 2.7725887 ## 4 2.7725887 4.8520303 4.8520303 ## 5 4.1588831 3.4657359 4.8520303 2.7725887 ## 6 2.7725887 3.4657359 3.4657359 4.1588831 1.3862944 ## 7 3.4657359 2.7725887 2.7725887 2.0794415 3.4657359 ## 8 2.0794415 1.3862944 2.7725887 4.8520303 3.4657359 ## 9 2.7725887 3.4657359 4.8520303 4.1588831 2.7725887 ## 10 2.7725887 4.8520303 3.4657359 2.7725887 2.7725887 ## 11 3.4657359 2.7725887 4.1588831 3.4657359 4.8520303 ## 12 2.7725887 4.8520303 3.4657359 2.7725887 4.1588831 ## 13 4.1588831 4.8520303 3.4657359 2.7725887 2.7725887 ## 14 4.8520303 4.1588831 4.1588831 3.4657359 3.4657359 ## 15 1.3862944 3.4657359 2.0794415 2.7725887 4.1588831 ## 16 3.4657359 4.1588831 5.5451774 2.0794415 0.6931472 ## 17 3.4657359 2.7725887 4.1588831 3.4657359 3.4657359 ## 18 2.7725887 4.8520303 3.4657359 4.1588831 4.1588831 ## 19 3.4657359 4.1588831 1.3862944 4.8520303 3.4657359 ## 20 4.8520303 2.7725887 4.1588831 3.4657359 3.4657359 ## 6 7 8 9 10 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 4.8520303 ## 8 2.0794415 4.1588831 ## 9 1.3862944 6.2383246 2.0794415 ## 10 2.7725887 3.4657359 3.4657359 2.7725887 ## 11 4.8520303 2.7725887 4.1588831 3.4657359 3.4657359 ## 12 2.7725887 3.4657359 4.8520303 2.7725887 2.7725887 ## 13 2.7725887 4.8520303 4.8520303 2.7725887 2.7725887 ## 14 3.4657359 4.1588831 4.1588831 2.0794415 2.0794415 ## 15 2.7725887 3.4657359 2.0794415 2.7725887 1.3862944 ## 16 2.0794415 4.1588831 4.1588831 2.0794415 2.0794415 ## 17 3.4657359 5.5451774 2.7725887 2.0794415 4.8520303 ## 18 4.1588831 3.4657359 4.8520303 4.1588831 2.7725887 ## 19 2.0794415 4.1588831 2.7725887 3.4657359 2.0794415 ## 20 3.4657359 4.1588831 4.1588831 2.0794415 3.4657359 ## 11 12 13 14 15 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 2.0794415 ## 13 3.4657359 2.7725887 ## 14 2.7725887 2.0794415 3.4657359 ## 15 3.4657359 2.7725887 2.7725887 3.4657359 ## 16 4.1588831 3.4657359 2.0794415 2.7725887 3.4657359 ## 17 4.1588831 4.8520303 2.0794415 4.1588831 3.4657359 ## 18 3.4657359 2.7725887 4.1588831 3.4657359 4.1588831 ## 19 4.1588831 3.4657359 2.0794415 4.1588831 2.0794415 ## 20 1.3862944 2.0794415 2.0794415 1.3862944 3.4657359 ## 16 17 18 19 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## 17 2.7725887 ## 18 3.4657359 4.8520303 ## 19 4.1588831 4.1588831 3.4657359 ## 20 2.7725887 2.7725887 4.8520303 4.1588831 dist(aux1) ## 1 2 3 4 5 ## 2 2.236068 ## 3 2.236068 2.000000 ## 4 2.000000 2.645751 2.645751 ## 5 2.449490 2.236068 2.645751 2.000000 ## 6 2.000000 2.236068 2.236068 2.449490 1.414214 ## 7 2.236068 2.000000 2.000000 1.732051 2.236068 ## 8 1.732051 1.414214 2.000000 2.645751 2.236068 ## 9 2.000000 2.236068 2.645751 2.449490 2.000000 ## 10 2.000000 2.645751 2.236068 2.000000 2.000000 ## 11 2.236068 2.000000 2.449490 2.236068 2.645751 ## 12 2.000000 2.645751 2.236068 2.000000 2.449490 ## 13 2.449490 2.645751 2.236068 2.000000 2.000000 ## 14 2.645751 2.449490 2.449490 2.236068 2.236068 ## 15 1.414214 2.236068 1.732051 2.000000 2.449490 ## 16 2.236068 2.449490 2.828427 1.732051 1.000000 ## 17 2.236068 2.000000 2.449490 2.236068 2.236068 ## 18 2.000000 2.645751 2.236068 2.449490 2.449490 ## 19 2.236068 2.449490 1.414214 2.645751 2.236068 ## 20 2.645751 2.000000 2.449490 2.236068 2.236068 ## 6 7 8 9 10 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 2.645751 ## 8 1.732051 2.449490 ## 9 1.414214 3.000000 1.732051 ## 10 2.000000 2.236068 2.236068 2.000000 ## 11 2.645751 2.000000 2.449490 2.236068 2.236068 ## 12 2.000000 2.236068 2.645751 2.000000 2.000000 ## 13 2.000000 2.645751 2.645751 2.000000 2.000000 ## 14 2.236068 2.449490 2.449490 1.732051 1.732051 ## 15 2.000000 2.236068 1.732051 2.000000 1.414214 ## 16 1.732051 2.449490 2.449490 1.732051 1.732051 ## 17 2.236068 2.828427 2.000000 1.732051 2.645751 ## 18 2.449490 2.236068 2.645751 2.449490 2.000000 ## 19 1.732051 2.449490 2.000000 2.236068 1.732051 ## 20 2.236068 2.449490 2.449490 1.732051 2.236068 ## 11 12 13 14 15 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 1.732051 ## 13 2.236068 2.000000 ## 14 2.000000 1.732051 2.236068 ## 15 2.236068 2.000000 2.000000 2.236068 ## 16 2.449490 2.236068 1.732051 2.000000 2.236068 ## 17 2.449490 2.645751 1.732051 2.449490 2.236068 ## 18 2.236068 2.000000 2.449490 2.236068 2.449490 ## 19 2.449490 2.236068 1.732051 2.449490 1.732051 ## 20 1.414214 1.732051 1.732051 1.414214 2.236068 ## 16 17 18 19 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## 17 2.000000 ## 18 2.236068 2.645751 ## 19 2.449490 2.449490 2.236068 ## 20 2.000000 2.000000 2.645751 2.449490 3.3.3 Distancias para variables mixtas (cuantitativas, nominales, ordinales) library(cluster) data(&quot;flower&quot;) str(flower) ## &#39;data.frame&#39;: 18 obs. of 8 variables: ## $ V1: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 1 1 1 1 1 2 2 ... ## $ V2: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 1 2 2 1 1 2 2 ... ## $ V3: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 2 1 1 1 2 1 1 ... ## $ V4: Factor w/ 5 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 4 2 3 4 5 4 4 2 3 5 ... ## $ V5: Ord.factor w/ 3 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;: 3 1 3 2 2 3 3 2 1 2 ... ## $ V6: Ord.factor w/ 18 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;&lt;..: 15 3 1 16 2 12 13 7 4 14 ... ## $ V7: num 25 150 150 125 20 50 40 100 25 100 ... ## $ V8: num 15 50 50 50 15 40 20 15 15 60 ... dd&lt;-daisy(flower,metric = &quot;gower&quot;) summary(dd) ## 153 dissimilarities, summarized : ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1418 0.3904 0.4829 0.4865 0.5865 0.8875 ## Metric : mixed ; Types = N, N, N, N, O, O, I, I ## Number of objects : 18 Ejercicios Busque funciones en R que permitan calcular los k-center para variables mixtas y medoides Crear una función k-center para variables mixtas y alternativas para incluir el medoide. 3.4 Cluster Jerárquico El objetivo es obtener una jerarquía de posibles soluciones que van desde un solo grupo a \\(n\\) grupos, donde \\(n\\) es el número de observaciones en el conjunto de datos. 3.4.1 Algoritmo (Johnson) Se inicia con \\(n\\) grupos y se genera una matriz de \\(nxn\\) de distancias, \\(D=\\{d_{ik}\\}\\) Buscar en la matriz de distancia los pares de cluster más cercanos entre ellos, “los cluster mas similares”, si defifinimos los clusters \\(V\\) y \\(U\\), estamos interesados en encontrar \\(d_{UV}\\) Unir los cluster \\(U\\) y \\(V\\), re etiquetar el nuevo cluster como \\(UV\\). Actualizar la matriz de distancias a) remover las filas y columnas correspondientes a \\(U\\) y \\(V\\) b) incluimos las nuevas filas y columnas para el nuevo cluster \\(UV\\). Repetimos el paso 2 y 3 un total de \\(n-1\\) veces. El momento de definir el cluster más cercano, se puede emplear los siguientes enlaces: Single linkage (Enlace simple): Se elige al cluster más cercano, con la regla de que las distincia individual entre las observaciones dentro de los clusters es la más corta Complete linkage (Enlace completo): Se elige el cluster mas cercano, con la regla que las distnaicas individuales entre las observaciones dentro de los cluster es la más larga Average linkage (Enlace promedio): Se elige el cluster mas cercano, considerando el promedio de las distancias entre los cluster. Nota: Se debe elegir la matriz de distancias acorde a la naturaleza de los datos, se recomienda: Todas Numéricas: Euclideana o Manhatan Todas nominales: Transformación a binarias y usar la distancia binomial Mixtas: Distancia de Gower d &lt;- dist(scale(iris[,-5]))#euclideana h &lt;- hclust(d) plot(h,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) #elegir la cantidad de grupos clus3 &lt;- cutree(h, 5) plot(h,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(h,k=5) Probando los tres tipos de enlaces para \\(k=3\\) hs &lt;- hclust(d,method = &quot;single&quot;) hc &lt;- hclust(d,method = &quot;complete&quot;) ha &lt;- hclust(d,method = &quot;average&quot;) cs&lt;-cutree(hs,3) cc&lt;-cutree(hc,3) ca&lt;-cutree(ha,3) table(cs,cc) ## cc ## cs 1 2 3 ## 1 49 0 0 ## 2 0 1 0 ## 3 0 23 77 table(cs,ca) ## ca ## cs 1 2 3 ## 1 49 0 0 ## 2 1 0 0 ## 3 0 97 3 table(cc,ca) ## ca ## cc 1 2 3 ## 1 49 0 0 ## 2 1 23 0 ## 3 0 74 3 par(mfrow=c(1,3)) plot(hs,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(hs,k=3) plot(hc,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(hc,k=3) plot(ha,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(ha,k=3) Nota: El dendograma es muy útil para ver las relaciones que existen basadas en las distancias y la creación de las jerarquias, a partir de estos se puede definir un \\(k\\) (de forma visual) Nota: El dendograma pierde su utilidad cuando la cantidad de observaciones es muy alta, Ejemplo, Usar los datos de las elecciones del 20 de octubre, agregar los resultados en términos relativos para los municipios y generar el dendograma para los tres tipos de enlaces, de forma visual sugerir un valor de \\(k\\) para cada tipo de enlace. library(dplyr) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\oct20.RData&quot;) #filtrar los casos aux&lt;-c(&quot;Número departamento&quot;,&quot;Departamento&quot; ,&quot;Número municipio&quot;,&quot;Municipio&quot; ,&quot;CC&quot;,&quot;FPV&quot;,&quot;MTS&quot;,&quot;UCS&quot;,&quot;MAS - IPSP&quot;,&quot;21F&quot;,&quot;PDC&quot;,&quot;MNR&quot;,&quot;PAN-BOL&quot;,&quot;Votos Válidos&quot;,&quot;Blancos&quot;,&quot;Nulos&quot;) names(computo)[1]&lt;-&quot;pais&quot; names(computo)[12]&lt;-&quot;eleccion&quot; bd&lt;-computo %&gt;% filter(pais==&quot;Bolivia&quot; &amp; eleccion==&quot;Presidente y Vicepresidente&quot;) %&gt;% select(aux) names(bd)[1:4]&lt;-c(&quot;idep&quot;,&quot;ddep&quot;,&quot;imun&quot;,&quot;dmun&quot;) bdmun&lt;-aggregate(bd[,5:16],bd[,1:4],sum) bdmun&lt;-bdmun[,-14] bdmun[,5:15]&lt;-prop.table(as.matrix(bdmun[,5:15]),1) #cluster jerarquico d&lt;-dist(bdmun[,5:15]) plot(hclust(d),hang=-0.1,label=bdmun$dmun,cex=0.5) # determinar el mejor k y el mejor enlace mm&lt;-c(&quot;single&quot;, &quot;complete&quot;, &quot;average&quot;) # método k&lt;-2:20 # cantidad de cluster d&lt;-dist(bdmun[,5:15]) # matriz de distancia # matriz de resultados res&lt;-matrix(NA, nrow = 19,ncol=3) colnames(res)&lt;-mm rownames(res)&lt;-k ####################### for(i in k){ for(j in 1:3){ h&lt;-hclust(d,method = mm[j]) c&lt;-cutree(h,i) s&lt;-silhouette(c,d) res[i-1,j]&lt;-median(s[,3]) } } #la mejor opción es k=2 con el método average h&lt;-hclust(d,method = &quot;average&quot;) c&lt;-cutree(h,2) plot(h,hang=-0.1,labels=bdmun$dmun,cex=0.4) rect.hclust(h,k=2) bdmun$cluster&lt;-c group_by(bdmun,cluster) %&gt;% summarise(mean(CC),mean(`MAS - IPSP`)) Algunas alternativas para la visualización son: library(ape) h$labels&lt;-bdmun$dmun plot(as.phylo(h),type=&quot;fan&quot;) library(dendextend) library(circlize) dend &lt;- as.dendrogram(h) # modify the dendrogram to have some colors in the branches and labels dend &lt;- dend %&gt;% color_branches(k=4) %&gt;% color_labels # plot the radial plot png(&quot;dendo.png&quot;,width = 1500,height = 1500) par(mar = rep(0,4)) # circlize_dendrogram(dend, ) circlize_dendrogram(dend,dend_track_height = 0.8) dev.off() 3.5 Ejercicios Pensar en un gráfico que permita ver como se asignaron los cluster Pensar en optimizar el código empleado para el k-center Hacer que la función desarrollada para el k-center retorne también los centroides Utilizando la base de datos de las elecciones del 20 de octubre, crear una base de datos a nivel municipal, aplicar el método k-center con medoides para los resultados a nivel municipal y terminar el \\(k\\) óptimo en un rango de \\(k=2:10\\) (Usar datos relativos). "],
["regresión.html", "4 Regresión 4.1 Regresión lineal 4.2 Probit y Logit", " 4 Regresión \\[y=f(x_1,x_2, \\ldots)\\] \\(y\\) Variable de resultado, dependiente, solo tenemos a una \\(y\\). \\(x_1, x_2, \\ldots\\), variables de control, independientes. A partir de estas variables: ¿Cuál es la relación de \\(x\\) sobre \\(y\\)? Lineal \\[y_i=\\beta_0+\\beta_1 x_1+\\beta_2x_2+\\ldots+\\epsilon_i\\] &gt; Nota: Diferenciar que la regresión busca establacer relaciones basadas en los datos y no asi un proceso causal. + Polinomial + Etc; No lineal, Conocer la naturaleza de \\(y\\) y las variables \\(x\\) \\(Y\\) es cuanti (real), \\(X\\) cuanti. \\(Y\\) es cuanti (discreta &gt;= 0), \\(X\\) cuanti. \\(Y\\) es cuali nominal binario, \\(X\\) mixtas. \\(Y\\) es cuali ordinal, \\(X\\) mixtas. 4.1 Regresión lineal 4.1.1 Paso 1: Base de datos library(dplyr) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\eh18.Rdata&quot;) bd&lt;-eh18p %&gt;% filter(s02a_03&gt;=18 &amp; s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot; &amp; ocupado==&quot;Si&quot;) %&gt;% select(s02a_02,s02a_03,aestudio,tothrs,ylab,ynolab,factor,estrato, upm,area,permanente,cob_op) 4.1.2 Paso 2: Establecer la relación de interés. \\(Y\\) Ingreso laboral puede ser un buena opción \\(X\\) el resto, pueden ser basadas en un modelo teórico o buscadas a partir de un proceso de minería de datos \\[IngresoLaboral=f(edad,sexo,educación,...)\\] 4.1.3 Paso 3: Definir el modelo a utilizar OLS, MCO. Modelos lineales m1&lt;-lm(ylab~s02a_03,data=bd)# regresión lineal simple y=f(x) m1 \\[ylab_i=\\beta_0+\\beta_1edad_i+\\epsilon_i\\] \\[E[ylab_i]=4112.16-21.84*edad_i\\] plot(bd$s02a_03,bd$ylab) abline(m1,col=&quot;red&quot;,lwd=2) summary(m1) #en los betas coefficients(m1) confint(m1,level=0.99) #mejorar el modelo m2&lt;-lm(ylab~s02a_03+aestudio+tothrs+ynolab+s02a_02+area+cob_op,data=bd) summary(m2) m3&lt;-lm(log(ylab)~s02a_03+aestudio+tothrs+ynolab+s02a_02+area+cob_op,data=bd) summary(m3) m4&lt;-lm(log(ylab)~s02a_03+factor(aestudio)+tothrs+ynolab+s02a_02+area+cob_op,data=bd) summary(m4) 4.1.4 Paso 4: Optimizar el modelo m5&lt;-lm(log(ylab)~.,data=bd[,-c(7,8,9,11)]) summary(m5) m6&lt;-step(m5) summary(m6) #un ejemplo de laboratorio bd2&lt;-as.data.frame(matrix(rnorm(1000),ncol = 8)) names(bd2)[1]&lt;-&quot;y&quot; bd2$x&lt;-bd2$y+runif(125) plot(bd2$x,bd2$y) plot(bd2) p1&lt;-lm(y~.,data=bd2) summary(p1) p2&lt;-step(p1) summary(p2) Nota: Se debe tener en cuenta siempre la fuente de la información summary(m4) summary(lm(ylab~factor(aestudio)+s02a_02+s02a_03,data=bd)) summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03,data=bd)) #estamos suponiendo una relación lineal #las relaciones que encontramos son a nivel de la muestra #para hacer inferencia el mejor camino es la libreria survey summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03,data=bd))#MCO,OLS summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03 ,weights = factor ,data=bd))#MCP 4.1.5 Paso 5: Validar el modelo #Supuestos del modelo ## los errores se distribuyen normal(media=0, varianza=constante) ## Independencia entre los X del modelo # los errores se distribuyen normal model&lt;-lm(log(ylab)~s02a_03+aestudio+tothrs+ynolab+s02a_02+area+cob_op,data=bd) summary(model) par(mfrow=c(2,2)) plot(model) dev.off() #errores ee&lt;-residuals(model) plot(density(ee)) #prueba de normalidad #H0 x ~ normal() library(normtest) library(nortest) ad.test(ee) lillie.test(ee) boxplot(bd$ylab) #La normalidad de los errores plot(density(ee)) curve(dnorm(x,mean(ee),sd(ee)),add=T,col=&quot;red&quot;) #limitar los datos hasta el percentil punto&lt;-c(0.01,0.90) puntonl&lt;-c(0.99) z&lt;-quantile(bd$ylab,punto,na.rm=T) znl&lt;-quantile(bd$ynolab,puntonl,na.rm=T) znl bd2&lt;-bd %&gt;% filter((ylab&lt;z[2] &amp; ylab&gt;z[1])) boxplot(bd2$ynolab) #definiendo el modelo sin atípicos model1&lt;-lm(log(ylab)~s02a_03+aestudio+tothrs+s02a_02+area,data=bd2) summary(model1) boxplot(bd2$ylab) ee1&lt;-residuals(model1) ad.test(ee1) lillie.test(ee1) ks.test(ee1,&quot;pnorm&quot;,mean(ee1),sd(ee1))#kolmogorov Smirnofv plot(density(ee1)) curve(dnorm(x,mean(ee1),sd(ee1)),add=T,col=&quot;red&quot;) plot(model1) # Distancia de Cook plot(cooks.distance(model1)) cc&lt;-cooks.distance(model1) bd3&lt;-bd2[cc&lt;quantile(cc,0.90),] model3&lt;-lm(log(ylab)~s02a_03+aestudio+s02a_02+area,data=bd3) summary(model3) ad.test(residuals(model3)) lillie.test(residuals(model3)) plot(density(residuals(model3))) curve(dnorm(x,mean(residuals(model3)),sd(residuals(model3))),add=T,col=&quot;red&quot;) plot(model3) plot(cooks.distance(model3)) #ajustando polinomios bd3&lt;-na.omit(bd3) model4&lt;-lm(log(ylab)~poly(s02a_03,2)+poly(aestudio,3)+s02a_02+area,data=bd3) summary(model4) ad.test(residuals(model4)) ## interacciones entre variables model5&lt;-lm(log(ylab)~poly(s02a_03,2)+poly(aestudio,3)+s02a_02+area+s02a_02:aestudio+area:aestudio+exp(aestudio)+I(aestudio^4),data=bd3) summary(model5) ad.test(residuals(model5)) #trabajando sobre los valores atípicos desde R library(MASS) modela&lt;-rlm(ylab~s02a_02+s02a_03+area,data=bd2) modelb&lt;-lm(ylab~s02a_02+s02a_03+area,data=bd2) summary(modela) summary(modelb) ad.test(residuals(model)) # Colinealidad (X1=f(X2) library(car) vif(model3) sqrt(vif(model3))&gt;2 ##Variance Inflation Factors # Verificar si la varianza es constante (homocedástico) o no (heterocedástico) library(lmtest) bptest(model3) # H0: Homocedsticidad https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test #corrigiendo library(rms) model1 = ols(ylab~s02a_02+s02a_03+area,data=bd3,x=T,y=T) bptest(model1) aux&lt;-robcov(model1) aux # H0: Homocedasticidad, implica que los EE de B no son los correctos 4.1.6 Paso 6: Predicir a partir del modelo test&lt;-bd3 yest&lt;-predict(model3,test) plot(log(bd3$ylab),yest) plot(bd3$ylab,exp(yest)) 4.2 Probit y Logit Estrategia, llevar valores binarios a valores continuos. Mediante una función de enlace (\\(F(Y)\\)). \\[F(Y)=Y&#39;=X \\beta +\\epsilon\\] Probit: \\[Y=\\Phi (X \\beta +\\epsilon)\\] \\[\\phi^{-1}(Y)=X \\beta +\\epsilon\\] \\[Y&#39;=X \\beta +\\epsilon\\] El enlace \\(F(Y)=\\Phi^{-1}(Y)\\), es conocida como probit. Logit: \\[logit(Y)=log(\\frac{Y}{1-Y})=X\\beta+\\epsilon\\] \\[Y=\\frac{e^{X\\beta+\\epsilon}}{1+e^{X\\beta+\\epsilon}}\\] 4.2.1 En R: library(dplyr) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\eh18.Rdata&quot;) aux&lt;-levels(eh18p$s04a_01a) eh18p&lt;-eh18p %&gt;% mutate(diabetes=(s04a_01a==aux[1] | s04a_01b==aux[1]),corazon=(s04a_01a==aux[4] | s04a_01b==aux[4]),hiper=(s04a_01a==aux[10] | s04a_01b==aux[10])) eh18p$diabetes&lt;-(eh18p$diabetes==T); eh18p$diabetes[is.na(eh18p$diabetes)]&lt;-F eh18p$corazon&lt;-(eh18p$corazon==T); eh18p$corazon[is.na(eh18p$corazon)]&lt;-F eh18p$hiper&lt;-(eh18p$hiper==T); eh18p$hiper[is.na(eh18p$hiper)]&lt;-F eh18p$cronicas&lt;-(eh18p$diabetes+eh18p$corazon+eh18p$hiper) #modelo para las enfermedades crónicas eh18p$cronicas&lt;-(eh18p$cronicas!=0) #probit logit logit&lt;-glm(cronicas~s02a_02+s02a_03,data=eh18p,family = binomial(link=&quot;logit&quot;)) probit&lt;-glm(cronicas~s02a_02+s02a_03,data=eh18p,family = binomial(link=&quot;probit&quot;)) lineal&lt;-lm(cronicas~s02a_02+s02a_03,data=eh18p) #resumen summary(logit) summary(probit) summary(lineal) #score probabilidades lres&lt;-predict(logit,eh18p,type=&quot;response&quot;) pres&lt;-predict(probit,eh18p,type=&quot;response&quot;) plot(density(lres)) points(density(pres),type = &quot;l&quot;,col=&quot;red&quot;) #efectos marginales library(mfx) probitmfx(cronicas~s02a_02+s02a_03,data=eh18p) logitmfx(cronicas~s02a_02+s02a_03,data=eh18p) #ajuste library(DescTools) PseudoR2(logit) PseudoR2(probit) summary(lineal) #comparando library(memisc) mtable(logit,probit,lineal) summary(glm(cronicas~s02a_02+s02a_03,data=eh18p)) summary(lm(cronicas~s02a_02+s02a_03,data=eh18p)) "],
["clasificación.html", "5 Clasificación 5.1 Logit Probit 5.2 Arboles de clasificación (CART) 5.3 Naive Bayes 5.4 Ejercicios.", " 5 Clasificación Pasos sugeridos Preparar la base de datos; base de entrenamiento (trainbd) y de testeo (testbd). 70/30 Definir el modelo de clasificación * Regresión logística (logit) * Árbol de clasificación (CART) * Naive bayes Base de datos, se empleara la base de datos de covid de México. library(dplyr) covid&lt;-read.csv(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\covid_mx\\\\200627COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,na.strings = c(99,98)) object.size(covid)/10^6 ## 123.5 bytes vv&lt;-c(&quot;SEXO&quot;,&quot;FECHA_DEF&quot;,&quot;NEUMONIA&quot;,&quot;EDAD&quot;,&quot;HABLA_LENGUA_INDIG&quot;,&quot;DIABETES&quot;,&quot;EPOC&quot;,&quot;ASMA&quot;,&quot;INMUSUPR&quot;,&quot;HIPERTENSION&quot;,&quot;OTRA_COM&quot;,&quot;CARDIOVASCULAR&quot;,&quot;OBESIDAD&quot;,&quot;RENAL_CRONICA&quot;,&quot;TABAQUISMO&quot;,&quot;RESULTADO&quot;) covid&lt;-covid[,vv] covid&lt;-covid %&gt;% filter(EDAD&lt;=90) #Descripción Hmisc::describe(covid) ## covid ## ## 16 Variables 549476 Observations ## ------------------------------------------------------- ## SEXO ## n missing distinct Info Mean Gmd ## 549476 0 2 0.75 1.507 0.4999 ## ## Value 1 2 ## Frequency 270904 278572 ## Proportion 0.493 0.507 ## ------------------------------------------------------- ## FECHA_DEF ## n missing distinct ## 549476 0 122 ## ## lowest : 2020-01-13 2020-01-14 2020-01-15 2020-01-29 2020-01-30 ## highest: 2020-06-24 2020-06-25 2020-06-26 2020-06-27 9999-99-99 ## ------------------------------------------------------- ## NEUMONIA ## n missing distinct Info Mean Gmd ## 549465 11 2 0.392 1.845 0.2615 ## ## Value 1 2 ## Frequency 84969 464496 ## Proportion 0.155 0.845 ## ------------------------------------------------------- ## EDAD ## n missing distinct Info Mean Gmd ## 549476 0 91 1 42.45 18.47 ## .05 .10 .25 .50 .75 .90 ## 19 24 31 41 53 65 ## .95 ## 72 ## ## lowest : 0 1 2 3 4, highest: 86 87 88 89 90 ## ------------------------------------------------------- ## HABLA_LENGUA_INDIG ## n missing distinct Info Mean Gmd ## 531854 17622 2 0.029 1.99 0.0196 ## ## Value 1 2 ## Frequency 5264 526590 ## Proportion 0.01 0.99 ## ------------------------------------------------------- ## DIABETES ## n missing distinct Info Mean Gmd ## 547557 1919 2 0.328 1.875 0.2187 ## ## Value 1 2 ## Frequency 68437 479120 ## Proportion 0.125 0.875 ## ------------------------------------------------------- ## EPOC ## n missing distinct Info Mean Gmd ## 547788 1688 2 0.047 1.984 0.03108 ## ## Value 1 2 ## Frequency 8650 539138 ## Proportion 0.016 0.984 ## ------------------------------------------------------- ## ASMA ## n missing distinct Info Mean Gmd ## 547781 1695 2 0.093 1.968 0.06216 ## ## Value 1 2 ## Frequency 17589 530192 ## Proportion 0.032 0.968 ## ------------------------------------------------------- ## INMUSUPR ## n missing distinct Info Mean Gmd ## 547565 1911 2 0.047 1.984 0.03133 ## ## Value 1 2 ## Frequency 8715 538850 ## Proportion 0.016 0.984 ## ------------------------------------------------------- ## HIPERTENSION ## n missing distinct Info Mean Gmd ## 547713 1763 2 0.409 1.837 0.2724 ## ## Value 1 2 ## Frequency 89094 458619 ## Proportion 0.163 0.837 ## ------------------------------------------------------- ## OTRA_COM ## n missing distinct Info Mean Gmd ## 546958 2518 2 0.088 1.97 0.05884 ## ## Value 1 2 ## Frequency 16596 530362 ## Proportion 0.03 0.97 ## ------------------------------------------------------- ## CARDIOVASCULAR ## n missing distinct Info Mean Gmd ## 547716 1760 2 0.066 1.978 0.04372 ## ## Value 1 2 ## Frequency 12246 535470 ## Proportion 0.022 0.978 ## ------------------------------------------------------- ## OBESIDAD ## n missing distinct Info Mean Gmd ## 547756 1720 2 0.411 1.836 0.274 ## ## Value 1 2 ## Frequency 89749 458007 ## Proportion 0.164 0.836 ## ------------------------------------------------------- ## RENAL_CRONICA ## n missing distinct Info Mean Gmd ## 547749 1727 2 0.058 1.98 0.03897 ## ## Value 1 2 ## Frequency 10889 536860 ## Proportion 0.02 0.98 ## ------------------------------------------------------- ## TABAQUISMO ## n missing distinct Info Mean Gmd ## 547637 1839 2 0.234 1.915 0.156 ## ## Value 1 2 ## Frequency 46713 500924 ## Proportion 0.085 0.915 ## ------------------------------------------------------- ## RESULTADO ## n missing distinct Info Mean Gmd ## 549476 0 3 0.821 1.736 0.688 ## ## Value 1 2 3 ## Frequency 212222 270355 66899 ## Proportion 0.386 0.492 0.122 ## ------------------------------------------------------- #variable muerte covid$muerte&lt;-(covid$FECHA_DEF!=&quot;9999-99-99&quot;) covid&lt;-covid %&gt;% dplyr::select(-FECHA_DEF) covid&lt;-na.omit(covid) 5.1 Logit Probit Se usa para realizar clasificaciones basadas en probabilidades Las clasificaciones son del tipo 1/0 Existen variaciones para clasificar considerando más grupos, empleando el logit y probit ordenado. 5.1.1 Pasos Identificar la variable (1/0) que se requiere clasificar, definir covariables para construir el modelos table(covid$muerte) ## ## FALSE TRUE ## 494841 33611 prop.table(table(covid$muerte)) ## ## FALSE TRUE ## 0.93639725 0.06360275 ################################################## #a factor ################################################## #sexo covid$SEXO&lt;-factor(covid$SEXO,levels=1:2,labels=c(&quot;Mujer&quot;,&quot;Hombre&quot;)) #resultado covid$RESULTADO&lt;-factor(covid$RESULTADO,levels = 1:3,labels=c(&quot;COVID +&quot;,&quot;COVID -&quot;,&quot;COVID pendiende&quot;)) covid2&lt;-covid#para cart #si/no aux&lt;-c(&quot;NEUMONIA&quot;,&quot;HABLA_LENGUA_INDIG&quot;,&quot;DIABETES&quot;,&quot;EPOC&quot;,&quot;ASMA&quot;,&quot;INMUSUPR&quot;,&quot;HIPERTENSION&quot;,&quot;OTRA_COM&quot;,&quot;CARDIOVASCULAR&quot;,&quot;OBESIDAD&quot;,&quot;RENAL_CRONICA&quot;,&quot;TABAQUISMO&quot;) for(i in aux){ covid[[i]]&lt;-covid[[i]]==1 } str(covid) ## &#39;data.frame&#39;: 528452 obs. of 16 variables: ## $ SEXO : Factor w/ 2 levels &quot;Mujer&quot;,&quot;Hombre&quot;: 2 2 2 2 2 1 2 1 2 2 ... ## $ NEUMONIA : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ EDAD : int 63 39 62 86 46 40 52 46 73 61 ... ## $ HABLA_LENGUA_INDIG: logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ DIABETES : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ EPOC : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ ASMA : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ INMUSUPR : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ HIPERTENSION : logi TRUE FALSE FALSE TRUE FALSE FALSE ... ## $ OTRA_COM : logi FALSE FALSE FALSE FALSE TRUE FALSE ... ## $ CARDIOVASCULAR : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ OBESIDAD : logi TRUE FALSE FALSE FALSE FALSE FALSE ... ## $ RENAL_CRONICA : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ TABAQUISMO : logi FALSE TRUE FALSE FALSE FALSE FALSE ... ## $ RESULTADO : Factor w/ 3 levels &quot;COVID +&quot;,&quot;COVID -&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ muerte : logi FALSE FALSE TRUE TRUE FALSE FALSE ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:21024] 23 184 230 236 363 642 737 820 831 915 ... ## ..- attr(*, &quot;names&quot;)= chr [1:21024] &quot;23&quot; &quot;184&quot; &quot;230&quot; &quot;236&quot; ... ## Bases: trainbd, testbd set.seed(123) index = sample(1:2, nrow(covid), replace = TRUE, prob=c(0.7, 0.3)) prop.table(table(index)) ## index ## 1 2 ## 0.7001544 0.2998456 trainbd&lt;-covid[index==1,] testbd&lt;-covid[index==2,] Especificar el modelo (logit/probit) m1&lt;-glm(muerte~.,data=trainbd,family = binomial(link=&quot;logit&quot;)) Identificar las variables significativas Construir el modelo con variables significativas summary(m1) ## ## Call: ## glm(formula = muerte ~ ., family = binomial(link = &quot;logit&quot;), ## data = trainbd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2988 -0.2468 -0.1466 -0.0944 3.8471 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) -6.0453094 0.0351339 ## SEXOHombre 0.4528000 0.0167200 ## NEUMONIATRUE 2.1601129 0.0166393 ## EDAD 0.0503957 0.0005735 ## HABLA_LENGUA_INDIGTRUE 0.1299336 0.0636812 ## DIABETESTRUE 0.3568379 0.0185218 ## EPOCTRUE 0.0606881 0.0396321 ## ASMATRUE -0.2211289 0.0539182 ## INMUSUPRTRUE 0.3809713 0.0467945 ## HIPERTENSIONTRUE 0.1546319 0.0187518 ## OTRA_COMTRUE 0.5776581 0.0358269 ## CARDIOVASCULARTRUE -0.0544158 0.0376414 ## OBESIDADTRUE 0.1697086 0.0194157 ## RENAL_CRONICATRUE 0.7287407 0.0354132 ## TABAQUISMOTRUE -0.0631770 0.0281778 ## RESULTADOCOVID - -1.1795001 0.0193564 ## RESULTADOCOVID pendiende -1.3541807 0.0317650 ## z value Pr(&gt;|z|) ## (Intercept) -172.065 &lt; 2e-16 *** ## SEXOHombre 27.081 &lt; 2e-16 *** ## NEUMONIATRUE 129.820 &lt; 2e-16 *** ## EDAD 87.881 &lt; 2e-16 *** ## HABLA_LENGUA_INDIGTRUE 2.040 0.0413 * ## DIABETESTRUE 19.266 &lt; 2e-16 *** ## EPOCTRUE 1.531 0.1257 ## ASMATRUE -4.101 4.11e-05 *** ## INMUSUPRTRUE 8.141 3.91e-16 *** ## HIPERTENSIONTRUE 8.246 &lt; 2e-16 *** ## OTRA_COMTRUE 16.124 &lt; 2e-16 *** ## CARDIOVASCULARTRUE -1.446 0.1483 ## OBESIDADTRUE 8.741 &lt; 2e-16 *** ## RENAL_CRONICATRUE 20.578 &lt; 2e-16 *** ## TABAQUISMOTRUE -2.242 0.0250 * ## RESULTADOCOVID - -60.936 &lt; 2e-16 *** ## RESULTADOCOVID pendiende -42.631 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 175579 on 369997 degrees of freedom ## Residual deviance: 113042 on 369981 degrees of freedom ## AIC: 113076 ## ## Number of Fisher Scoring iterations: 7 m2&lt;-step(m1) ## Start: AIC=113075.8 ## muerte ~ SEXO + NEUMONIA + EDAD + HABLA_LENGUA_INDIG + DIABETES + ## EPOC + ASMA + INMUSUPR + HIPERTENSION + OTRA_COM + CARDIOVASCULAR + ## OBESIDAD + RENAL_CRONICA + TABAQUISMO + RESULTADO ## ## Df Deviance AIC ## &lt;none&gt; 113042 113076 ## - CARDIOVASCULAR 1 113044 113076 ## - EPOC 1 113044 113076 ## - HABLA_LENGUA_INDIG 1 113046 113078 ## - TABAQUISMO 1 113047 113079 ## - ASMA 1 113059 113091 ## - INMUSUPR 1 113105 113137 ## - HIPERTENSION 1 113109 113141 ## - OBESIDAD 1 113117 113149 ## - OTRA_COM 1 113286 113318 ## - DIABETES 1 113406 113438 ## - RENAL_CRONICA 1 113445 113477 ## - SEXO 1 113789 113821 ## - RESULTADO 2 118350 118380 ## - EDAD 1 121441 121473 ## - NEUMONIA 1 131172 131204 summary(m2) ## ## Call: ## glm(formula = muerte ~ SEXO + NEUMONIA + EDAD + HABLA_LENGUA_INDIG + ## DIABETES + EPOC + ASMA + INMUSUPR + HIPERTENSION + OTRA_COM + ## CARDIOVASCULAR + OBESIDAD + RENAL_CRONICA + TABAQUISMO + ## RESULTADO, family = binomial(link = &quot;logit&quot;), data = trainbd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2988 -0.2468 -0.1466 -0.0944 3.8471 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) -6.0453094 0.0351339 ## SEXOHombre 0.4528000 0.0167200 ## NEUMONIATRUE 2.1601129 0.0166393 ## EDAD 0.0503957 0.0005735 ## HABLA_LENGUA_INDIGTRUE 0.1299336 0.0636812 ## DIABETESTRUE 0.3568379 0.0185218 ## EPOCTRUE 0.0606881 0.0396321 ## ASMATRUE -0.2211289 0.0539182 ## INMUSUPRTRUE 0.3809713 0.0467945 ## HIPERTENSIONTRUE 0.1546319 0.0187518 ## OTRA_COMTRUE 0.5776581 0.0358269 ## CARDIOVASCULARTRUE -0.0544158 0.0376414 ## OBESIDADTRUE 0.1697086 0.0194157 ## RENAL_CRONICATRUE 0.7287407 0.0354132 ## TABAQUISMOTRUE -0.0631770 0.0281778 ## RESULTADOCOVID - -1.1795001 0.0193564 ## RESULTADOCOVID pendiende -1.3541807 0.0317650 ## z value Pr(&gt;|z|) ## (Intercept) -172.065 &lt; 2e-16 *** ## SEXOHombre 27.081 &lt; 2e-16 *** ## NEUMONIATRUE 129.820 &lt; 2e-16 *** ## EDAD 87.881 &lt; 2e-16 *** ## HABLA_LENGUA_INDIGTRUE 2.040 0.0413 * ## DIABETESTRUE 19.266 &lt; 2e-16 *** ## EPOCTRUE 1.531 0.1257 ## ASMATRUE -4.101 4.11e-05 *** ## INMUSUPRTRUE 8.141 3.91e-16 *** ## HIPERTENSIONTRUE 8.246 &lt; 2e-16 *** ## OTRA_COMTRUE 16.124 &lt; 2e-16 *** ## CARDIOVASCULARTRUE -1.446 0.1483 ## OBESIDADTRUE 8.741 &lt; 2e-16 *** ## RENAL_CRONICATRUE 20.578 &lt; 2e-16 *** ## TABAQUISMOTRUE -2.242 0.0250 * ## RESULTADOCOVID - -60.936 &lt; 2e-16 *** ## RESULTADOCOVID pendiende -42.631 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 175579 on 369997 degrees of freedom ## Residual deviance: 113042 on 369981 degrees of freedom ## AIC: 113076 ## ## Number of Fisher Scoring iterations: 7 Predecir la clase de pertenencia en la base de test (\\(prob&gt;0.5\\)) clase&lt;-predict(m2,testbd,type=&quot;response&quot;)&gt;0.5 Observar la clasificación dada en base a la probabilidad fijada table(clase) ## clase ## FALSE TRUE ## 153984 4470 Comparar lo observado y lo predicho table(testbd$muerte,clase) ## clase ## FALSE TRUE ## FALSE 146404 2041 ## TRUE 7580 2429 Generar la matriz de confusión (librería caret) library(caret) ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:vegan&#39;: ## ## tolerance ## The following object is masked from &#39;package:survival&#39;: ## ## cluster confusionMatrix(table(testbd$muerte,clase)) ## Confusion Matrix and Statistics ## ## clase ## FALSE TRUE ## FALSE 146404 2041 ## TRUE 7580 2429 ## ## Accuracy : 0.9393 ## 95% CI : (0.9381, 0.9405) ## No Information Rate : 0.9718 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.3086 ## ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.9508 ## Specificity : 0.5434 ## Pos Pred Value : 0.9863 ## Neg Pred Value : 0.2427 ## Prevalence : 0.9718 ## Detection Rate : 0.9240 ## Detection Prevalence : 0.9368 ## Balanced Accuracy : 0.7471 ## ## &#39;Positive&#39; Class : FALSE ## Efectos marginales library(mfx) ## Loading required package: sandwich ## Loading required package: lmtest ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## Loading required package: betareg info&lt;-logitmfx(formula(m2),data=testbd) barplot(info$mfxest[,1],horiz = T,las=1,cex.names = 0.5,xlim=c(-0.1,0.1),pos=-0.02) ## Warning in plot.window(xlim, ylim, log = log, ...): ## &quot;pos&quot; is not a graphical parameter ## Warning in title(main = main, sub = sub, xlab = xlab, ## ylab = ylab, ...): &quot;pos&quot; is not a graphical parameter 5.2 Arboles de clasificación (CART) El método CART uso condiciones basadas en cortes sobre covariables para realizar la clasificación (predicción) de una clase. El proceso de clasificación comienza desde el nodo raíz del árbol; en cada nodo, el proceso verificará si el valor de entrada debe continuar de forma recursiva hacia la sub-rama derecha o izquierda de acuerdo con la condición de división, y se detiene al encontrar cualquier nodo hoja (terminal) del árbol de decisión. 5.2.1 Pasos Crear el modelo de clasificación Cargar la librería rpart library(rpart) Usar la función rpart para construir el modelo de clasificación covid2$muerte&lt;-factor(covid2$muerte,c(T,F),labels = c(&quot;Muerte&quot;,&quot;No muerte&quot;)) #si/no aux&lt;-c(&quot;NEUMONIA&quot;,&quot;HABLA_LENGUA_INDIG&quot;,&quot;DIABETES&quot;,&quot;EPOC&quot;,&quot;ASMA&quot;,&quot;INMUSUPR&quot;,&quot;HIPERTENSION&quot;,&quot;OTRA_COM&quot;,&quot;CARDIOVASCULAR&quot;,&quot;OBESIDAD&quot;,&quot;RENAL_CRONICA&quot;,&quot;TABAQUISMO&quot;) for(i in aux){ covid2[[i]]&lt;-factor(covid2[[i]],1:2,c(&quot;SI&quot;,&quot;NO&quot;)) } str(covid2) ## &#39;data.frame&#39;: 528452 obs. of 16 variables: ## $ SEXO : Factor w/ 2 levels &quot;Mujer&quot;,&quot;Hombre&quot;: 2 2 2 2 2 1 2 1 2 2 ... ## $ NEUMONIA : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 1 2 2 2 2 2 2 2 ... ## $ EDAD : int 63 39 62 86 46 40 52 46 73 61 ... ## $ HABLA_LENGUA_INDIG: Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ DIABETES : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 1 2 ... ## $ EPOC : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ ASMA : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ INMUSUPR : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ HIPERTENSION : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 1 2 2 1 2 2 2 2 2 2 ... ## $ OTRA_COM : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 1 2 2 1 2 2 ... ## $ CARDIOVASCULAR : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 1 2 2 ... ## $ OBESIDAD : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 1 2 2 2 2 2 2 1 2 2 ... ## $ RENAL_CRONICA : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ TABAQUISMO : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 1 2 2 2 2 2 2 2 2 ... ## $ RESULTADO : Factor w/ 3 levels &quot;COVID +&quot;,&quot;COVID -&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ muerte : Factor w/ 2 levels &quot;Muerte&quot;,&quot;No muerte&quot;: 2 2 1 1 2 2 2 2 2 2 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:21024] 23 184 230 236 363 642 737 820 831 915 ... ## ..- attr(*, &quot;names&quot;)= chr [1:21024] &quot;23&quot; &quot;184&quot; &quot;230&quot; &quot;236&quot; ... ## Bases: trainbd, testbd set.seed(123) index = sample(1:2, nrow(covid2), replace = TRUE, prob=c(0.7, 0.3)) prop.table(table(index)) ## index ## 1 2 ## 0.7001544 0.2998456 trainbd&lt;-covid2[index==1,] testbd&lt;-covid2[index==2,] mod1&lt;-rpart(muerte~.,data=trainbd) Explorar los nodos creados por rpart mod1 ## n= 369998 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 369998 23602 No muerte (0.06378953 0.93621047) ## 2) NEUMONIA=SI 57116 16946 No muerte (0.29669445 0.70330555) ## 4) RESULTADO=COVID + 34442 13362 No muerte (0.38795656 0.61204344) ## 8) EDAD&gt;=51.5 20669 10039 No muerte (0.48570323 0.51429677) ## 16) EDAD&gt;=64.5 9808 4363 Muerte (0.55515905 0.44484095) * ## 17) EDAD&lt; 64.5 10861 4594 No muerte (0.42298131 0.57701869) * ## 9) EDAD&lt; 51.5 13773 3323 No muerte (0.24126915 0.75873085) * ## 5) RESULTADO=COVID -,COVID pendiende 22674 3584 No muerte (0.15806651 0.84193349) * ## 3) NEUMONIA=NO 312882 6656 No muerte (0.02127320 0.97872680) * Examinar los parámetros del árbol con printcp printcp(mod1) ## ## Classification tree: ## rpart(formula = muerte ~ ., data = trainbd) ## ## Variables actually used in tree construction: ## [1] EDAD NEUMONIA RESULTADO ## ## Root node error: 23602/369998 = 0.06379 ## ## n= 369998 ## ## CP nsplit rel error xerror xstd ## 1 0.011461 0 1.00000 1.0000 0.0062981 ## 2 0.010000 4 0.95416 0.9567 0.0061694 Usar el comando plotcp para explorar los parámetros de forma gráfica plotcp(mod1) Usar la función summary para para examinar el modelo summary(mod1) ## Call: ## rpart(formula = muerte ~ ., data = trainbd) ## n= 369998 ## ## CP nsplit rel error xerror xstd ## 1 0.01146089 0 1.0000000 1.0000000 0.006298140 ## 2 0.01000000 4 0.9541564 0.9566986 0.006169353 ## ## Variable importance ## NEUMONIA RESULTADO EDAD ## 72 14 13 ## ## Node number 1: 369998 observations, complexity param=0.01146089 ## predicted class=No muerte expected loss=0.06378953 P(node) =1 ## class counts: 23602 346396 ## probabilities: 0.064 0.936 ## left son=2 (57116 obs) right son=3 (312882 obs) ## Primary splits: ## NEUMONIA splits as LR, improve=7327.636, (0 missing) ## EDAD &lt; 57.5 to the right, improve=3636.657, (0 missing) ## RESULTADO splits as LRR, improve=1662.835, (0 missing) ## DIABETES splits as LR, improve=1643.543, (0 missing) ## HIPERTENSION splits as LR, improve=1431.782, (0 missing) ## ## Node number 2: 57116 observations, complexity param=0.01146089 ## predicted class=No muerte expected loss=0.2966944 P(node) =0.1543684 ## class counts: 16946 40170 ## probabilities: 0.297 0.703 ## left son=4 (34442 obs) right son=5 (22674 obs) ## Primary splits: ## RESULTADO splits as LRR, improve=1445.2040, (0 missing) ## EDAD &lt; 49.5 to the right, improve=1368.6730, (0 missing) ## HIPERTENSION splits as LR, improve= 357.6824, (0 missing) ## DIABETES splits as LR, improve= 303.1878, (0 missing) ## SEXO splits as RL, improve= 104.5722, (0 missing) ## Surrogate splits: ## EDAD &lt; 30.5 to the right, agree=0.631, adj=0.072, (0 split) ## INMUSUPR splits as RL, agree=0.609, adj=0.015, (0 split) ## OTRA_COM splits as RL, agree=0.607, adj=0.010, (0 split) ## EPOC splits as RL, agree=0.604, adj=0.002, (0 split) ## CARDIOVASCULAR splits as RL, agree=0.604, adj=0.002, (0 split) ## ## Node number 3: 312882 observations ## predicted class=No muerte expected loss=0.0212732 P(node) =0.8456316 ## class counts: 6656 306226 ## probabilities: 0.021 0.979 ## ## Node number 4: 34442 observations, complexity param=0.01146089 ## predicted class=No muerte expected loss=0.3879566 P(node) =0.09308699 ## class counts: 13362 21080 ## probabilities: 0.388 0.612 ## left son=8 (20669 obs) right son=9 (13773 obs) ## Primary splits: ## EDAD &lt; 51.5 to the right, improve=987.67290, (0 missing) ## HIPERTENSION splits as LR, improve=276.65030, (0 missing) ## DIABETES splits as LR, improve=194.20950, (0 missing) ## RENAL_CRONICA splits as LR, improve= 89.58991, (0 missing) ## EPOC splits as LR, improve= 51.97436, (0 missing) ## ## Node number 5: 22674 observations ## predicted class=No muerte expected loss=0.1580665 P(node) =0.06128141 ## class counts: 3584 19090 ## probabilities: 0.158 0.842 ## ## Node number 8: 20669 observations, complexity param=0.01146089 ## predicted class=No muerte expected loss=0.4857032 P(node) =0.05586246 ## class counts: 10039 10630 ## probabilities: 0.486 0.514 ## left son=16 (9808 obs) right son=17 (10861 obs) ## Primary splits: ## EDAD &lt; 64.5 to the right, improve=180.08500, (0 missing) ## SEXO splits as RL, improve= 26.65526, (0 missing) ## RENAL_CRONICA splits as LR, improve= 23.86824, (0 missing) ## HIPERTENSION splits as LR, improve= 20.12766, (0 missing) ## DIABETES splits as LR, improve= 13.55075, (0 missing) ## Surrogate splits: ## HIPERTENSION splits as LR, agree=0.573, adj=0.100, (0 split) ## EPOC splits as LR, agree=0.547, adj=0.044, (0 split) ## CARDIOVASCULAR splits as LR, agree=0.543, adj=0.037, (0 split) ## OTRA_COM splits as LR, agree=0.531, adj=0.011, (0 split) ## RENAL_CRONICA splits as LR, agree=0.529, adj=0.007, (0 split) ## ## Node number 9: 13773 observations ## predicted class=No muerte expected loss=0.2412691 P(node) =0.03722453 ## class counts: 3323 10450 ## probabilities: 0.241 0.759 ## ## Node number 16: 9808 observations ## predicted class=Muerte expected loss=0.4448409 P(node) =0.02650825 ## class counts: 5445 4363 ## probabilities: 0.555 0.445 ## ## Node number 17: 10861 observations ## predicted class=No muerte expected loss=0.4229813 P(node) =0.02935421 ## class counts: 4594 6267 ## probabilities: 0.423 0.577 Visualizar el árbol Usar la función plot y text(,all=T, n=T) plot(mod1) text(mod1,all=T,use.n=T) #install.packages(&quot;rpart.plot&quot;) library(rpart.plot) rpart.plot(mod1) Ajustes en el layout plot(…,uniform=TRUE, branch=0.6, margin=0.1) plot(mod1,uniform=T, branch=1, margin=0.1) text(mod1,all=T,use.n=T) Predicción de la clasificación predict(…, testbd, type=“class”), predicción sobre la base de test clase&lt;-predict(mod1,testbd,type = &quot;class&quot;) Elaborar una tabla de contingencia de la clasificación table(clase,testbd$muerte) ## ## clase Muerte No muerte ## Muerte 2405 1862 ## No muerte 7604 146583 Emplear el comando confusionMatrix sobre la tabla del paso anterior, para evaluar la calidad de la clasificación. Mcnemar’s Test H0: \\(ij=ji\\) library(caret) confusionMatrix(table(clase,testbd$muerte)) ## Confusion Matrix and Statistics ## ## ## clase Muerte No muerte ## Muerte 2405 1862 ## No muerte 7604 146583 ## ## Accuracy : 0.9403 ## 95% CI : (0.9391, 0.9414) ## No Information Rate : 0.9368 ## P-Value [Acc &gt; NIR] : 8.06e-09 ## ## Kappa : 0.3109 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.24028 ## Specificity : 0.98746 ## Pos Pred Value : 0.56363 ## Neg Pred Value : 0.95068 ## Prevalence : 0.06317 ## Detection Rate : 0.01518 ## Detection Prevalence : 0.02693 ## Balanced Accuracy : 0.61387 ## ## &#39;Positive&#39; Class : Muerte ## 5.2.2 Proceso de pruning (podado) El objetivo es eliminar variables redundantes y crear un modelo de clasificación mas robusto Pasos: Encuentre el valor mínimo en cross-validation error. (xerror) min(mod1$cptable[,&quot;xerror&quot;]) ## [1] 0.9566986 Encontrar el registro que contiene el valor del anterior paso. (which.min, cptable) which.min(mod1$cptable[,&quot;xerror&quot;]) ## 2 ## 2 Obtenga el “cost complexity parameter” del valor mínimo encontrado (CP) mod1.cp&lt;-mod1$cptable[2,&quot;CP&quot;] Realizar el podado con la función prune, empleando el modelo original y el CP del valor mínimo en xerror (paso anterior) mod1.prune&lt;-prune(mod1,cp=mod1.cp) Visualice el nuevo árbol rpart.plot(mod1.prune) Realice la predicción a partir del árbol podado clase&lt;-predict(mod1.prune,testbd,type = &quot;class&quot;) Evalúe los resultados con la matriz de confusión confusionMatrix(table(clase,testbd$muerte)) ## Confusion Matrix and Statistics ## ## ## clase Muerte No muerte ## Muerte 2405 1862 ## No muerte 7604 146583 ## ## Accuracy : 0.9403 ## 95% CI : (0.9391, 0.9414) ## No Information Rate : 0.9368 ## P-Value [Acc &gt; NIR] : 8.06e-09 ## ## Kappa : 0.3109 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.24028 ## Specificity : 0.98746 ## Pos Pred Value : 0.56363 ## Neg Pred Value : 0.95068 ## Prevalence : 0.06317 ## Detection Rate : 0.01518 ## Detection Prevalence : 0.02693 ## Balanced Accuracy : 0.61387 ## ## &#39;Positive&#39; Class : Muerte ## 5.3 Naive Bayes Es un modelo basado en probabilidad, su base teórica aplica el teorema de Bayes (fuerte supuesto de independencia). nota \\[P(muerte/edad,neumonia, ...)&lt;&gt;P(\\sim muerte/edad,neumonia,...)\\] \\[P(C/X)=\\frac{P(C)*P(X/C)}{P(X)}\\] * \\(P(C/X)\\) Probabilidad Posterior * \\(P(C)\\) Probabilidad a Priori * \\(P(X/C)\\) Verosimilitud * \\(P(X)\\) Marginal Si se tiene varios predictores (\\(X\\)) se supone independencia, esto es: \\[P(C/X) = \\frac{P(X_1/C)*P(X_2/C)*\\ldots *P(X_n/C)*P(C)}{P(X)}\\] Pasos, Cargar la librería e1071 y emplear la función naiveBayes para construir el clasificador library(e1071) ## ## Attaching package: &#39;e1071&#39; ## The following object is masked from &#39;package:Hmisc&#39;: ## ## impute mod1&lt;-naiveBayes(muerte~.,data=trainbd) Explorar los resultados mod1 ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## Muerte No muerte ## 0.06378953 0.93621047 ## ## Conditional probabilities: ## SEXO ## Y Mujer Hombre ## Muerte 0.3513685 0.6486315 ## No muerte 0.5045959 0.4954041 ## ## NEUMONIA ## Y SI NO ## Muerte 0.7179900 0.2820100 ## No muerte 0.1159655 0.8840345 ## ## EDAD ## Y [,1] [,2] ## Muerte 60.42543 15.03884 ## No muerte 41.26280 15.78475 ## ## HABLA_LENGUA_INDIG ## Y SI NO ## Muerte 0.017710364 0.982289636 ## No muerte 0.009295719 0.990704281 ## ## DIABETES ## Y SI NO ## Muerte 0.3705618 0.6294382 ## No muerte 0.1089822 0.8910178 ## ## EPOC ## Y SI NO ## Muerte 0.05448691 0.94551309 ## No muerte 0.01310927 0.98689073 ## ## ASMA ## Y SI NO ## Muerte 0.02076095 0.97923905 ## No muerte 0.03311239 0.96688761 ## ## INMUSUPR ## Y SI NO ## Muerte 0.03694602 0.96305398 ## No muerte 0.01450074 0.98549926 ## ## HIPERTENSION ## Y SI NO ## Muerte 0.4184815 0.5815185 ## No muerte 0.1461045 0.8538955 ## ## OTRA_COM ## Y SI NO ## Muerte 0.06448606 0.93551394 ## No muerte 0.02853670 0.97146330 ## ## CARDIOVASCULAR ## Y SI NO ## Muerte 0.05982544 0.94017456 ## No muerte 0.01967113 0.98032887 ## ## OBESIDAD ## Y SI NO ## Muerte 0.232904 0.767096 ## No muerte 0.159589 0.840411 ## ## RENAL_CRONICA ## Y SI NO ## Muerte 0.07825608 0.92174392 ## No muerte 0.01579695 0.98420305 ## ## TABAQUISMO ## Y SI NO ## Muerte 0.09397509 0.90602491 ## No muerte 0.08496345 0.91503655 ## ## RESULTADO ## Y COVID + COVID - COVID pendiende ## Muerte 0.74934328 0.19455978 0.05609694 ## No muerte 0.36267162 0.51143200 0.12589637 Predecir los resultados en la base de testeo clase&lt;-predict(mod1,testbd,type = &quot;class&quot;) Realizar la matriz de confusión confusionMatrix(table(clase,testbd$muerte)) ## Confusion Matrix and Statistics ## ## ## clase Muerte No muerte ## Muerte 5261 8124 ## No muerte 4748 140321 ## ## Accuracy : 0.9188 ## 95% CI : (0.9174, 0.9201) ## No Information Rate : 0.9368 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.4069 ## ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.52563 ## Specificity : 0.94527 ## Pos Pred Value : 0.39305 ## Neg Pred Value : 0.96727 ## Prevalence : 0.06317 ## Detection Rate : 0.03320 ## Detection Prevalence : 0.08447 ## Balanced Accuracy : 0.73545 ## ## &#39;Positive&#39; Class : Muerte ## 5.4 Ejercicios. Usando la ENDSA para un año en particular, defina clases de violencia en base a las variables de violencia y aplique los métodos de clasificación con las variables que considere relevante. Usando la Encuesta a hogares, para la clase nivel de educación (ninguno, primaria, secundaria, superior) aplique los métodos de clasificación considerando las variables relevantes. "],
["minería-de-texto.html", "6 Minería de Texto 6.1 Introducción 6.2 Recolección de texto 6.3 Nubes de palabras 6.4 Análisis de sentimiento (sentimental scoring)", " 6 Minería de Texto La minería de texto es el proceso de destilar información procesable del texto. (Ted 2017) Minería de texto puede ser sinónimo de análisis de texto, sin embargo, el uso de minería de texto describe de forma más adecuada el descubrimiento de ideas y el uso de algorítmos específicos más alla del análisis estadístico básico. 6.1 Introducción 6.1.1 En la práctica Análisis de mercado (impacto en los consumidores, marca, etc.) Análisis político (percepción, sentimientos, etc.) Match: CV y expectativas de una empresa Inteligencia de negocios 6.1.2 ¿Por qué importa? Las redes sociales continúan evolucionando y afectan los esfuerzos públicos de una organización. El contenido en línea de una organización, sus competidores y fuentes externas, como los blogs, continúa creciendo. La digitalización de los registros en papel se está produciendo en muchas industrias. 6.1.3 Las consecuencias de ignorarlo Ignorar el texto no es una respuesta adecuada de un esfuerzo analítico. La exploración científica y analítica rigurosa requiere investigar fuentes de información que puedan explicar los fenómenos. No realizar minería de texto puede conducir a un análisis o resultado falso. Algunos problemas se basan casi exclusivamente en texto, por lo que no usar estos métodos significaría una reducción significativa en la efectividad o incluso no poder realizar el análisis. 6.1.4 Beneficios La confianza se genera entre las partes interesadas ya que se necesita poco o ningún muestreo para extraer información. Las metodologías se pueden aplicar rápidamente. El uso de R permite métodos auditables y repetibles. La minería de texto identifica nuevas ideas o refuerza las percepciones existentes basadas en toda la información relevante. Posibles usos 6.1.5 Flujo de trabajo en la minería de texto Flujo de trabajo Definir el problema y establecer las metas Identificar el texto que se quiere recolectar Organizar el texto (corpus, colección de documentos) Extraer características Analizar el texto Llegar a una idea o una recomendación 6.1.6 Librerías en R para texto stringi stringr qdap tm Comandos para texto nchar paste, paste0 sub, gsub, grep mgsub (qdap) library(qdap) fake.text&lt;-&#39;R text mining is good but text mining in python is also&#39; patterns&lt;-c(&#39;good&#39;,&#39;also&#39;,&#39;text mining&#39;) replacements&lt;-c(&#39;great&#39;,&#39;just as suitable&#39;,&#39;tm&#39;) mgsub(patterns,replacements,fake.text) tolower removePunctuation stripWhitespace removeNumbers removeWords (stopwords) stemDocument 6.2 Recolección de texto La recolección de texto puede provenir de: Base de datos en csv u otro similar Colección de documentos Scraping Web, API 6.2.1 CSV library(tm) library(dplyr) setwd(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data&quot;) fb&lt;-read.csv(&quot;bd_sc.csv&quot;) fb&lt;-read.csv(&quot;bd_sc.csv&quot;,encoding = &quot;UTF-8&quot;) #fb&lt;-read.csv(&quot;bd_sc.csv&quot;,encoding = &quot;Latin-1&quot;) fb$post_text[5] 6.2.2 Colección de documentos library(pdftools) dir&lt;-&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\pdf&quot; pdfdocs &lt;- VCorpus(DirSource(dir, pattern = &quot;.pdf&quot;), readerControl = list(reader = readPDF)) 6.2.3 Twitter (API) library(rtweet) tw&lt;-search_tweets(&quot;Dioxido de cloro&quot;,n=10000,include_rts = F) 6.3 Nubes de palabras library(wordcloud2) docs&lt;-Corpus(VectorSource(fb$post_text)) docs &lt;- docs %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(stripWhitespace) docs &lt;- tm_map(docs, content_transformer(tolower)) docs &lt;- tm_map(docs, removeWords, stopwords(&quot;sp&quot;)) dtm &lt;- TermDocumentMatrix(docs) matrix &lt;- as.matrix(dtm) words &lt;- sort(rowSums(matrix),decreasing=TRUE) df &lt;- data.frame(word = names(words),freq=words) ##funciones nube&lt;-function(aux){ docs&lt;-Corpus(VectorSource(aux)) docs &lt;- docs %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(stripWhitespace) docs &lt;- tm_map(docs, content_transformer(tolower)) docs &lt;- tm_map(docs, removeWords, stopwords(&quot;sp&quot;)) dtm &lt;- TermDocumentMatrix(docs) matrix &lt;- as.matrix(dtm) words &lt;- sort(rowSums(matrix),decreasing=TRUE) df &lt;- data.frame(word = names(words),freq=words) return(df) } nube2&lt;-function(aux){ docs &lt;- aux %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(stripWhitespace) docs &lt;- tm_map(docs, content_transformer(tolower)) docs &lt;- tm_map(docs, removeWords, stopwords(&quot;sp&quot;)) dtm &lt;- TermDocumentMatrix(docs) matrix &lt;- as.matrix(dtm) words &lt;- sort(rowSums(matrix),decreasing=TRUE) df &lt;- data.frame(word = names(words),freq=words) return(df) } Sobre los tipos de datos #csv df&lt;-nube(fb$post_text) wordcloud2(data=df[df$freq&gt;5,],color=&#39;random-dark&#39;,size = 0.4,shape = &#39;star&#39;) #colección de documentos df&lt;-nube2(pdfdocs) wordcloud2(data=df,color=&#39;random-dark&#39;,size = 0.4,shape = &#39;pentagon&#39;) #scrape df&lt;-nube(tw$text) wordcloud2(data=df[df$freq&gt;1,],color=&#39;random-dark&#39;,shape = &#39;pentagon&#39;) library(wordcloud) wordcloud(df$word,freq=df$freq,min.freq = 5) Gráfico de correlaciones library(ggplot2) library(ggthemes) docs&lt;-VCorpus(VectorSource(fb$text)) docs &lt;- docs %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(stripWhitespace) docs &lt;- tm_map(docs, content_transformer(tolower)) docs &lt;- tm_map(docs, removeWords, stopwords(&quot;sp&quot;)) tdm&lt;-TermDocumentMatrix(docs) associations&lt;-findAssocs(tdm, &#39;evo&#39;, 0.55) associations&lt;-as.data.frame(associations) associations$terms&lt;-row.names(associations) associations$terms&lt;-factor(associations$terms, levels=associations$terms) names(associations)[1]&lt;-&quot;palabra&quot; ggplot(associations, aes(y=terms)) + geom_point(aes(x=palabra), data=associations, size=5)+ theme_gdocs()+ geom_text(aes(x=palabra, label=palabra), colour=&quot;darkred&quot;,hjust=-.25,size=8)+ theme(text=element_text(size=20), axis.title.y=element_blank()) 6.4 Análisis de sentimiento (sentimental scoring) El análisis de sentimientos es el proceso de extraer la intención emocional del autor de un texto. Se debe tener en cuenta: Aspectos culturales diferencias demográficas texto con sentimientos compuestos Hay varios marcos de referencias de emociones que se pueden considerar. Uno de los más usados es el creado en 1980 por Robert Plutchik (psicólogo), se establecen 8 emociones: (-) ira (anger) (-) miedo (fear) (-) tristeza (sadness) (-) asco (disgust) (+) sorpresa (surprise) (+) anticipación (anticipation) (+) confianza (trust) (+) alegría (joy) Espectro de emociones de Plutchik’s a partir de las primarias 6.4.1 Polarización y léxico subjetivo El análisis de sentimientos en R es bueno pero desafiante El análisis de sentimiento en R es muy bueno El análisis de sentimiento en Python no es bueno 6.4.2 Polarización en QDAP #solo ingles library(qdap) library(rtweet) tw&lt;-search_tweets(&quot;Bolivia&quot;,n=100,include_rts = F,lang=&quot;en&quot;) detach(package:dplyr, unload=TRUE) detach(package:rtweet, unload=TRUE) detach(package:qdap, unload=TRUE) `[[.qdap_hash` &lt;- `[[.data.frame` tw$text&lt;-removePunctuation(tw$text) score&lt;-polarity(tw$text[1:2]) 6.4.3 Librería syuzhet library(syuzhet) library(rtweet) tw&lt;-search_tweets(&quot;coronavirus&quot;,n=1000,include_rts = F,lang=&quot;es&quot;) ww&lt;-get_sentiment_dictionary(&quot;nrc&quot;,language = &quot;spanish&quot;) aa&lt;-get_nrc_sentiment(tw$text,language = &quot;spanish&quot;) barplot(apply(aa,2,sum),horiz = T,las=1) #ampliar el léxico ww&lt;-rbind(ww,c(&quot;spanish&quot;,&quot;xxxx&quot;,&quot;negative&quot;,&quot;1&quot;)) tail(ww) get_nrc_sentiment #tarea "]
]
