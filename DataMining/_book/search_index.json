[
["index.html", "Minería de datos con R EST-384 Prefacio Audiencia Estructura del libro Software y acuerdos Datos Agradecimiento", " Minería de datos con R EST-384 Alvaro Chirino Gutierrez 2020-11-11 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Programación Estadística II, una materia optativa del pregrado de la carrera de Estadística de la Universidad Mayor de San Andrés. Este documento representa un primer acercamiento a los estudiantes de estadistica al software R y al mundo de la minería de datos. Estructura del libro El libro inluye 5 capitulos, estos son: Introducción a R Preparación de los datos Modelado en Minería de datos Minería de Texto Machine Learning Software y acuerdos sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19041) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Spanish_Bolivia.1252 LC_CTYPE=Spanish_Bolivia.1252 ## [3] LC_MONETARY=Spanish_Bolivia.1252 LC_NUMERIC=C ## [5] LC_TIME=Spanish_Bolivia.1252 ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets ## [7] methods base ## ## other attached packages: ## [1] fastDummies_1.6.1 vegan_2.5-6 permute_0.9-5 ## [4] fpc_2.2-5 cluster_2.1.0 shiny_1.4.0.2 ## [7] explor_0.3.6 FactoMineR_2.3 corrplot_0.84 ## [10] TeachingDemos_2.12 ggridges_0.5.2 GGally_2.0.0 ## [13] Hmisc_4.4-0 ggplot2_3.3.2 Formula_1.2-3 ## [16] lattice_0.20-41 DMwR2_0.0.2 mice_3.11.0 ## [19] srvyr_0.3.10 survey_4.0 survival_3.1-12 ## [22] Matrix_1.2-18 dplyr_1.0.0 rvest_0.3.6 ## [25] xml2_1.3.2 stringi_1.4.6 tm_0.7-7 ## [28] NLP_0.2-0 lubridate_1.7.9 tidyr_1.0.2 ## ## loaded via a namespace (and not attached): ## [1] backports_1.1.6 plyr_1.8.6 selectr_0.4-2 ## [4] splines_4.0.2 crosstalk_1.1.0.1 digest_0.6.25 ## [7] htmltools_0.5.0 fansi_0.4.1 magrittr_1.5 ## [10] checkmate_2.0.0 readr_1.3.1 xts_0.12-0 ## [13] prettyunits_1.1.1 jpeg_0.1-8.1 colorspace_1.4-1 ## [16] ggrepel_0.8.2 mitools_2.4 scatterD3_0.9.1 ## [19] xfun_0.13 callr_3.5.1 crayon_1.3.4 ## [22] jsonlite_1.7.1 zoo_1.8-8 glue_1.4.1 ## [25] gtable_0.3.0 webshot_0.5.2 kernlab_0.9-29 ## [28] DEoptimR_1.0-8 prabclus_2.3-2 quantmod_0.4.17 ## [31] scales_1.1.1 DBI_1.1.0 Rcpp_1.0.4.6 ## [34] xtable_1.8-4 progress_1.2.2 htmlTable_2.0.0 ## [37] flashClust_1.01-2 foreign_0.8-80 mclust_5.4.6 ## [40] stats4_4.0.2 DT_0.16 htmlwidgets_1.5.1 ## [43] httr_1.4.2 RColorBrewer_1.1-2 acepack_1.4.1 ## [46] ellipsis_0.3.1 modeltools_0.2-23 pkgconfig_2.0.3 ## [49] reshape_0.8.8 flexmix_2.3-15 farver_2.0.3 ## [52] nnet_7.3-14 utf8_1.1.4 tidyselect_1.1.0 ## [55] labeling_0.3 rlang_0.4.7 later_1.1.0.1 ## [58] munsell_0.5.0 tools_4.0.2 cli_2.0.2 ## [61] generics_0.0.2 broom_0.5.6 evaluate_0.14 ## [64] stringr_1.4.0 fastmap_1.0.1 yaml_2.2.1 ## [67] processx_3.4.4 knitr_1.28 robustbase_0.93-6 ## [70] purrr_0.3.4 packrat_0.5.0 nlme_3.1-148 ## [73] mime_0.9 slam_0.1-47 formatR_1.7 ## [76] leaps_3.1 compiler_4.0.2 rstudioapi_0.11 ## [79] curl_4.3 png_0.1-7 tibble_3.0.1 ## [82] highr_0.8 ps_1.3.3 vctrs_0.3.1 ## [85] pillar_1.4.4 lifecycle_0.2.0 data.table_1.12.8 ## [88] httpuv_1.5.4 R6_2.4.1 latticeExtra_0.6-29 ## [91] bookdown_0.18 promises_1.1.1 gridExtra_2.3 ## [94] MASS_7.3-51.6 assertthat_0.2.1 withr_2.2.0 ## [97] mgcv_1.8-31 diptest_0.75-7 parallel_4.0.2 ## [100] hms_0.5.3 rpart_4.1-15 class_7.3-17 ## [103] rmarkdown_2.3 Cairo_1.5-12 TTR_0.23-6 ## [106] scatterplot3d_0.3-41 base64enc_0.1-3 ellipse_0.4.2 Datos Agradecimiento Peeta… "],
["acerca-del-autor.html", "Acerca del autor Bibliografía", " Acerca del autor Bibliografía Torgo, L. (2016). Data mining with R: Learning with case studies, second edition. Hernandez, J. (2004). Introducción a la Minería de Datos Step, I., &amp; Blueprint, S. (2017). MACHINE LEARNING Intuitive Step by Step. "],
["minería-de-datos.html", "1 Minería de Datos 1.1 Motivación para la Minería de datos 1.2 ¿Qué es la minería de datos? 1.3 Datos y conocimiento 1.4 Requerimientos 1.5 knowledge discovery in databases (KDD) 1.6 Preparación de los datos 1.7 Imputación de variables", " 1 Minería de Datos 1.1 Motivación para la Minería de datos Los métodos de recolección de datos han evolucionado muy rápidamente. Las bases de datos han crecido exponencialmente Estos datos contienen información útil para las empresas, países, etc.. El tamaño hace que la inspección manual sea casi imposible Se requieren métodos de análisis de datos automáticos para optimizar el uso de estos enormes conjuntos de datos 1.2 ¿Qué es la minería de datos? Es el análisis de conjuntos de datos (a menudo grandes) para encontrar relaciones insospechadas (conocimiento) y resumir los datos de formas novedosas que sean comprensibles y útiles para el propietario/usuario de los datos. Principles of Data Mining (Hand et.al. 2001) 1.3 Datos y conocimiento 1.3.1 Datos: se refieren a instancias únicas y primitivas (single objetos, personas, eventos, puntos en el tiempo, etc.) describir propiedades individuales a menudo son fáciles de recolectar u obtener (por ejemplo, cajeros de escáner, internet, etc.) no nos permiten hacer predicciones o pronósticos 1.3.2 Conocimiento: se refiere a clases de instancias (conjuntos de …) describe patrones generales, estructuras, leyes, consta de la menor cantidad de declaraciones posibles a menudo es difícil y lleva mucho tiempo encontrar u obtener nos permite hacer predicciones y pronósticos 1.4 Requerimientos Disponibilidad para aprender Mucha paciencia Interactúa con otras áreas Preprocesamiento de datos Creatividad Rigor, prueba y error 1.5 knowledge discovery in databases (KDD) 1.6 Preparación de los datos 1.6.1 Recopilación Instituto de Estadística UDAPE, ASFI Ministerio Salud (SNIS), Ministerio de educación (SIE) APIs, Twitter, Facebook, etc. Kaggle Banco Mundial, UNICEF, FAO, BID (Open Data) 1.6.2 Data Warehouse 1.6.3 Data Warehouse in R 1.6.4 Importación library(foreign) library(readr) apropos(&quot;read&quot;) getwd() setwd(&quot;C:\\\\Users\\\\ALVARO\\\\Downloads\\\\bd49 (1)\\\\Base EH2019&quot;) dir() eh19v&lt;-read.spss(&quot;EH2019_Vivienda.sav&quot;,to.data.frame = T) eh19p&lt;-read.spss(&quot;EH2019_Persona.sav&quot;,to.data.frame = T) object.size(eh19p)/10^6 #exportación de datos setwd(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data&quot;) save(eh19p,eh19v,file=&quot;eh19.RData&quot;) # cargando la base de datos que acabamos de guardar rm(list=ls()) load(&quot;eh19.RData&quot;) load(&quot;oct20.RData&quot;) # cargando desde github rm(list=ls()) load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData&quot;)) load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/oct20.RData&quot;)) 1.6.5 Recopilación read.table(&quot;clipboard&quot;,header = T) library(readxl) # excel library(DBI) # Bases de datos relacionales en el sistema #library(help=DBI) library(RMySQL) # bases de datos en mysql # web scraping (API) library(gtrendsR) # API gg&lt;-gtrends(c(&quot;data mining&quot;,&quot;machine learning&quot;),time=&quot;today 12-m&quot;) gg$interest_over_time plot(gg) gg&lt;-gtrends(c(&quot;data mining&quot;,&quot;machine learning&quot;),time=&quot;today 12-m&quot;,geo=&quot;BO&quot;) 1.6.6 Limpieza std&lt;-data.frame(name=c(&quot;ana&quot;,&quot;juan&quot;,&quot;carla&quot;),math=c(86,43,80),stat=c(90,75,82)) std ## name math stat ## 1 ana 86 90 ## 2 juan 43 75 ## 3 carla 80 82 library(tidyr) bd&lt;-gather(std,materia,nota,math:stat) bd ## name materia nota ## 1 ana math 86 ## 2 juan math 43 ## 3 carla math 80 ## 4 ana stat 90 ## 5 juan stat 75 ## 6 carla stat 82 # otra opción más relacionada a bases de datos con información de tiempo, # es el comando reshape 1.6.7 Ejercicio (reshape) http://www.udape.gob.bo/portales_html/dossierweb2019/htms/CAP07/C070311.xls 1.6.8 Limpieza (fechas) library(lubridate) date() ## [1] &quot;Wed Nov 11 11:29:45 2020&quot; today() ## [1] &quot;2020-11-11&quot; ymd(&quot;20151021&quot;) ## [1] &quot;2015-10-21&quot; ymd(&quot;2015/11/30&quot;) ## [1] &quot;2015-11-30&quot; myd(&quot;11.2012.3&quot;) ## [1] &quot;2012-11-03&quot; dmy_hms(&quot;2/12/2013 14:05:01&quot;) ## [1] &quot;2013-12-02 14:05:01 UTC&quot; mdy(&quot;120112&quot;) ## [1] &quot;2012-12-01&quot; d1&lt;-dmy(&quot;15032020&quot;) class(d1) ## [1] &quot;Date&quot; #ts() 1.6.9 Limpieza (String) toupper(&quot;hola&quot;) ## [1] &quot;HOLA&quot; tolower(&quot;HOLA&quot;) ## [1] &quot;hola&quot; abc&lt;-letters[1:10] toupper(abc) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; tolower(&quot;Juan&quot;) ## [1] &quot;juan&quot; # Extraer partes de un texto substr(&quot;hola como estan&quot;,1,3) ## [1] &quot;hol&quot; substr(&quot;hola como estan&quot;,3,7) ## [1] &quot;la co&quot; # contar la cantidad de caracteres nchar(&quot;hola&quot;) ## [1] 4 nchar(c(&quot;hola&quot;,&quot;chau&quot;,&quot;LA paz&quot;)) ## [1] 4 4 30 x&lt;-c(&quot;LA-.paz&quot;,&quot;La Paz&quot;, &quot;La pas&quot;, &quot;La paz&quot;,&quot;lapaz&quot;,&quot;la 78 paz&quot;) x&lt;-toupper(x) x&lt;-gsub(&quot;PAS&quot;,&quot;PAZ&quot;,x) library(tm) x&lt;-removeNumbers(x) x&lt;-removePunctuation(x) x&lt;-gsub(&quot;LAPAZ&quot;,&quot;LA PAZ&quot;,x) x&lt;-stripWhitespace(x) nchar(x) ## [1] 6 6 6 6 6 6 nchar(gsub(&quot; &quot;,&quot; &quot;,x)) ## [1] 6 6 6 6 6 6 gsub(&quot;a&quot;,&quot;x&quot;,&quot;hola como estas&quot;) ## [1] &quot;holx como estxs&quot; grepl(&quot;a&quot;,c(&quot;hola&quot;,&quot;como&quot;)) ## [1] TRUE FALSE grepl(&quot;o&quot;,c(&quot;hola&quot;,&quot;como&quot;)) ## [1] TRUE TRUE #otra alternativa x&lt;-c(&quot;LA-.paz&quot;,&quot;La Paz&quot;, &quot;La pas&quot;, &quot;La paz&quot;,&quot;lapaz&quot;,&quot;la 78 paz&quot;) x&lt;-toupper(x) x[grepl(&quot;PAZ&quot;,x)]&lt;-&quot;LA PAZ&quot; x&lt;-gsub(&quot;PAS&quot;,&quot;PAZ&quot;,x) # para llevar a ascii utf8ToInt(&quot;la paz&quot;) ## [1] 108 97 32 112 97 122 utf8ToInt(&quot;@&quot;) ## [1] 64 library(stringi) Ejemplo de web scraping sobre la página https://www.worldometers.info/ library(rvest) url&lt;-&quot;https://www.worldometers.info/coronavirus/&quot; covid&lt;-read_html(url) bdcov&lt;-html_table(covid) bdnow&lt;-bdcov[[1]] str(bdnow) ## &#39;data.frame&#39;: 235 obs. of 19 variables: ## $ # : int NA NA NA NA NA NA NA NA 1 2 ... ## $ Country,Other : chr &quot;North America&quot; &quot;Asia&quot; &quot;South America&quot; &quot;Europe&quot; ... ## $ TotalCases : chr &quot;12,538,126&quot; &quot;14,659,295&quot; &quot;10,076,226&quot; &quot;12,767,242&quot; ... ## $ NewCases : chr &quot;+16,402&quot; &quot;+53,095&quot; &quot;+103&quot; &quot;+121,327&quot; ... ## $ TotalDeaths : chr &quot;368,071&quot; &quot;259,402&quot; &quot;304,163&quot; &quot;303,999&quot; ... ## $ NewDeaths : chr &quot;+723&quot; &quot;+990&quot; &quot;+6&quot; &quot;+2,276&quot; ... ## $ TotalRecovered : chr &quot;8,076,584&quot; &quot;13,084,184&quot; &quot;8,987,575&quot; &quot;4,717,158&quot; ... ## $ NewRecovered : chr &quot;+7,797&quot; &quot;+39,413&quot; &quot;+318&quot; &quot;+78,815&quot; ... ## $ ActiveCases : chr &quot;4,093,471&quot; &quot;1,315,709&quot; &quot;784,488&quot; &quot;7,746,085&quot; ... ## $ Serious,Critical : chr &quot;22,894&quot; &quot;23,694&quot; &quot;17,580&quot; &quot;28,517&quot; ... ## $ Tot Cases/1M pop : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Deaths/1M pop : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ TotalTests : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Tests/1M pop : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Population : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Continent : chr &quot;North America&quot; &quot;Asia&quot; &quot;South America&quot; &quot;Europe&quot; ... ## $ 1 Caseevery X ppl : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ 1 Deathevery X ppl: chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ 1 Testevery X ppl : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... Tarea: limpiar la base de datos Convertir las variables necesarias a numéricas Debe ser una base de solo países 1.6.10 Transformación load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData&quot;)) names(eh19p) ## [1] &quot;folio&quot; &quot;depto&quot; &quot;area&quot; ## [4] &quot;nro&quot; &quot;s02a_02&quot; &quot;s02a_03&quot; ## [7] &quot;s02a_04a&quot; &quot;s02a_04b&quot; &quot;s02a_04c&quot; ## [10] &quot;s02a_05&quot; &quot;s02a_06a&quot; &quot;s02a_06b&quot; ## [13] &quot;s02a_06c&quot; &quot;s02a_06d&quot; &quot;s02a_06e&quot; ## [16] &quot;s02a_06_b&quot; &quot;s02a_07_1&quot; &quot;s02a_07_2&quot; ## [19] &quot;s02a_07_3&quot; &quot;s02a_08&quot; &quot;s02a_10&quot; ## [22] &quot;s03a_01a&quot; &quot;s03a_01b&quot; &quot;s03a_01c&quot; ## [25] &quot;s03a_01d&quot; &quot;s03a_01d2_cod&quot; &quot;s03a_01e&quot; ## [28] &quot;s03a_02&quot; &quot;s03a_02e&quot; &quot;s03a_03&quot; ## [31] &quot;s03a_03a&quot; &quot;s03a_04&quot; &quot;s03a_04npioc&quot; ## [34] &quot;s04a_01a&quot; &quot;s04a_01b&quot; &quot;s04a_01e&quot; ## [37] &quot;s04a_02a&quot; &quot;s04a_02b&quot; &quot;s04a_02e&quot; ## [40] &quot;s04a_03a&quot; &quot;s04a_03b&quot; &quot;s04a_03c&quot; ## [43] &quot;s04a_03d&quot; &quot;s04a_03e&quot; &quot;s04a_03f&quot; ## [46] &quot;s04a_03g&quot; &quot;s04a_04a&quot; &quot;s04a_04b&quot; ## [49] &quot;s04a_04e&quot; &quot;S04A_0&quot; &quot;S04A_1&quot; ## [52] &quot;S04A_2&quot; &quot;s04a_05a&quot; &quot;s04a_05b&quot; ## [55] &quot;s04a_05c&quot; &quot;s04a_05d&quot; &quot;s04a_05e&quot; ## [58] &quot;s04a_06a&quot; &quot;s04a_07a&quot; &quot;s04a_07a_e&quot; ## [61] &quot;s04a_06b&quot; &quot;s04a_07b&quot; &quot;s04a_07b_e&quot; ## [64] &quot;s04a_06c&quot; &quot;s04a_07c&quot; &quot;s04a_07c_e&quot; ## [67] &quot;s04a_06d&quot; &quot;s04a_07d&quot; &quot;s04a_07d_e&quot; ## [70] &quot;s04a_06e&quot; &quot;s04a_07e&quot; &quot;s04a_07e_e&quot; ## [73] &quot;s04a_06f&quot; &quot;s04a_07f&quot; &quot;s04a_07f_e&quot; ## [76] &quot;s04a_06g&quot; &quot;s04a_07g&quot; &quot;s04a_07g_e&quot; ## [79] &quot;s04a_08&quot; &quot;s04a_08a1&quot; &quot;s04a_08a2&quot; ## [82] &quot;s04a_08b&quot; &quot;s04a_09&quot; &quot;s04a_09a&quot; ## [85] &quot;s04b_11a&quot; &quot;s04b_11b&quot; &quot;s04b_12&quot; ## [88] &quot;s04b_13&quot; &quot;s04b_14a&quot; &quot;s04b_14b&quot; ## [91] &quot;s04b_15&quot; &quot;s04b_15e&quot; &quot;S04B_9&quot; ## [94] &quot;S04B_A&quot; &quot;S04B_B&quot; &quot;s04b_16&quot; ## [97] &quot;s04b_16e&quot; &quot;S04B_6&quot; &quot;S04B_7&quot; ## [100] &quot;S04B_8&quot; &quot;s04b_17&quot; &quot;s04b_17e&quot; ## [103] &quot;S04B_3&quot; &quot;S04B_4&quot; &quot;S04B_5&quot; ## [106] &quot;s04b_18&quot; &quot;s04b_18e&quot; &quot;S04B_0&quot; ## [109] &quot;S04B_1&quot; &quot;S04B_2&quot; &quot;s04b_19&quot; ## [112] &quot;s04b_20a1&quot; &quot;s04b_20a2&quot; &quot;s04b_20b&quot; ## [115] &quot;s04b_21a&quot; &quot;s04b_21b&quot; &quot;s04b_21b2&quot; ## [118] &quot;s04c_22&quot; &quot;s04c_23&quot; &quot;s04d_24&quot; ## [121] &quot;s04d_25&quot; &quot;s04d_26&quot; &quot;s04d_27a&quot; ## [124] &quot;s04d_27b&quot; &quot;s04e_28a&quot; &quot;s04e_28b&quot; ## [127] &quot;s04e_29a&quot; &quot;s04e_29b&quot; &quot;s04e_30a&quot; ## [130] &quot;s04e_30b&quot; &quot;s04e_30c_cod&quot; &quot;s04e_31a&quot; ## [133] &quot;s04e_31b&quot; &quot;s04e_31c&quot; &quot;s04e_31d&quot; ## [136] &quot;s04e_31e&quot; &quot;s04e_31f&quot; &quot;s04e_31_e&quot; ## [139] &quot;s04e_32a&quot; &quot;s04e_32b&quot; &quot;s04e_33a&quot; ## [142] &quot;s04e_33b&quot; &quot;s04_e_34a&quot; &quot;s04f_34&quot; ## [145] &quot;s04f_35a&quot; &quot;s04f_35b&quot; &quot;s04f_35c&quot; ## [148] &quot;s04f_35e&quot; &quot;s05a_01&quot; &quot;s05a_01a&quot; ## [151] &quot;s05a_02a&quot; &quot;s05a_02c&quot; &quot;s05a_03a&quot; ## [154] &quot;s05a_03c&quot; &quot;s05a_04&quot; &quot;s05a_05&quot; ## [157] &quot;s05a_05_e&quot; &quot;s05a_06a&quot; &quot;s05a_06c&quot; ## [160] &quot;s05a_07a&quot; &quot;s05a_07b&quot; &quot;s05a_08&quot; ## [163] &quot;s05a_09&quot; &quot;s05b_10&quot; &quot;s05b_11&quot; ## [166] &quot;s05b_11_e&quot; &quot;s05b_11a&quot; &quot;s05c_13a&quot; ## [169] &quot;s05c_13b&quot; &quot;s05c_13c&quot; &quot;s05c_13d&quot; ## [172] &quot;s05c_13e&quot; &quot;s05c_13f&quot; &quot;s05c_13g&quot; ## [175] &quot;s05c_13h&quot; &quot;s05c_13_e&quot; &quot;s05c_14a&quot; ## [178] &quot;s05c_14b&quot; &quot;s05c_15a&quot; &quot;s05c_15b&quot; ## [181] &quot;s05d_17&quot; &quot;s05d_18&quot; &quot;s05d_19a&quot; ## [184] &quot;s05d_19b&quot; &quot;s05d_20a&quot; &quot;s05d_20b&quot; ## [187] &quot;s05d_21a&quot; &quot;s05d_21b&quot; &quot;s05d_21e&quot; ## [190] &quot;s05d_22a&quot; &quot;s05d_22b&quot; &quot;s05d_22c&quot; ## [193] &quot;s05d_22d&quot; &quot;s05d_22e&quot; &quot;s05d_22f&quot; ## [196] &quot;s05d_22g&quot; &quot;s05d_22h&quot; &quot;s05d_22i&quot; ## [199] &quot;s05d_22j&quot; &quot;s05d_22k&quot; &quot;s05d_22l&quot; ## [202] &quot;s05d_22_e&quot; &quot;s06a_01&quot; &quot;s06a_02&quot; ## [205] &quot;s06a_03&quot; &quot;s06a_04&quot; &quot;s06a_05&quot; ## [208] &quot;s06a_06aa&quot; &quot;s06a_06ab&quot; &quot;s06a_06ac&quot; ## [211] &quot;s06a_06e&quot; &quot;s06a_07&quot; &quot;s06a_08a&quot; ## [214] &quot;s06a_08b&quot; &quot;s06a_09&quot; &quot;s06a_09e&quot; ## [217] &quot;s06a_10&quot; &quot;s06a_10e&quot; &quot;s06b_11a&quot; ## [220] &quot;s06b_11a_cod&quot; &quot;s06b_11b&quot; &quot;s06b_12a&quot; ## [223] &quot;s06b_12a_cod&quot; &quot;s06b_12b&quot; &quot;s06b_13&quot; ## [226] &quot;s06b_13a&quot; &quot;s06b_13b&quot; &quot;s06b_13c&quot; ## [229] &quot;s06b_14&quot; &quot;s06b_15aa&quot; &quot;s06b_15ab&quot; ## [232] &quot;s06b_15ba&quot; &quot;s06b_15bb&quot; &quot;s06b_15ca&quot; ## [235] &quot;s06b_15cb&quot; &quot;s06b_15da&quot; &quot;s06b_15db&quot; ## [238] &quot;s06b_17&quot; &quot;s06b_18&quot; &quot;s06b_19a&quot; ## [241] &quot;s06b_19b&quot; &quot;s06b_20&quot; &quot;s06b_20e&quot; ## [244] &quot;s06b_21a&quot; &quot;s06b_21b&quot; &quot;s06b_22&quot; ## [247] &quot;s06b_23aa&quot; &quot;s06b_23ab&quot; &quot;s06c_25a&quot; ## [250] &quot;s06c_25b&quot; &quot;s06c_26a&quot; &quot;s06c_26b&quot; ## [253] &quot;s06c_27aa&quot; &quot;s06c_27ab&quot; &quot;s06c_27ba&quot; ## [256] &quot;s06c_27bb&quot; &quot;s06c_28a&quot; &quot;s06c_28a1&quot; ## [259] &quot;s06c_28b&quot; &quot;s06c_29a&quot; &quot;s06c_29b&quot; ## [262] &quot;s06c_30a&quot; &quot;s06c_30a1&quot; &quot;s06c_30a2&quot; ## [265] &quot;s06c_30b&quot; &quot;s06c_30b1&quot; &quot;s06c_30b2&quot; ## [268] &quot;s06c_30c&quot; &quot;s06c_30c1&quot; &quot;s06c_30c2&quot; ## [271] &quot;s06c_30d&quot; &quot;s06c_30d1&quot; &quot;s06c_30d2&quot; ## [274] &quot;s06c_30e&quot; &quot;s06c_30e1&quot; &quot;s06c_30e2&quot; ## [277] &quot;s06d_31a&quot; &quot;s06d_31b&quot; &quot;s06d_32aa&quot; ## [280] &quot;s06d_32ab&quot; &quot;s06d_32ba&quot; &quot;s06d_32bb&quot; ## [283] &quot;s06d_32ca&quot; &quot;s06d_32cb&quot; &quot;s06d_32da&quot; ## [286] &quot;s06d_32db&quot; &quot;s06d_32ea&quot; &quot;s06d_32eb&quot; ## [289] &quot;s06d_32fa&quot; &quot;s06d_32fb&quot; &quot;s06d_32ga&quot; ## [292] &quot;s06d_32gb&quot; &quot;s06d_32ha&quot; &quot;s06d_32hb&quot; ## [295] &quot;s06d_33a&quot; &quot;s06d_33b&quot; &quot;s06d_34&quot; ## [298] &quot;s06e_35a&quot; &quot;s06e_35a_cod&quot; &quot;s06e_35b&quot; ## [301] &quot;s06e_36&quot; &quot;s06e_37&quot; &quot;s06e_38a&quot; ## [304] &quot;s06e_38b&quot; &quot;s06e_39&quot; &quot;s06e_40&quot; ## [307] &quot;s06e_40b&quot; &quot;s06f_42a&quot; &quot;s06f_42b&quot; ## [310] &quot;s06f_43a&quot; &quot;s06f_43a1&quot; &quot;s06f_43b&quot; ## [313] &quot;s06f_43b1&quot; &quot;s06f_43c&quot; &quot;s06f_43c1&quot; ## [316] &quot;s06f_44a&quot; &quot;s06f_44b&quot; &quot;s06f_45aa&quot; ## [319] &quot;s06f_45ab&quot; &quot;s06f_45ba&quot; &quot;s06f_45bb&quot; ## [322] &quot;s06f_45ca&quot; &quot;s06f_45cb&quot; &quot;s06f_45da&quot; ## [325] &quot;s06f_45db&quot; &quot;s06f_45ea&quot; &quot;s06f_45eb&quot; ## [328] &quot;s06f_45fa&quot; &quot;s06f_45fb&quot; &quot;s06f_45ga&quot; ## [331] &quot;s06f_45gb&quot; &quot;s06f_45ha&quot; &quot;s06f_45hb&quot; ## [334] &quot;s06f_46a&quot; &quot;s06f_46b&quot; &quot;s06g_47&quot; ## [337] &quot;s06g_48&quot; &quot;s06g_49&quot; &quot;s06g_49e&quot; ## [340] &quot;s06g_50&quot; &quot;s06g_50e&quot; &quot;s06g_51&quot; ## [343] &quot;s06g_51e&quot; &quot;s06g_52&quot; &quot;s06g_53&quot; ## [346] &quot;s06g_54&quot; &quot;s06g_55&quot; &quot;s07a_01a&quot; ## [349] &quot;s07a_01b&quot; &quot;s07a_01c&quot; &quot;s07a_01d&quot; ## [352] &quot;s07a_01e&quot; &quot;s07a_01e0&quot; &quot;s07a_01e1&quot; ## [355] &quot;s07a_01e1e&quot; &quot;s07a_01e2&quot; &quot;s07a_01e2e&quot; ## [358] &quot;s07a_02a&quot; &quot;s07a_02b&quot; &quot;s07a_02c&quot; ## [361] &quot;s07a_02ce&quot; &quot;s07a_03a&quot; &quot;s07a_03b&quot; ## [364] &quot;s07a_03c&quot; &quot;s07a_04a&quot; &quot;s07a_04b&quot; ## [367] &quot;s07a_04c&quot; &quot;s07a_04d&quot; &quot;s07b_05aa&quot; ## [370] &quot;s07b_05ab&quot; &quot;s07b_05ba&quot; &quot;s07b_05bb&quot; ## [373] &quot;s07b_05ca&quot; &quot;s07b_05cb&quot; &quot;s07b_05da&quot; ## [376] &quot;s07b_05db&quot; &quot;s07b_05de&quot; &quot;s07b_05ea&quot; ## [379] &quot;s07b_05eb&quot; &quot;s07b_05ee&quot; &quot;s07c_06&quot; ## [382] &quot;s07c_07&quot; &quot;s07c_08a&quot; &quot;s07c_08b&quot; ## [385] &quot;s07c_08e&quot; &quot;s07c_09&quot; &quot;s07c_09e&quot; ## [388] &quot;s07c_10&quot; &quot;s08a_01&quot; &quot;s08a_03a&quot; ## [391] &quot;s08a_03b&quot; &quot;s08a_03c&quot; &quot;s08a_03e&quot; ## [394] &quot;s08a_04&quot; &quot;s08a_06&quot; &quot;upm&quot; ## [397] &quot;estrato&quot; &quot;factor&quot; &quot;tipohogar&quot; ## [400] &quot;cobersalud&quot; &quot;hnv_ult_a&quot; &quot;quienatenparto&quot; ## [403] &quot;dondeatenparto&quot; &quot;niv_ed&quot; &quot;niv_ed_g&quot; ## [406] &quot;cmasi&quot; &quot;educ_prev&quot; &quot;aestudio&quot; ## [409] &quot;cob_op&quot; &quot;caeb_op&quot; &quot;pet&quot; ## [412] &quot;ocupado&quot; &quot;cesante&quot; &quot;aspirante&quot; ## [415] &quot;desocupado&quot; &quot;pea&quot; &quot;temporal&quot; ## [418] &quot;permanente&quot; &quot;pei&quot; &quot;condact&quot; ## [421] &quot;phrs&quot; &quot;shrs&quot; &quot;tothrs&quot; ## [424] &quot;yprilab&quot; &quot;yseclab&quot; &quot;ylab&quot; ## [427] &quot;ynolab&quot; &quot;yper&quot; &quot;yhog&quot; ## [430] &quot;yhogpc&quot; &quot;z&quot; &quot;zext&quot; ## [433] &quot;p0&quot; &quot;p1&quot; &quot;p2&quot; ## [436] &quot;pext0&quot; &quot;pext1&quot; &quot;pext2&quot; Estandarizar variables summary(eh19p$s02a_03) #edad ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 12.00 26.00 29.69 44.00 98.00 summary(eh19p$tothrs) # total de horas de trabajo semanal ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.00 30.00 42.00 42.29 54.00 112.50 20454 summary(eh19p$ylab) # ingreso laboral mensual ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 10 1416 2598 3075 4000 32917 23816 sd(eh19p$s02a_03) ## [1] 21.05689 sd(eh19p$tothrs,na.rm = T) ## [1] 19.33807 sd(eh19p$ylab,na.rm = T) ## [1] 2470.079 x1&lt;-scale(eh19p$s02a_03) x2&lt;-scale(eh19p$tothrs) x3&lt;-scale(eh19p$ylab) sd(x1);sd(x2,na.rm = T);sd(x3,na.rm = T) ## [1] 1 ## [1] 1 ## [1] 1 par(mfrow=c(2,3)) boxplot(eh19p$s02a_03,ylim=c(0,25000)) boxplot(eh19p$tothrs,ylim=c(0,25000)) boxplot(eh19p$ylab,ylim=c(0,25000)) boxplot(x1,ylim=c(-3,3)) boxplot(x2,ylim=c(-3,3)) boxplot(x3,ylim=c(-3,3)) par(mfrow=c(2,3)) plot(density(eh19p$s02a_03)) plot(density(eh19p$tothrs,na.rm=T)) plot(density(eh19p$ylab,na.rm=T)) plot(density(x1)) plot(density(x2,na.rm=T)) plot(density(x3,na.rm=T)) mean(eh19p$ylab,na.rm=T) ## [1] 3074.659 median(eh19p$ylab,na.rm=T) ## [1] 2598 Función logarítmo dev.off() ## null device ## 1 curve(log,xlim=c(10,30000)) x1&lt;-log(eh19p$s02a_03) x2&lt;-log(eh19p$tothrs) x3&lt;-log(eh19p$ylab) par(mfrow=c(2,3)) plot(density(eh19p$s02a_03)) plot(density(eh19p$tothrs,na.rm=T)) plot(density(eh19p$ylab,na.rm=T)) plot(density(x1)) plot(density(x2,na.rm=T)) plot(density(x3,na.rm=T)) dev.off() ## null device ## 1 Creación de variables eh19p$log_ylab &lt;-log(eh19p$ylab) eh19p$scale_ylab &lt;-scale(eh19p$ylab) names(eh19p) ## [1] &quot;folio&quot; &quot;depto&quot; &quot;area&quot; ## [4] &quot;nro&quot; &quot;s02a_02&quot; &quot;s02a_03&quot; ## [7] &quot;s02a_04a&quot; &quot;s02a_04b&quot; &quot;s02a_04c&quot; ## [10] &quot;s02a_05&quot; &quot;s02a_06a&quot; &quot;s02a_06b&quot; ## [13] &quot;s02a_06c&quot; &quot;s02a_06d&quot; &quot;s02a_06e&quot; ## [16] &quot;s02a_06_b&quot; &quot;s02a_07_1&quot; &quot;s02a_07_2&quot; ## [19] &quot;s02a_07_3&quot; &quot;s02a_08&quot; &quot;s02a_10&quot; ## [22] &quot;s03a_01a&quot; &quot;s03a_01b&quot; &quot;s03a_01c&quot; ## [25] &quot;s03a_01d&quot; &quot;s03a_01d2_cod&quot; &quot;s03a_01e&quot; ## [28] &quot;s03a_02&quot; &quot;s03a_02e&quot; &quot;s03a_03&quot; ## [31] &quot;s03a_03a&quot; &quot;s03a_04&quot; &quot;s03a_04npioc&quot; ## [34] &quot;s04a_01a&quot; &quot;s04a_01b&quot; &quot;s04a_01e&quot; ## [37] &quot;s04a_02a&quot; &quot;s04a_02b&quot; &quot;s04a_02e&quot; ## [40] &quot;s04a_03a&quot; &quot;s04a_03b&quot; &quot;s04a_03c&quot; ## [43] &quot;s04a_03d&quot; &quot;s04a_03e&quot; &quot;s04a_03f&quot; ## [46] &quot;s04a_03g&quot; &quot;s04a_04a&quot; &quot;s04a_04b&quot; ## [49] &quot;s04a_04e&quot; &quot;S04A_0&quot; &quot;S04A_1&quot; ## [52] &quot;S04A_2&quot; &quot;s04a_05a&quot; &quot;s04a_05b&quot; ## [55] &quot;s04a_05c&quot; &quot;s04a_05d&quot; &quot;s04a_05e&quot; ## [58] &quot;s04a_06a&quot; &quot;s04a_07a&quot; &quot;s04a_07a_e&quot; ## [61] &quot;s04a_06b&quot; &quot;s04a_07b&quot; &quot;s04a_07b_e&quot; ## [64] &quot;s04a_06c&quot; &quot;s04a_07c&quot; &quot;s04a_07c_e&quot; ## [67] &quot;s04a_06d&quot; &quot;s04a_07d&quot; &quot;s04a_07d_e&quot; ## [70] &quot;s04a_06e&quot; &quot;s04a_07e&quot; &quot;s04a_07e_e&quot; ## [73] &quot;s04a_06f&quot; &quot;s04a_07f&quot; &quot;s04a_07f_e&quot; ## [76] &quot;s04a_06g&quot; &quot;s04a_07g&quot; &quot;s04a_07g_e&quot; ## [79] &quot;s04a_08&quot; &quot;s04a_08a1&quot; &quot;s04a_08a2&quot; ## [82] &quot;s04a_08b&quot; &quot;s04a_09&quot; &quot;s04a_09a&quot; ## [85] &quot;s04b_11a&quot; &quot;s04b_11b&quot; &quot;s04b_12&quot; ## [88] &quot;s04b_13&quot; &quot;s04b_14a&quot; &quot;s04b_14b&quot; ## [91] &quot;s04b_15&quot; &quot;s04b_15e&quot; &quot;S04B_9&quot; ## [94] &quot;S04B_A&quot; &quot;S04B_B&quot; &quot;s04b_16&quot; ## [97] &quot;s04b_16e&quot; &quot;S04B_6&quot; &quot;S04B_7&quot; ## [100] &quot;S04B_8&quot; &quot;s04b_17&quot; &quot;s04b_17e&quot; ## [103] &quot;S04B_3&quot; &quot;S04B_4&quot; &quot;S04B_5&quot; ## [106] &quot;s04b_18&quot; &quot;s04b_18e&quot; &quot;S04B_0&quot; ## [109] &quot;S04B_1&quot; &quot;S04B_2&quot; &quot;s04b_19&quot; ## [112] &quot;s04b_20a1&quot; &quot;s04b_20a2&quot; &quot;s04b_20b&quot; ## [115] &quot;s04b_21a&quot; &quot;s04b_21b&quot; &quot;s04b_21b2&quot; ## [118] &quot;s04c_22&quot; &quot;s04c_23&quot; &quot;s04d_24&quot; ## [121] &quot;s04d_25&quot; &quot;s04d_26&quot; &quot;s04d_27a&quot; ## [124] &quot;s04d_27b&quot; &quot;s04e_28a&quot; &quot;s04e_28b&quot; ## [127] &quot;s04e_29a&quot; &quot;s04e_29b&quot; &quot;s04e_30a&quot; ## [130] &quot;s04e_30b&quot; &quot;s04e_30c_cod&quot; &quot;s04e_31a&quot; ## [133] &quot;s04e_31b&quot; &quot;s04e_31c&quot; &quot;s04e_31d&quot; ## [136] &quot;s04e_31e&quot; &quot;s04e_31f&quot; &quot;s04e_31_e&quot; ## [139] &quot;s04e_32a&quot; &quot;s04e_32b&quot; &quot;s04e_33a&quot; ## [142] &quot;s04e_33b&quot; &quot;s04_e_34a&quot; &quot;s04f_34&quot; ## [145] &quot;s04f_35a&quot; &quot;s04f_35b&quot; &quot;s04f_35c&quot; ## [148] &quot;s04f_35e&quot; &quot;s05a_01&quot; &quot;s05a_01a&quot; ## [151] &quot;s05a_02a&quot; &quot;s05a_02c&quot; &quot;s05a_03a&quot; ## [154] &quot;s05a_03c&quot; &quot;s05a_04&quot; &quot;s05a_05&quot; ## [157] &quot;s05a_05_e&quot; &quot;s05a_06a&quot; &quot;s05a_06c&quot; ## [160] &quot;s05a_07a&quot; &quot;s05a_07b&quot; &quot;s05a_08&quot; ## [163] &quot;s05a_09&quot; &quot;s05b_10&quot; &quot;s05b_11&quot; ## [166] &quot;s05b_11_e&quot; &quot;s05b_11a&quot; &quot;s05c_13a&quot; ## [169] &quot;s05c_13b&quot; &quot;s05c_13c&quot; &quot;s05c_13d&quot; ## [172] &quot;s05c_13e&quot; &quot;s05c_13f&quot; &quot;s05c_13g&quot; ## [175] &quot;s05c_13h&quot; &quot;s05c_13_e&quot; &quot;s05c_14a&quot; ## [178] &quot;s05c_14b&quot; &quot;s05c_15a&quot; &quot;s05c_15b&quot; ## [181] &quot;s05d_17&quot; &quot;s05d_18&quot; &quot;s05d_19a&quot; ## [184] &quot;s05d_19b&quot; &quot;s05d_20a&quot; &quot;s05d_20b&quot; ## [187] &quot;s05d_21a&quot; &quot;s05d_21b&quot; &quot;s05d_21e&quot; ## [190] &quot;s05d_22a&quot; &quot;s05d_22b&quot; &quot;s05d_22c&quot; ## [193] &quot;s05d_22d&quot; &quot;s05d_22e&quot; &quot;s05d_22f&quot; ## [196] &quot;s05d_22g&quot; &quot;s05d_22h&quot; &quot;s05d_22i&quot; ## [199] &quot;s05d_22j&quot; &quot;s05d_22k&quot; &quot;s05d_22l&quot; ## [202] &quot;s05d_22_e&quot; &quot;s06a_01&quot; &quot;s06a_02&quot; ## [205] &quot;s06a_03&quot; &quot;s06a_04&quot; &quot;s06a_05&quot; ## [208] &quot;s06a_06aa&quot; &quot;s06a_06ab&quot; &quot;s06a_06ac&quot; ## [211] &quot;s06a_06e&quot; &quot;s06a_07&quot; &quot;s06a_08a&quot; ## [214] &quot;s06a_08b&quot; &quot;s06a_09&quot; &quot;s06a_09e&quot; ## [217] &quot;s06a_10&quot; &quot;s06a_10e&quot; &quot;s06b_11a&quot; ## [220] &quot;s06b_11a_cod&quot; &quot;s06b_11b&quot; &quot;s06b_12a&quot; ## [223] &quot;s06b_12a_cod&quot; &quot;s06b_12b&quot; &quot;s06b_13&quot; ## [226] &quot;s06b_13a&quot; &quot;s06b_13b&quot; &quot;s06b_13c&quot; ## [229] &quot;s06b_14&quot; &quot;s06b_15aa&quot; &quot;s06b_15ab&quot; ## [232] &quot;s06b_15ba&quot; &quot;s06b_15bb&quot; &quot;s06b_15ca&quot; ## [235] &quot;s06b_15cb&quot; &quot;s06b_15da&quot; &quot;s06b_15db&quot; ## [238] &quot;s06b_17&quot; &quot;s06b_18&quot; &quot;s06b_19a&quot; ## [241] &quot;s06b_19b&quot; &quot;s06b_20&quot; &quot;s06b_20e&quot; ## [244] &quot;s06b_21a&quot; &quot;s06b_21b&quot; &quot;s06b_22&quot; ## [247] &quot;s06b_23aa&quot; &quot;s06b_23ab&quot; &quot;s06c_25a&quot; ## [250] &quot;s06c_25b&quot; &quot;s06c_26a&quot; &quot;s06c_26b&quot; ## [253] &quot;s06c_27aa&quot; &quot;s06c_27ab&quot; &quot;s06c_27ba&quot; ## [256] &quot;s06c_27bb&quot; &quot;s06c_28a&quot; &quot;s06c_28a1&quot; ## [259] &quot;s06c_28b&quot; &quot;s06c_29a&quot; &quot;s06c_29b&quot; ## [262] &quot;s06c_30a&quot; &quot;s06c_30a1&quot; &quot;s06c_30a2&quot; ## [265] &quot;s06c_30b&quot; &quot;s06c_30b1&quot; &quot;s06c_30b2&quot; ## [268] &quot;s06c_30c&quot; &quot;s06c_30c1&quot; &quot;s06c_30c2&quot; ## [271] &quot;s06c_30d&quot; &quot;s06c_30d1&quot; &quot;s06c_30d2&quot; ## [274] &quot;s06c_30e&quot; &quot;s06c_30e1&quot; &quot;s06c_30e2&quot; ## [277] &quot;s06d_31a&quot; &quot;s06d_31b&quot; &quot;s06d_32aa&quot; ## [280] &quot;s06d_32ab&quot; &quot;s06d_32ba&quot; &quot;s06d_32bb&quot; ## [283] &quot;s06d_32ca&quot; &quot;s06d_32cb&quot; &quot;s06d_32da&quot; ## [286] &quot;s06d_32db&quot; &quot;s06d_32ea&quot; &quot;s06d_32eb&quot; ## [289] &quot;s06d_32fa&quot; &quot;s06d_32fb&quot; &quot;s06d_32ga&quot; ## [292] &quot;s06d_32gb&quot; &quot;s06d_32ha&quot; &quot;s06d_32hb&quot; ## [295] &quot;s06d_33a&quot; &quot;s06d_33b&quot; &quot;s06d_34&quot; ## [298] &quot;s06e_35a&quot; &quot;s06e_35a_cod&quot; &quot;s06e_35b&quot; ## [301] &quot;s06e_36&quot; &quot;s06e_37&quot; &quot;s06e_38a&quot; ## [304] &quot;s06e_38b&quot; &quot;s06e_39&quot; &quot;s06e_40&quot; ## [307] &quot;s06e_40b&quot; &quot;s06f_42a&quot; &quot;s06f_42b&quot; ## [310] &quot;s06f_43a&quot; &quot;s06f_43a1&quot; &quot;s06f_43b&quot; ## [313] &quot;s06f_43b1&quot; &quot;s06f_43c&quot; &quot;s06f_43c1&quot; ## [316] &quot;s06f_44a&quot; &quot;s06f_44b&quot; &quot;s06f_45aa&quot; ## [319] &quot;s06f_45ab&quot; &quot;s06f_45ba&quot; &quot;s06f_45bb&quot; ## [322] &quot;s06f_45ca&quot; &quot;s06f_45cb&quot; &quot;s06f_45da&quot; ## [325] &quot;s06f_45db&quot; &quot;s06f_45ea&quot; &quot;s06f_45eb&quot; ## [328] &quot;s06f_45fa&quot; &quot;s06f_45fb&quot; &quot;s06f_45ga&quot; ## [331] &quot;s06f_45gb&quot; &quot;s06f_45ha&quot; &quot;s06f_45hb&quot; ## [334] &quot;s06f_46a&quot; &quot;s06f_46b&quot; &quot;s06g_47&quot; ## [337] &quot;s06g_48&quot; &quot;s06g_49&quot; &quot;s06g_49e&quot; ## [340] &quot;s06g_50&quot; &quot;s06g_50e&quot; &quot;s06g_51&quot; ## [343] &quot;s06g_51e&quot; &quot;s06g_52&quot; &quot;s06g_53&quot; ## [346] &quot;s06g_54&quot; &quot;s06g_55&quot; &quot;s07a_01a&quot; ## [349] &quot;s07a_01b&quot; &quot;s07a_01c&quot; &quot;s07a_01d&quot; ## [352] &quot;s07a_01e&quot; &quot;s07a_01e0&quot; &quot;s07a_01e1&quot; ## [355] &quot;s07a_01e1e&quot; &quot;s07a_01e2&quot; &quot;s07a_01e2e&quot; ## [358] &quot;s07a_02a&quot; &quot;s07a_02b&quot; &quot;s07a_02c&quot; ## [361] &quot;s07a_02ce&quot; &quot;s07a_03a&quot; &quot;s07a_03b&quot; ## [364] &quot;s07a_03c&quot; &quot;s07a_04a&quot; &quot;s07a_04b&quot; ## [367] &quot;s07a_04c&quot; &quot;s07a_04d&quot; &quot;s07b_05aa&quot; ## [370] &quot;s07b_05ab&quot; &quot;s07b_05ba&quot; &quot;s07b_05bb&quot; ## [373] &quot;s07b_05ca&quot; &quot;s07b_05cb&quot; &quot;s07b_05da&quot; ## [376] &quot;s07b_05db&quot; &quot;s07b_05de&quot; &quot;s07b_05ea&quot; ## [379] &quot;s07b_05eb&quot; &quot;s07b_05ee&quot; &quot;s07c_06&quot; ## [382] &quot;s07c_07&quot; &quot;s07c_08a&quot; &quot;s07c_08b&quot; ## [385] &quot;s07c_08e&quot; &quot;s07c_09&quot; &quot;s07c_09e&quot; ## [388] &quot;s07c_10&quot; &quot;s08a_01&quot; &quot;s08a_03a&quot; ## [391] &quot;s08a_03b&quot; &quot;s08a_03c&quot; &quot;s08a_03e&quot; ## [394] &quot;s08a_04&quot; &quot;s08a_06&quot; &quot;upm&quot; ## [397] &quot;estrato&quot; &quot;factor&quot; &quot;tipohogar&quot; ## [400] &quot;cobersalud&quot; &quot;hnv_ult_a&quot; &quot;quienatenparto&quot; ## [403] &quot;dondeatenparto&quot; &quot;niv_ed&quot; &quot;niv_ed_g&quot; ## [406] &quot;cmasi&quot; &quot;educ_prev&quot; &quot;aestudio&quot; ## [409] &quot;cob_op&quot; &quot;caeb_op&quot; &quot;pet&quot; ## [412] &quot;ocupado&quot; &quot;cesante&quot; &quot;aspirante&quot; ## [415] &quot;desocupado&quot; &quot;pea&quot; &quot;temporal&quot; ## [418] &quot;permanente&quot; &quot;pei&quot; &quot;condact&quot; ## [421] &quot;phrs&quot; &quot;shrs&quot; &quot;tothrs&quot; ## [424] &quot;yprilab&quot; &quot;yseclab&quot; &quot;ylab&quot; ## [427] &quot;ynolab&quot; &quot;yper&quot; &quot;yhog&quot; ## [430] &quot;yhogpc&quot; &quot;z&quot; &quot;zext&quot; ## [433] &quot;p0&quot; &quot;p1&quot; &quot;p2&quot; ## [436] &quot;pext0&quot; &quot;pext1&quot; &quot;pext2&quot; ## [439] &quot;log_ylab&quot; &quot;scale_ylab&quot; #install.packages(&quot;dplyr&quot;) library(dplyr) # filtrado, selección, creación de variables, resumen #nota: dplyr se enfoca en el encadenamiento de comandos, con una lógica similar al SQL # %&gt;% # operador pipe: ctr + mayus + m eh19p &lt;- eh19p %&gt;% mutate(x1=ylab^2,llylab=log(ylab), tothrs_mensual=tothrs*4.35,mujer=s02a_02==&quot;2.Mujer&quot;) #cut() # crear clases # grandes grupos de edad eh19p&lt;-eh19p %&gt;% mutate(gedad=cut(s02a_03,c(-1,18,60,max(s02a_03)),labels = c(&quot;&lt;=18&quot;,&quot;19 a 60&quot;,&quot;&gt;60&quot;) )) eh19p %&gt;% select(1,folio,s02a_03,gedad) %&gt;% head() ## folio s02a_03 gedad ## 1 111-00416110273-A-0021 42 19 a 60 ## 2 111-00416110273-A-0021 36 19 a 60 ## 3 111-00416110273-A-0021 19 19 a 60 ## 4 111-00416110273-A-0021 13 &lt;=18 ## 5 111-00416110273-A-0021 3 &lt;=18 ## 6 111-00416110273-A-0021 86 &gt;60 table(eh19p$gedad) ## ## &lt;=18 19 a 60 &gt;60 ## 14831 20736 4038 eh19p %&gt;% select(gedad) %&gt;% table() ## . ## &lt;=18 19 a 60 &gt;60 ## 14831 20736 4038 barplot(table(eh19p$gedad)) Recodificar variables load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData&quot;)) library(dplyr) a&lt;-c(1:10) recode(a,`1` = 20L,`2` = 20L,`4` = 30L) eh19p$sexo&lt;-recode(eh19p$s02a_02,&quot;1.Hombre&quot;=&quot;H&quot;,&quot;2.Mujer&quot;=&quot;M&quot;) table(eh19p$sexo) eh19p&lt;-eh19p %&gt;% mutate(sexo2=recode(s02a_02,&quot;1.Hombre&quot;=&quot;M&quot;,&quot;2.Mujer&quot;=&quot;F&quot;)) eh19p %&gt;% select(sexo2) %&gt;% table() # binarias unique(eh19p$depto) # se quiere crear una nueva variable, llamada región: # Altiplano: LP, OR, PT # Valle: CB, CH, TR # Llano: SC, BN, PD # tarea #if_else() # trabaja con spark, para crear binarios #case_when() # múltiple categorías basadas en reglas v1&lt;-c(&quot;La Paz&quot;,&quot;Oruro&quot;,&quot;Potosí&quot;) v2&lt;-c(&quot;Chuquisaca&quot;,&quot;Cochabamba&quot;,&quot;Tarija&quot;) v3&lt;-c(&quot;Santa Cruz&quot;,&quot;Beni&quot;,&quot;Pando&quot;) eh19p %&gt;% mutate(altiplano = depto %in% v1 ) %&gt;% select(altiplano) %&gt;% table() eh19p &lt;- eh19p %&gt;% mutate(altiplano = depto %in% v1 , valle = depto %in% v2,llano = depto %in% v3) names(eh19p) 1.6.11 Definir diseño de encuesta por muestreo Si tenemos una base de datos proveniente de una encuesta por muestreo, debemos tener conocimiento de las características del diseño muestral empleado en la encuesta, ya que este diseño afecta de forma directa el proceso de estimación y tiene un error de muestreo. Principalmente lo siguiente: Si el diseño tiene etapas (o mono etápico con conglomerados), la variable de conglomeración y de estratificación son muy relevantes. Estas afectan directamente a los errores de las estimaciones. Si el esquema de selección de las unidades muestrales a sido autoponderada (MAS, todas las unidades tenían la misma probabilidad de ser seleccionadas) o no. Si no es autponderada se requiere la variable conocida como el factor de expansión (inverso de la probabilidad de selección) Hay tres variables relevantes: conglomerados (principalmente primera etapa), estratificación (principalmente primera etapa) y factor de expansión. Idealmente en un muestreo de varias etapas y estratificado el factor por finitud es necesario ya que permite mejorar la aproximación a la varianza. #install.packages(&quot;survey&quot;) library(survey) # no trabaja con el concepto de dplyr, no permite el uso de &quot;%&gt;%&quot; library(srvyr) # permite el uso del operador %&gt;% names(eh19p) #survey sd_eh19p&lt;-svydesign(ids = ~upm + folio, strata = ~estrato, weights = ~factor,data = eh19p) str(sd_eh19p) table(eh19p$p0) prop.table(table(eh19p$p0))*100 # pobreza moderada en la muestra svytable(~p0 ,design = sd_eh19p) prop.table(svytable(~p0 ,design = sd_eh19p))*100 summary(svytable(~p0 ,design = sd_eh19p)) t1&lt;-svymean(~ylab,design = sd_eh19p,na.rm=T,deff=T) cv(t1) confint(t1) t2&lt;-svyby(~ylab,by=~depto+area,design = sd_eh19p,FUN = svymean,na.rm=T) cv(t2) confint(t2) summary(svytable(~depto+p0 ,design = sd_eh19p)) # revisar # departamento prop.table(table(eh19p$depto,eh19p$p0),1)*100 prop.table(svytable(~depto+p0 ,design = sd_eh19p),1)*100 svydesign(ids=~1,data=bd)#mas svydesign(ids=~1,strata = estrato,data=bd)#mas estraficado svydesign(ids=~1,strata = ~estrato,weights = ~ponderador,data=bd)#pps estraficado svymean(~p0, design=sd_eh19p,na.rm=T) svytotal(~p0, design=sd_eh19p,na.rm=T) # svytable(~p0,design=sd_eh19p) # tablas de contigencias y hacer pruebas sobre estas t1&lt;-svymean(~p0,design=sd_eh19p,na.rm=T) # proporciones / medias t2&lt;-svytotal(~p0,design=sd_eh19p,na.rm=T) # totales clase / total cv(t1) cv(t2) confint(t1) confint(t2) sd2_eh19p&lt;-svydesign(ids = ~1, weights = ~factor,data = eh19p) # pps t3&lt;-svymean(~p0,design=sd2_eh19p,na.rm=T) # proporciones / medias t1 t3 cv(t1)*100 cv(t3)*100 summary(lm(ylab~aestudio,data=eh19p)) # ingreso= B0+B1*aestudio+e OLS summary(lm(ylab~aestudio,data=eh19p,weights = factor )) # Minimos cuadrados ponderados m1&lt;-svyglm(ylab~aestudio,design=sd_eh19p) summary(m1) install.packages(&quot;jtools&quot;) library(jtools) summ(m1) psrsq(m1) # srvyr library(srvyr) sd3_eh19p&lt;-as_survey_design(sd_eh19p) sd_eh19p %&gt;% select(p0) %&gt;% svymean() sd3_eh19p %&gt;% summarise(survey_mean(s02a_03)) sd3_eh19p %&gt;% group_by(depto,area,s02a_02) %&gt;% summarise(m_edad=survey_mean(s02a_03)) sd3_eh19p %&gt;% group_by(depto,area,s02a_02) %&gt;% summarise(m_edad=survey_mean(s02a_03),m_ylab=survey_mean(ylab,na.rm=T)) # p0 sd3_eh19p %&gt;% group_by(depto) %&gt;% survey_tally() sd3_eh19p %&gt;% group_by(depto) %&gt;% survey_count() sd3_eh19p %&gt;% mutate(pobreza=p0==&quot;Pobre&quot;) %&gt;% summarise(p0=survey_mean(pobreza,na.rm=T)) sd3_eh19p %&gt;% mutate(pobreza=p0==&quot;Pobre&quot;) %&gt;% group_by(depto,area) %&gt;% summarise(p0=survey_mean(pobreza,na.rm=T),N=survey_total(pobreza,na.rm=T)) sd4_eh19p&lt;- eh19p %&gt;% as_survey_design(ids=c(upm,folio),strata=estrato,weights=factor) sd4_eh19p&lt;- eh19p %&gt;% as_survey_design(ids=upm,strata=estrato,weights=factor) sd4_eh19p %&gt;% mutate(pobreza=p0==&quot;Pobre&quot;) %&gt;% group_by(depto,area) %&gt;% summarise(p0=survey_mean(pobreza,na.rm=T,vartype = c(&quot;se&quot;, &quot;ci&quot;, &quot;var&quot;, &quot;cv&quot;),deff=T)) sd4_eh19p %&gt;% mutate(pobreza=p0==&quot;Pobre&quot;) %&gt;% summarise(p0=survey_mean(pobreza,na.rm=T,vartype = c(&quot;se&quot;, &quot;ci&quot;, &quot;var&quot;, &quot;cv&quot;),deff=T)) 1.7 Imputación de variables We should be suspicious of any dataset (large or small) which appears perfect. — David J. Hand 1.7.1 La falta de información es información MCAR missing completely at random MAR missing at random MNAR missing not at random 1.7.2 Aproximación formal Sea \\(Y\\) una matriz de datos con \\(n\\) observaciones y \\(p\\) variables. Sea \\(R\\) una matriz de respuesta binaria, tal que si \\(y_{ij}\\) es observada, entonces \\(r_{ij}=1\\). Los valores observados son colectados en \\(Y_{obs}\\), las observaciones perdidas en \\(Y_{mis}\\). Así, \\(Y=(Y_{obs},Y_{mis})\\). La distribución de \\(R\\) depende de \\(Y=(Y_{obs},Y_{mis})\\). Sea \\(\\psi\\) que contiene los parámetros del modelo de los datos perdidos, así la expresión del modelo de los datos perdidos es \\(\\Pr(R|Y_\\mathrm{obs},Y_\\mathrm{mis},\\psi)\\) 1.7.3 MCAR, MAR, MNAR MCAR (missing completely at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) = \\Pr(R=0|\\psi) \\] MAR (missing at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) = \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},\\psi) \\] MNAR (missing not at random ) \\[ \\Pr(R=0|{\\mbox{$Y_\\mathrm{obs}$}},{\\mbox{$Y_\\mathrm{mis}$}},\\psi) \\] 1.7.4 Alternativas para trabajar con los Missings (Ad-hoc) Listwise deletion Pairwise deletion Mean imputation Regression imputation Stochastic regression imputation Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) Indicator method 1.7.5 Imputación Multiple 1.7.6 Patrones en datos multivariados 1.7.7 Influx and outflux \\[ I_j = \\frac{\\sum_j^p\\sum_k^p\\sum_i^n (1-r_{ij})r_{ik}}{\\sum_k^p\\sum_i^n r_{ik}} \\] La variable con mayor influx está mejor conectada a los datos observados y, por lo tanto, podría ser más fácil de imputar. \\[ O_j = \\frac{\\sum_j^p\\sum_k^p\\sum_i^n r_{ij}(1-r_{ik})}{\\sum_k^p\\sum_i^n 1-r_{ij}} \\] La variable con mayor outflux está mejor conectada a los datos faltantes, por lo tanto, es potencialmente más útil para imputar otras variables. 1.7.8 Imputación de datos monótonos 1.7.9 Multivariate Imputation by Chained Equations (mice) (Imputación multivariante por ecuaciones encadenadas) 1.7.10 En R 1.7.10.1 Métodos ad-hoc Listwise deletion (trabajar solo con casos completos) table(is.na(airquality$Ozone)) ## ## FALSE TRUE ## 116 37 R&lt;-(!is.na(airquality))*1 mean(airquality$Ozone) ## [1] NA #listwise x&lt;-na.omit(airquality$Ozone) mean(x) ## [1] 42.12931 bd&lt;-airquality bd2&lt;-na.omit(bd) na.action(x) ## [1] 5 10 25 26 27 32 33 34 35 36 37 39 42 43 45 46 ## [17] 52 53 54 55 56 57 58 59 60 61 65 72 75 83 84 102 ## [33] 103 107 115 119 150 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; na.action(bd2) ## 5 6 10 11 25 26 27 32 33 34 35 36 37 39 42 43 45 ## 5 6 10 11 25 26 27 32 33 34 35 36 37 39 42 43 45 ## 46 52 53 54 55 56 57 58 59 60 61 65 72 75 83 84 96 ## 46 52 53 54 55 56 57 58 59 60 61 65 72 75 83 84 96 ## 97 98 102 103 107 115 119 150 ## 97 98 102 103 107 115 119 150 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; naprint(na.action(x)) ## [1] &quot;37 observations deleted due to missingness&quot; naprint(na.action(bd2)) ## [1] &quot;42 observations deleted due to missingness&quot; table(complete.cases(bd)) ## ## FALSE TRUE ## 42 111 ii&lt;-complete.cases(bd) bd[ii,] ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 ## 9 8 19 20.1 61 5 9 ## 12 16 256 9.7 69 5 12 ## 13 11 290 9.2 66 5 13 ## 14 14 274 10.9 68 5 14 ## 15 18 65 13.2 58 5 15 ## 16 14 334 11.5 64 5 16 ## 17 34 307 12.0 66 5 17 ## 18 6 78 18.4 57 5 18 ## 19 30 322 11.5 68 5 19 ## 20 11 44 9.7 62 5 20 ## 21 1 8 9.7 59 5 21 ## 22 11 320 16.6 73 5 22 ## 23 4 25 9.7 61 5 23 ## 24 32 92 12.0 61 5 24 ## 28 23 13 12.0 67 5 28 ## 29 45 252 14.9 81 5 29 ## 30 115 223 5.7 79 5 30 ## 31 37 279 7.4 76 5 31 ## 38 29 127 9.7 82 6 7 ## 40 71 291 13.8 90 6 9 ## 41 39 323 11.5 87 6 10 ## 44 23 148 8.0 82 6 13 ## 47 21 191 14.9 77 6 16 ## 48 37 284 20.7 72 6 17 ## 49 20 37 9.2 65 6 18 ## 50 12 120 11.5 73 6 19 ## 51 13 137 10.3 76 6 20 ## 62 135 269 4.1 84 7 1 ## 63 49 248 9.2 85 7 2 ## 64 32 236 9.2 81 7 3 ## 66 64 175 4.6 83 7 5 ## 67 40 314 10.9 83 7 6 ## 68 77 276 5.1 88 7 7 ## 69 97 267 6.3 92 7 8 ## 70 97 272 5.7 92 7 9 ## 71 85 175 7.4 89 7 10 ## 73 10 264 14.3 73 7 12 ## 74 27 175 14.9 81 7 13 ## 76 7 48 14.3 80 7 15 ## 77 48 260 6.9 81 7 16 ## 78 35 274 10.3 82 7 17 ## 79 61 285 6.3 84 7 18 ## 80 79 187 5.1 87 7 19 ## 81 63 220 11.5 85 7 20 ## 82 16 7 6.9 74 7 21 ## 85 80 294 8.6 86 7 24 ## 86 108 223 8.0 85 7 25 ## 87 20 81 8.6 82 7 26 ## 88 52 82 12.0 86 7 27 ## 89 82 213 7.4 88 7 28 ## 90 50 275 7.4 86 7 29 ## 91 64 253 7.4 83 7 30 ## 92 59 254 9.2 81 7 31 ## 93 39 83 6.9 81 8 1 ## 94 9 24 13.8 81 8 2 ## 95 16 77 7.4 82 8 3 ## 99 122 255 4.0 89 8 7 ## 100 89 229 10.3 90 8 8 ## 101 110 207 8.0 90 8 9 ## 104 44 192 11.5 86 8 12 ## 105 28 273 11.5 82 8 13 ## 106 65 157 9.7 80 8 14 ## 108 22 71 10.3 77 8 16 ## 109 59 51 6.3 79 8 17 ## 110 23 115 7.4 76 8 18 ## 111 31 244 10.9 78 8 19 ## 112 44 190 10.3 78 8 20 ## 113 21 259 15.5 77 8 21 ## 114 9 36 14.3 72 8 22 ## 116 45 212 9.7 79 8 24 ## 117 168 238 3.4 81 8 25 ## 118 73 215 8.0 86 8 26 ## 120 76 203 9.7 97 8 28 ## 121 118 225 2.3 94 8 29 ## 122 84 237 6.3 96 8 30 ## 123 85 188 6.3 94 8 31 ## 124 96 167 6.9 91 9 1 ## 125 78 197 5.1 92 9 2 ## 126 73 183 2.8 93 9 3 ## 127 91 189 4.6 93 9 4 ## 128 47 95 7.4 87 9 5 ## 129 32 92 15.5 84 9 6 ## 130 20 252 10.9 80 9 7 ## 131 23 220 10.3 78 9 8 ## 132 21 230 10.9 75 9 9 ## 133 24 259 9.7 73 9 10 ## 134 44 236 14.9 81 9 11 ## 135 21 259 15.5 76 9 12 ## 136 28 238 6.3 77 9 13 ## 137 9 24 10.9 71 9 14 ## 138 13 112 11.5 71 9 15 ## 139 46 237 6.9 78 9 16 ## 140 18 224 13.8 67 9 17 ## 141 13 27 10.3 76 9 18 ## 142 24 238 10.3 68 9 19 ## 143 16 201 8.0 82 9 20 ## 144 13 238 12.6 64 9 21 ## 145 23 14 9.2 71 9 22 ## 146 36 139 10.3 81 9 23 ## 147 7 49 10.3 69 9 24 ## 148 14 20 16.6 63 9 25 ## 149 30 193 6.9 70 9 26 ## 151 14 191 14.3 75 9 28 ## 152 18 131 8.0 76 9 29 ## 153 20 223 11.5 68 9 30 Y_obs&lt;-na.omit(bd) Y_mis&lt;-mice::ic(bd) bd$Ozone[bd$Ozone==&quot;**&quot;]&lt;-NA #gsub recode table(R[,1],R[,2]) ## ## 0 1 ## 0 2 35 ## 1 5 111 chisq.test(table(R[,1],R[,2])) ## Warning in chisq.test(table(R[, 1], R[, 2])): Chi-squared approximation ## may be incorrect ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(R[, 1], R[, 2]) ## X-squared = 0.00000000000000000000000000001799, df = 1, ## p-value = 1 apply(R,2,mean) ## Ozone Solar.R Wind Temp Month Day ## 0.7581699 0.9542484 1.0000000 1.0000000 1.0000000 1.0000000 table(R[,1],R[,3]) ## ## 1 ## 0 37 ## 1 116 chisq.test(table(R[,1],R[,3])) ## ## Chi-squared test for given probabilities ## ## data: table(R[, 1], R[, 3]) ## X-squared = 40.791, df = 1, p-value = 0.0000000001694 table(R[,1],R[,4]) ## ## 1 ## 0 37 ## 1 116 chisq.test(table(R[,1],R[,4])) ## ## Chi-squared test for given probabilities ## ## data: table(R[, 1], R[, 4]) ## X-squared = 40.791, df = 1, p-value = 0.0000000001694 R&lt;-as.data.frame(R) m1&lt;-glm(Ozone~Solar.R,data = R,family = &quot;binomial&quot;) # logit summary(m1) ## ## Call: ## glm(formula = Ozone ~ Solar.R, family = &quot;binomial&quot;, data = R) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6901 0.7404 0.7404 0.7404 0.8203 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.9163 0.8367 1.095 0.273 ## Solar.R 0.2379 0.8588 0.277 0.782 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 169.27 on 152 degrees of freedom ## Residual deviance: 169.20 on 151 degrees of freedom ## AIC: 173.2 ## ## Number of Fisher Scoring iterations: 4 summary(lm(Ozone~Solar.R,data=bd2)) ## ## Call: ## lm(formula = Ozone ~ Solar.R, data = bd2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.292 -21.361 -8.864 16.373 119.136 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.59873 6.74790 2.756 0.006856 ** ## Solar.R 0.12717 0.03278 3.880 0.000179 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31.33 on 109 degrees of freedom ## Multiple R-squared: 0.1213, Adjusted R-squared: 0.1133 ## F-statistic: 15.05 on 1 and 109 DF, p-value: 0.0001793 Pairwise deletion (casos parciales) dim(na.omit(airquality[,1:2])) ## [1] 111 2 dim(na.omit(airquality[,c(1,3)])) ## [1] 116 2 dim(na.omit(airquality[,c(2,3)])) ## [1] 146 2 cor(na.omit(airquality[,c(2,3)])) ## Solar.R Wind ## Solar.R 1.00000000 -0.05679167 ## Wind -0.05679167 1.00000000 Mean imputation (MCAR) bd&lt;-airquality m1&lt;-mean(bd$Ozone,na.rm=T) bd$Ozone[is.na(bd$Ozone)]&lt;-m1 mean(bd$Ozone) ## [1] 42.12931 par(mfrow=c(1,2)) hist(airquality$Ozone) hist(bd$Ozone) par(mfrow=c(1,2)) plot(density(airquality$Ozone,na.rm=T)) plot(density(bd$Ozone,na.rm=T)) library(mice) imp &lt;- mice(airquality, method = &quot;mean&quot;, m = 1, maxit = 1) ## ## iter imp variable ## 1 1 Ozone Solar.R bdi&lt;-complete(imp) cor(na.omit(airquality[,1:2])) ## Ozone Solar.R ## Ozone 1.0000000 0.3483417 ## Solar.R 0.3483417 1.0000000 cor(bd[,1:2]) ## Ozone Solar.R ## Ozone 1 NA ## Solar.R NA 1 plot(airquality$Ozone,airquality$Solar.R) plot(bdi$Ozone,bdi$Solar.R) Regression imputation (MAR) \\[y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i\\] \\[E[y/x]=\\beta_0+\\beta_1 x\\] fit &lt;- lm(Ozone ~ Solar.R, data = airquality) summary(fit) ## ## Call: ## lm(formula = Ozone ~ Solar.R, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.292 -21.361 -8.864 16.373 119.136 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.59873 6.74790 2.756 0.006856 ** ## Solar.R 0.12717 0.03278 3.880 0.000179 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31.33 on 109 degrees of freedom ## (42 observations deleted due to missingness) ## Multiple R-squared: 0.1213, Adjusted R-squared: 0.1133 ## F-statistic: 15.05 on 1 and 109 DF, p-value: 0.0001793 pred &lt;- predict(fit, newdata = ic(airquality)) #para el caso 10 18.599+0.127*194 ## [1] 43.237 data &lt;- airquality[, c(&quot;Ozone&quot;, &quot;Solar.R&quot;)] imp &lt;- mice(data, method = &quot;norm.predict&quot;, seed = 1, m = 1, print = FALSE) complete(imp) ## Ozone Solar.R ## 1 41.00000 190.0000 ## 2 36.00000 118.0000 ## 3 12.00000 149.0000 ## 4 18.00000 313.0000 ## 5 42.27981 185.9649 ## 6 28.00000 168.9457 ## 7 23.00000 299.0000 ## 8 19.00000 99.0000 ## 9 8.00000 19.0000 ## 10 43.32279 194.0000 ## 11 7.00000 143.9171 ## 12 16.00000 256.0000 ## 13 11.00000 290.0000 ## 14 14.00000 274.0000 ## 15 18.00000 65.0000 ## 16 14.00000 334.0000 ## 17 34.00000 307.0000 ## 18 6.00000 78.0000 ## 19 30.00000 322.0000 ## 20 11.00000 44.0000 ## 21 1.00000 8.0000 ## 22 11.00000 320.0000 ## 23 4.00000 25.0000 ## 24 32.00000 92.0000 ## 25 26.57468 66.0000 ## 26 52.74360 266.0000 ## 27 42.27706 185.9617 ## 28 23.00000 13.0000 ## 29 45.00000 252.0000 ## 30 115.00000 223.0000 ## 31 37.00000 279.0000 ## 32 55.36049 286.0000 ## 33 55.49133 287.0000 ## 34 49.60333 242.0000 ## 35 42.27603 186.0000 ## 36 46.72475 220.0000 ## 37 52.48191 264.0000 ## 38 29.00000 127.0000 ## 39 53.65951 273.0000 ## 40 71.00000 291.0000 ## 41 39.00000 323.0000 ## 42 51.82769 259.0000 ## 43 50.65008 250.0000 ## 44 23.00000 148.0000 ## 45 61.37934 332.0000 ## 46 60.07089 322.0000 ## 47 21.00000 191.0000 ## 48 37.00000 284.0000 ## 49 20.00000 37.0000 ## 50 12.00000 120.0000 ## 51 13.00000 137.0000 ## 52 37.56563 150.0000 ## 53 25.65877 59.0000 ## 54 29.84580 91.0000 ## 55 50.65008 250.0000 ## 56 35.60296 135.0000 ## 57 34.55620 127.0000 ## 58 24.08864 47.0000 ## 59 30.76171 98.0000 ## 60 21.99512 31.0000 ## 61 35.99549 138.0000 ## 62 135.00000 269.0000 ## 63 49.00000 248.0000 ## 64 32.00000 236.0000 ## 65 31.15424 101.0000 ## 66 64.00000 175.0000 ## 67 40.00000 314.0000 ## 68 77.00000 276.0000 ## 69 97.00000 267.0000 ## 70 97.00000 272.0000 ## 71 85.00000 175.0000 ## 72 36.12634 139.0000 ## 73 10.00000 264.0000 ## 74 27.00000 175.0000 ## 75 56.01471 291.0000 ## 76 7.00000 48.0000 ## 77 48.00000 260.0000 ## 78 35.00000 274.0000 ## 79 61.00000 285.0000 ## 80 79.00000 187.0000 ## 81 63.00000 220.0000 ## 82 16.00000 7.0000 ## 83 51.69684 258.0000 ## 84 56.53809 295.0000 ## 85 80.00000 294.0000 ## 86 108.00000 223.0000 ## 87 20.00000 81.0000 ## 88 52.00000 82.0000 ## 89 82.00000 213.0000 ## 90 50.00000 275.0000 ## 91 64.00000 253.0000 ## 92 59.00000 254.0000 ## 93 39.00000 83.0000 ## 94 9.00000 24.0000 ## 95 16.00000 77.0000 ## 96 78.00000 228.5377 ## 97 35.00000 177.2886 ## 98 66.00000 214.2356 ## 99 122.00000 255.0000 ## 100 89.00000 229.0000 ## 101 110.00000 207.0000 ## 102 46.98644 222.0000 ## 103 35.86465 137.0000 ## 104 44.00000 192.0000 ## 105 28.00000 273.0000 ## 106 65.00000 157.0000 ## 107 26.31299 64.0000 ## 108 22.00000 71.0000 ## 109 59.00000 51.0000 ## 110 23.00000 115.0000 ## 111 31.00000 244.0000 ## 112 44.00000 190.0000 ## 113 21.00000 259.0000 ## 114 9.00000 36.0000 ## 115 51.30431 255.0000 ## 116 45.00000 212.0000 ## 117 168.00000 238.0000 ## 118 73.00000 215.0000 ## 119 37.95816 153.0000 ## 120 76.00000 203.0000 ## 121 118.00000 225.0000 ## 122 84.00000 237.0000 ## 123 85.00000 188.0000 ## 124 96.00000 167.0000 ## 125 78.00000 197.0000 ## 126 73.00000 183.0000 ## 127 91.00000 189.0000 ## 128 47.00000 95.0000 ## 129 32.00000 92.0000 ## 130 20.00000 252.0000 ## 131 23.00000 220.0000 ## 132 21.00000 230.0000 ## 133 24.00000 259.0000 ## 134 44.00000 236.0000 ## 135 21.00000 259.0000 ## 136 28.00000 238.0000 ## 137 9.00000 24.0000 ## 138 13.00000 112.0000 ## 139 46.00000 237.0000 ## 140 18.00000 224.0000 ## 141 13.00000 27.0000 ## 142 24.00000 238.0000 ## 143 16.00000 201.0000 ## 144 13.00000 238.0000 ## 145 23.00000 14.0000 ## 146 36.00000 139.0000 ## 147 7.00000 49.0000 ## 148 14.00000 20.0000 ## 149 30.00000 193.0000 ## 150 36.91140 145.0000 ## 151 14.00000 191.0000 ## 152 18.00000 131.0000 ## 153 20.00000 223.0000 plot(airquality[,1:2]) plot(complete(imp)[,1:2]) cor(na.omit(airquality[,1:2])) ## Ozone Solar.R ## Ozone 1.0000000 0.3483417 ## Solar.R 0.3483417 1.0000000 cor(complete(imp)[,1:2]) ## Ozone Solar.R ## Ozone 1.0000000 0.3948995 ## Solar.R 0.3948995 1.0000000 Stochastic regression imputation (MAR) \\[y_i=\\hat{\\beta_0}+\\hat{\\beta_1} x_i+\\hat{\\epsilon_i}\\] data &lt;- airquality[, c(&quot;Ozone&quot;, &quot;Solar.R&quot;)] imp &lt;- mice(data, method = &quot;norm.nob&quot;, m = 1, maxit = 1, seed = 1, print = FALSE) complete(imp) ## Ozone Solar.R ## 1 41.0000000 190.00000 ## 2 36.0000000 118.00000 ## 3 12.0000000 149.00000 ## 4 18.0000000 313.00000 ## 5 93.1901074 230.44150 ## 6 28.0000000 103.66507 ## 7 23.0000000 299.00000 ## 8 19.0000000 99.00000 ## 9 8.0000000 19.00000 ## 10 21.8114143 194.00000 ## 11 7.0000000 169.74746 ## 12 16.0000000 256.00000 ## 13 11.0000000 290.00000 ## 14 14.0000000 274.00000 ## 15 18.0000000 65.00000 ## 16 14.0000000 334.00000 ## 17 34.0000000 307.00000 ## 18 6.0000000 78.00000 ## 19 30.0000000 322.00000 ## 20 11.0000000 44.00000 ## 21 1.0000000 8.00000 ## 22 11.0000000 320.00000 ## 23 4.0000000 25.00000 ## 24 32.0000000 92.00000 ## 25 -12.5439113 66.00000 ## 26 53.7714213 266.00000 ## 27 47.1516807 72.91707 ## 28 23.0000000 13.00000 ## 29 45.0000000 252.00000 ## 30 115.0000000 223.00000 ## 31 37.0000000 279.00000 ## 32 37.7852286 286.00000 ## 33 41.3376364 287.00000 ## 34 29.0262061 242.00000 ## 35 65.1911154 186.00000 ## 36 82.6871079 220.00000 ## 37 83.1114195 264.00000 ## 38 29.0000000 127.00000 ## 39 39.7307790 273.00000 ## 40 71.0000000 291.00000 ## 41 39.0000000 323.00000 ## 42 90.1979525 259.00000 ## 43 41.5950585 250.00000 ## 44 23.0000000 148.00000 ## 45 115.4565706 332.00000 ## 46 76.7599696 322.00000 ## 47 21.0000000 191.00000 ## 48 37.0000000 284.00000 ## 49 20.0000000 37.00000 ## 50 12.0000000 120.00000 ## 51 13.0000000 137.00000 ## 52 23.8411395 150.00000 ## 53 0.7555855 59.00000 ## 54 -5.7687193 91.00000 ## 55 16.9902402 250.00000 ## 56 -12.7755115 135.00000 ## 57 71.3683543 127.00000 ## 58 51.3526593 47.00000 ## 59 24.4868657 98.00000 ## 60 31.6708004 31.00000 ## 61 24.7428741 138.00000 ## 62 135.0000000 269.00000 ## 63 49.0000000 248.00000 ## 64 32.0000000 236.00000 ## 65 108.3710476 101.00000 ## 66 64.0000000 175.00000 ## 67 40.0000000 314.00000 ## 68 77.0000000 276.00000 ## 69 97.0000000 267.00000 ## 70 97.0000000 272.00000 ## 71 85.0000000 175.00000 ## 72 11.7652825 139.00000 ## 73 10.0000000 264.00000 ## 74 27.0000000 175.00000 ## 75 53.6734104 291.00000 ## 76 7.0000000 48.00000 ## 77 48.0000000 260.00000 ## 78 35.0000000 274.00000 ## 79 61.0000000 285.00000 ## 80 79.0000000 187.00000 ## 81 63.0000000 220.00000 ## 82 16.0000000 7.00000 ## 83 59.1510109 258.00000 ## 84 75.2311817 295.00000 ## 85 80.0000000 294.00000 ## 86 108.0000000 223.00000 ## 87 20.0000000 81.00000 ## 88 52.0000000 82.00000 ## 89 82.0000000 213.00000 ## 90 50.0000000 275.00000 ## 91 64.0000000 253.00000 ## 92 59.0000000 254.00000 ## 93 39.0000000 83.00000 ## 94 9.0000000 24.00000 ## 95 16.0000000 77.00000 ## 96 78.0000000 254.64941 ## 97 35.0000000 199.67614 ## 98 66.0000000 216.97628 ## 99 122.0000000 255.00000 ## 100 89.0000000 229.00000 ## 101 110.0000000 207.00000 ## 102 41.4834780 222.00000 ## 103 -33.1867993 137.00000 ## 104 44.0000000 192.00000 ## 105 28.0000000 273.00000 ## 106 65.0000000 157.00000 ## 107 -12.1337321 64.00000 ## 108 22.0000000 71.00000 ## 109 59.0000000 51.00000 ## 110 23.0000000 115.00000 ## 111 31.0000000 244.00000 ## 112 44.0000000 190.00000 ## 113 21.0000000 259.00000 ## 114 9.0000000 36.00000 ## 115 62.1793726 255.00000 ## 116 45.0000000 212.00000 ## 117 168.0000000 238.00000 ## 118 73.0000000 215.00000 ## 119 38.0347443 153.00000 ## 120 76.0000000 203.00000 ## 121 118.0000000 225.00000 ## 122 84.0000000 237.00000 ## 123 85.0000000 188.00000 ## 124 96.0000000 167.00000 ## 125 78.0000000 197.00000 ## 126 73.0000000 183.00000 ## 127 91.0000000 189.00000 ## 128 47.0000000 95.00000 ## 129 32.0000000 92.00000 ## 130 20.0000000 252.00000 ## 131 23.0000000 220.00000 ## 132 21.0000000 230.00000 ## 133 24.0000000 259.00000 ## 134 44.0000000 236.00000 ## 135 21.0000000 259.00000 ## 136 28.0000000 238.00000 ## 137 9.0000000 24.00000 ## 138 13.0000000 112.00000 ## 139 46.0000000 237.00000 ## 140 18.0000000 224.00000 ## 141 13.0000000 27.00000 ## 142 24.0000000 238.00000 ## 143 16.0000000 201.00000 ## 144 13.0000000 238.00000 ## 145 23.0000000 14.00000 ## 146 36.0000000 139.00000 ## 147 7.0000000 49.00000 ## 148 14.0000000 20.00000 ## 149 30.0000000 193.00000 ## 150 7.9575138 145.00000 ## 151 14.0000000 191.00000 ## 152 18.0000000 131.00000 ## 153 20.0000000 223.00000 plot(airquality[,1:2]) plot(complete(imp)[,1:2]) cor(na.omit(airquality[,1:2])) ## Ozone Solar.R ## Ozone 1.0000000 0.3483417 ## Solar.R 0.3483417 1.0000000 cor(complete(imp)[,1:2]) ## Ozone Solar.R ## Ozone 1.0000000 0.3964026 ## Solar.R 0.3964026 1.0000000 plot(density(airquality$Ozone,na.rm=T)) plot(density(complete(imp)[,1],na.rm=T)) plot(density(airquality$Solar.R,na.rm=T)) plot(density(complete(imp)[,2],na.rm=T)) imp&lt;-mice(airquality, method = &quot;norm.nob&quot;, m = 1, maxit = 1, seed = 1, print = FALSE) complete(imp) ## Ozone Solar.R Wind Temp Month Day ## 1 41.0000000 190.000000 7.4 67 5 1 ## 2 36.0000000 118.000000 8.0 72 5 2 ## 3 12.0000000 149.000000 12.6 74 5 3 ## 4 18.0000000 313.000000 11.5 62 5 4 ## 5 20.5007386 174.406477 14.3 56 5 5 ## 6 28.0000000 140.047720 14.9 66 5 6 ## 7 23.0000000 299.000000 8.6 65 5 7 ## 8 19.0000000 99.000000 13.8 59 5 8 ## 9 8.0000000 19.000000 20.1 61 5 9 ## 10 19.9813337 194.000000 8.6 69 5 10 ## 11 7.0000000 175.896881 6.9 74 5 11 ## 12 16.0000000 256.000000 9.7 69 5 12 ## 13 11.0000000 290.000000 9.2 66 5 13 ## 14 14.0000000 274.000000 10.9 68 5 14 ## 15 18.0000000 65.000000 13.2 58 5 15 ## 16 14.0000000 334.000000 11.5 64 5 16 ## 17 34.0000000 307.000000 12.0 66 5 17 ## 18 6.0000000 78.000000 18.4 57 5 18 ## 19 30.0000000 322.000000 11.5 68 5 19 ## 20 11.0000000 44.000000 9.7 62 5 20 ## 21 1.0000000 8.000000 9.7 59 5 21 ## 22 11.0000000 320.000000 16.6 73 5 22 ## 23 4.0000000 25.000000 9.7 61 5 23 ## 24 32.0000000 92.000000 12.0 61 5 24 ## 25 -43.0343740 66.000000 16.6 57 5 25 ## 26 3.3396801 266.000000 14.9 58 5 26 ## 27 18.3933655 8.744525 8.0 57 5 27 ## 28 23.0000000 13.000000 12.0 67 5 28 ## 29 45.0000000 252.000000 14.9 81 5 29 ## 30 115.0000000 223.000000 5.7 79 5 30 ## 31 37.0000000 279.000000 7.4 76 5 31 ## 32 39.7516986 286.000000 8.6 78 6 1 ## 33 31.3839433 287.000000 9.7 74 6 2 ## 34 -8.6316915 242.000000 16.1 67 6 3 ## 35 71.4038792 186.000000 9.2 84 6 4 ## 36 86.2547074 220.000000 8.6 85 6 5 ## 37 56.3247528 264.000000 14.3 79 6 6 ## 38 29.0000000 127.000000 9.7 82 6 7 ## 39 65.7382192 273.000000 6.9 87 6 8 ## 40 71.0000000 291.000000 13.8 90 6 9 ## 41 39.0000000 323.000000 11.5 87 6 10 ## 42 99.7663357 259.000000 10.9 93 6 11 ## 43 71.0135556 250.000000 9.2 92 6 12 ## 44 23.0000000 148.000000 8.0 82 6 13 ## 45 81.8344779 332.000000 13.8 80 6 14 ## 46 61.6649678 322.000000 11.5 79 6 15 ## 47 21.0000000 191.000000 14.9 77 6 16 ## 48 37.0000000 284.000000 20.7 72 6 17 ## 49 20.0000000 37.000000 9.2 65 6 18 ## 50 12.0000000 120.000000 11.5 73 6 19 ## 51 13.0000000 137.000000 10.3 76 6 20 ## 52 45.2131440 150.000000 6.3 77 6 21 ## 53 45.1192943 59.000000 1.7 76 6 22 ## 54 30.9805688 91.000000 4.6 76 6 23 ## 55 36.6503552 250.000000 6.3 76 6 24 ## 56 12.9901939 135.000000 8.0 75 6 25 ## 57 75.8450620 127.000000 8.0 78 6 26 ## 58 48.2955351 47.000000 10.3 73 6 27 ## 59 38.4690813 98.000000 11.5 80 6 28 ## 60 29.1923696 31.000000 14.9 77 6 29 ## 61 54.6909678 138.000000 8.0 83 6 30 ## 62 135.0000000 269.000000 4.1 84 7 1 ## 63 49.0000000 248.000000 9.2 85 7 2 ## 64 32.0000000 236.000000 9.2 81 7 3 ## 65 94.7972277 101.000000 10.9 84 7 4 ## 66 64.0000000 175.000000 4.6 83 7 5 ## 67 40.0000000 314.000000 10.9 83 7 6 ## 68 77.0000000 276.000000 5.1 88 7 7 ## 69 97.0000000 267.000000 6.3 92 7 8 ## 70 97.0000000 272.000000 5.7 92 7 9 ## 71 85.0000000 175.000000 7.4 89 7 10 ## 72 34.0203036 139.000000 8.6 82 7 11 ## 73 10.0000000 264.000000 14.3 73 7 12 ## 74 27.0000000 175.000000 14.9 81 7 13 ## 75 55.9305086 291.000000 14.9 91 7 14 ## 76 7.0000000 48.000000 14.3 80 7 15 ## 77 48.0000000 260.000000 6.9 81 7 16 ## 78 35.0000000 274.000000 10.3 82 7 17 ## 79 61.0000000 285.000000 6.3 84 7 18 ## 80 79.0000000 187.000000 5.1 87 7 19 ## 81 63.0000000 220.000000 11.5 85 7 20 ## 82 16.0000000 7.000000 6.9 74 7 21 ## 83 60.2221568 258.000000 9.7 81 7 22 ## 84 66.5040195 295.000000 11.5 82 7 23 ## 85 80.0000000 294.000000 8.6 86 7 24 ## 86 108.0000000 223.000000 8.0 85 7 25 ## 87 20.0000000 81.000000 8.6 82 7 26 ## 88 52.0000000 82.000000 12.0 86 7 27 ## 89 82.0000000 213.000000 7.4 88 7 28 ## 90 50.0000000 275.000000 7.4 86 7 29 ## 91 64.0000000 253.000000 7.4 83 7 30 ## 92 59.0000000 254.000000 9.2 81 7 31 ## 93 39.0000000 83.000000 6.9 81 8 1 ## 94 9.0000000 24.000000 13.8 81 8 2 ## 95 16.0000000 77.000000 7.4 82 8 3 ## 96 78.0000000 252.798784 6.9 86 8 4 ## 97 35.0000000 201.411206 7.4 85 8 5 ## 98 66.0000000 204.435959 4.6 87 8 6 ## 99 122.0000000 255.000000 4.0 89 8 7 ## 100 89.0000000 229.000000 10.3 90 8 8 ## 101 110.0000000 207.000000 8.0 90 8 9 ## 102 67.4342874 222.000000 8.6 92 8 10 ## 103 -0.5541087 137.000000 11.5 86 8 11 ## 104 44.0000000 192.000000 11.5 86 8 12 ## 105 28.0000000 273.000000 11.5 82 8 13 ## 106 65.0000000 157.000000 9.7 80 8 14 ## 107 3.6275851 64.000000 11.5 79 8 15 ## 108 22.0000000 71.000000 10.3 77 8 16 ## 109 59.0000000 51.000000 6.3 79 8 17 ## 110 23.0000000 115.000000 7.4 76 8 18 ## 111 31.0000000 244.000000 10.9 78 8 19 ## 112 44.0000000 190.000000 10.3 78 8 20 ## 113 21.0000000 259.000000 15.5 77 8 21 ## 114 9.0000000 36.000000 14.3 72 8 22 ## 115 39.3715767 255.000000 12.6 75 8 23 ## 116 45.0000000 212.000000 9.7 79 8 24 ## 117 168.0000000 238.000000 3.4 81 8 25 ## 118 73.0000000 215.000000 8.0 86 8 26 ## 119 73.3973647 153.000000 5.7 88 8 27 ## 120 76.0000000 203.000000 9.7 97 8 28 ## 121 118.0000000 225.000000 2.3 94 8 29 ## 122 84.0000000 237.000000 6.3 96 8 30 ## 123 85.0000000 188.000000 6.3 94 8 31 ## 124 96.0000000 167.000000 6.9 91 9 1 ## 125 78.0000000 197.000000 5.1 92 9 2 ## 126 73.0000000 183.000000 2.8 93 9 3 ## 127 91.0000000 189.000000 4.6 93 9 4 ## 128 47.0000000 95.000000 7.4 87 9 5 ## 129 32.0000000 92.000000 15.5 84 9 6 ## 130 20.0000000 252.000000 10.9 80 9 7 ## 131 23.0000000 220.000000 10.3 78 9 8 ## 132 21.0000000 230.000000 10.9 75 9 9 ## 133 24.0000000 259.000000 9.7 73 9 10 ## 134 44.0000000 236.000000 14.9 81 9 11 ## 135 21.0000000 259.000000 15.5 76 9 12 ## 136 28.0000000 238.000000 6.3 77 9 13 ## 137 9.0000000 24.000000 10.9 71 9 14 ## 138 13.0000000 112.000000 11.5 71 9 15 ## 139 46.0000000 237.000000 6.9 78 9 16 ## 140 18.0000000 224.000000 13.8 67 9 17 ## 141 13.0000000 27.000000 10.3 76 9 18 ## 142 24.0000000 238.000000 10.3 68 9 19 ## 143 16.0000000 201.000000 8.0 82 9 20 ## 144 13.0000000 238.000000 12.6 64 9 21 ## 145 23.0000000 14.000000 9.2 71 9 22 ## 146 36.0000000 139.000000 10.3 81 9 23 ## 147 7.0000000 49.000000 10.3 69 9 24 ## 148 14.0000000 20.000000 16.6 63 9 25 ## 149 30.0000000 193.000000 6.9 70 9 26 ## 150 6.2345801 145.000000 13.2 77 9 27 ## 151 14.0000000 191.000000 14.3 75 9 28 ## 152 18.0000000 131.000000 8.0 76 9 29 ## 153 20.0000000 223.000000 11.5 68 9 30 md.pattern(airquality, plot = T) ## Wind Temp Month Day Solar.R Ozone ## 111 1 1 1 1 1 1 0 ## 35 1 1 1 1 1 0 1 ## 5 1 1 1 1 0 1 1 ## 2 1 1 1 1 0 0 2 ## 0 0 0 0 7 37 44 flux(airquality)[,1:3] ## pobs influx outflux ## Ozone 0.7581699 0.20938215 0.1136364 ## Solar.R 0.9542484 0.03775744 0.7954545 ## Wind 1.0000000 0.00000000 1.0000000 ## Temp 1.0000000 0.00000000 1.0000000 ## Month 1.0000000 0.00000000 1.0000000 ## Day 1.0000000 0.00000000 1.0000000 "],
["modelado-en-minería-de-datos.html", "2 Modelado en Minería de datos 2.1 Explorando los datos 2.2 Componentes Principales 2.3 Análisis de correspondencia", " 2 Modelado en Minería de datos 2.1 Explorando los datos Existen dos aproximaciones para empezar a explorar la información existente en una base de datos: Resumen de los datos Visualización de los datos 2.1.1 Resumen de los datos Dado el tamaño de las bases de datos resulta imposible o muy difícil conocer todas sus propiedad, el resumen de los datos intenta brindar propiedades claves de los datos, estas propiedades podrían ser: Cual es el valor más común Cuan variable o dispersa esta la información Existen valores extraños o inesperados en la base de datos Los datos siguen alguna distribución A continuación se emplea la encuesta a hogares 2019 para ir respondiendo estas preguntas. En cuanto a los valores mas comunes, la media y la mediana de los datos son suficientes para las variables cuantitativas, mientra que para variables cualitativas, las categorías mas frecuentes son una buena opción. load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData&quot;)) # media de edad mean(eh19p$s02a_03) ## [1] 29.69211 #mediana de edad median(eh19p$s02a_03) ## [1] 26 # para las categorías más frecuentes library(DMwR2) library(dplyr) #para la edad centralValue(eh19p$s02a_03) ## [1] 26 #para el sexo centralValue(eh19p$s02a_02) ## [1] &quot;2.Mujer&quot; #para el departamento centralValue(eh19p$depto) ## [1] &quot;La Paz&quot; A veces es mejor ver los resultados por grupos, por ejemplo, podemos verlo por departamento y área. eh19p %&gt;% group_by(depto,area,p0) %&gt;% summarise(media_edad=mean(s02a_03),mediana_edad=median(s02a_03),centralValue(s02a_02),n()) ## `summarise()` regrouping output by &#39;depto&#39;, &#39;area&#39; (override with `.groups` argument) ## # A tibble: 43 x 7 ## # Groups: depto, area [18] ## depto area p0 media_edad mediana_edad `centralValue(s0~ `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 Chuqui~ Urbana No p~ 32.5 28 2.Mujer 987 ## 2 Chuqui~ Urbana Pobre 24.6 20 2.Mujer 832 ## 3 Chuqui~ Urbana &lt;NA&gt; 14 14 2.Mujer 1 ## 4 Chuqui~ Rural No p~ 35.6 35.5 1.Hombre 436 ## 5 Chuqui~ Rural Pobre 32.0 26 2.Mujer 876 ## 6 Chuqui~ Rural &lt;NA&gt; 51 51 2.Mujer 1 ## 7 La Paz Urbana No p~ 33.9 30.5 2.Mujer 5356 ## 8 La Paz Urbana Pobre 25.7 21 2.Mujer 3246 ## 9 La Paz Urbana &lt;NA&gt; 28.9 29.5 2.Mujer 14 ## 10 La Paz Rural No p~ 35.1 30.5 1.Hombre 480 ## # ... with 33 more rows La función n() realiza un proceso de conteo. De forma similar podemos hacer para las medidas de variabilidad, las mas comunes la desviación estándar, el rango, el rango intercuartil y los cuantiles. Usando la eh19 para la edad y el ingreso laboral. ##EDAD # Desviacion estandar sd(eh19p$s02a_03) ## [1] 21.05689 # Rango range(eh19p$s02a_03) ## [1] 0 98 # Rango intercuartil IQR(eh19p$s02a_03) ## [1] 32 # Quantiles quantile(eh19p$s02a_03) ## 0% 25% 50% 75% 100% ## 0 12 26 44 98 quantile(eh19p$s02a_03,c(0.10,0.90)) ## 10% 90% ## 5 61 ##INGRESO LABORAL # Desviacion estandar sd(eh19p$ylab,na.rm = T) ## [1] 2470.079 # Rango range(eh19p$ylab,na.rm = T) ## [1] 10.00 32916.67 # Rango intercuartil IQR(eh19p$ylab,na.rm = T) ## [1] 2583.5 # Quantiles quantile(eh19p$ylab,na.rm = T) ## 0% 25% 50% 75% 100% ## 10.00 1416.50 2598.00 4000.00 32916.67 quantile(eh19p$ylab,probs=c(0.10,0.90),na.rm = T) ## 10% 90% ## 649.2667 5984.3335 ##por departamento y area tapply(eh19p$ylab,list(eh19p$depto,eh19p$area),sd,na.rm=T)#opcion1 ## Urbana Rural ## Chuquisaca 2774.932 2088.921 ## La Paz 2246.716 1848.078 ## Cochabamba 2403.059 1684.063 ## Oruro 2539.094 1970.399 ## Potosí 2227.650 2880.285 ## Tarija 2515.370 1893.959 ## Santa Cruz 2702.120 2076.687 ## Beni 2735.839 1639.508 ## Pando 2661.182 2043.595 eh19p %&gt;% group_by(depto,area) %&gt;% summarise(sd(ylab,na.rm=T),n()) ## `summarise()` regrouping output by &#39;depto&#39; (override with `.groups` argument) ## # A tibble: 18 x 4 ## # Groups: depto [9] ## depto area `sd(ylab, na.rm = T)` `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Chuquisaca Urbana 2775. 1820 ## 2 Chuquisaca Rural 2089. 1313 ## 3 La Paz Urbana 2247. 8616 ## 4 La Paz Rural 1848. 1097 ## 5 Cochabamba Urbana 2403. 6251 ## 6 Cochabamba Rural 1684. 1186 ## 7 Oruro Urbana 2539. 1804 ## 8 Oruro Rural 1970. 967 ## 9 Potosí Urbana 2228. 1208 ## 10 Potosí Rural 2880. 1159 ## 11 Tarija Urbana 2515. 2313 ## 12 Tarija Rural 1894. 844 ## 13 Santa Cruz Urbana 2702. 6043 ## 14 Santa Cruz Rural 2077. 838 ## 15 Beni Urbana 2736. 1634 ## 16 Beni Rural 1640. 609 ## 17 Pando Urbana 2661. 1059 ## 18 Pando Rural 2044. 844 t1&lt;-eh19p %&gt;% group_by(depto,area) %&gt;% summarise(quantile(ylab,na.rm=T)) ## `summarise()` regrouping output by &#39;depto&#39;, &#39;area&#39; (override with `.groups` argument) View(t1) tapply(eh19p$ylab,list(eh19p$depto,eh19p$area),quantile,na.rm=T)#con problemas ## Urbana Rural ## Chuquisaca Numeric,5 Numeric,5 ## La Paz Numeric,5 Numeric,5 ## Cochabamba Numeric,5 Numeric,5 ## Oruro Numeric,5 Numeric,5 ## Potosí Numeric,5 Numeric,5 ## Tarija Numeric,5 Numeric,5 ## Santa Cruz Numeric,5 Numeric,5 ## Beni Numeric,5 Numeric,5 ## Pando Numeric,5 Numeric,5 aggregate(eh19p$ylab,list(depto=eh19p$depto,area=eh19p$area),quantile,na.rm=T) ## depto area x.0% x.25% x.50% x.75% ## 1 Chuquisaca Urbana 83.33334 1206.58752 2355.00000 3597.50000 ## 2 La Paz Urbana 10.00000 1739.75000 2793.92493 4156.79980 ## 3 Cochabamba Urbana 96.66667 1796.00000 2800.00000 4141.66675 ## 4 Oruro Urbana 25.00000 1515.50000 2811.76001 4330.00000 ## 5 Potosí Urbana 66.66667 1500.00000 2598.00000 4000.00000 ## 6 Tarija Urbana 41.66667 1732.00000 2736.56006 4333.33350 ## 7 Santa Cruz Urbana 60.00000 2035.09998 3000.00000 4409.60010 ## 8 Beni Urbana 151.55000 2036.87497 2734.00000 4185.83325 ## 9 Pando Urbana 216.50000 2121.27496 3101.66663 4568.69824 ## 10 Chuquisaca Rural 26.66667 333.33334 842.91672 2079.63330 ## 11 La Paz Rural 20.00000 342.16667 1057.08337 2549.61664 ## 12 Cochabamba Rural 11.66667 705.83337 1550.00000 2705.00012 ## 13 Oruro Rural 33.33334 389.69998 1034.59998 2537.50000 ## 14 Potosí Rural 25.00000 625.00000 993.75000 2691.66669 ## 15 Tarija Rural 41.66667 722.79167 1630.63000 3031.00000 ## 16 Santa Cruz Rural 45.00000 1213.70834 2381.50000 3809.99994 ## 17 Beni Rural 133.33334 1500.00000 2416.13989 3199.66663 ## 18 Pando Rural 150.00000 1500.00000 2592.75000 4100.00000 ## x.100% ## 1 32916.66797 ## 2 23916.66602 ## 3 30000.00000 ## 4 30000.00000 ## 5 15900.00000 ## 6 20000.00000 ## 7 24500.00000 ## 8 22389.00000 ## 9 20000.00000 ## 10 14750.00000 ## 11 14433.43262 ## 12 12549.16699 ## 13 14283.33301 ## 14 24216.66602 ## 15 11150.58301 ## 16 12561.33008 ## 17 8000.00000 ## 18 12340.50000 Finalmente, para explorar a fondo las variables la función describe es bastante útil, también, el comando summary. #analizando las 5 primeras variables de la base de datos library(Hmisc) describe(eh19p[,1:10]) ## eh19p[, 1:10] ## ## 10 Variables 39605 Observations ## ----------------------------------------------------------------------- ## folio ## n missing distinct ## 39605 0 11869 ## ## lowest : 111-00416110273-A-0021 111-00416110273-A-0031 111-00416110273-A-0051 111-00416110273-A-0071 111-00416110273-A-0091 ## highest: 953-12108090472-A-0021 953-12108090472-A-0031 953-12108090472-A-0051 953-12108090472-A-0061 953-12108090472-A-0071 ## ----------------------------------------------------------------------- ## depto ## n missing distinct ## 39605 0 9 ## ## lowest : Chuquisaca La Paz Cochabamba Oruro Potosí ## highest: Potosí Tarija Santa Cruz Beni Pando ## ## Value Chuquisaca La Paz Cochabamba Oruro Potosí ## Frequency 3133 9713 7437 2771 2367 ## Proportion 0.079 0.245 0.188 0.070 0.060 ## ## Value Tarija Santa Cruz Beni Pando ## Frequency 3157 6881 2243 1903 ## Proportion 0.080 0.174 0.057 0.048 ## ----------------------------------------------------------------------- ## area ## n missing distinct ## 39605 0 2 ## ## Value Urbana Rural ## Frequency 30748 8857 ## Proportion 0.776 0.224 ## ----------------------------------------------------------------------- ## nro ## n missing distinct Info Mean Gmd .05 ## 39605 0 15 0.948 2.636 1.709 1 ## .10 .25 .50 .75 .90 .95 ## 1 1 2 4 5 6 ## ## lowest : 1 2 3 4 5, highest: 11 12 13 14 15 ## ## Value 1 2 3 4 5 6 7 8 9 10 ## Frequency 11869 9879 7555 5211 2847 1307 551 232 98 31 ## Proportion 0.300 0.249 0.191 0.132 0.072 0.033 0.014 0.006 0.002 0.001 ## ## Value 11 12 13 14 15 ## Frequency 15 5 3 1 1 ## Proportion 0.000 0.000 0.000 0.000 0.000 ## ----------------------------------------------------------------------- ## s02a_02 ## n missing distinct ## 39605 0 2 ## ## Value 1.Hombre 2.Mujer ## Frequency 19295 20310 ## Proportion 0.487 0.513 ## ----------------------------------------------------------------------- ## s02a_03 ## n missing distinct Info Mean Gmd .05 ## 39605 0 99 1 29.69 23.76 2 ## .10 .25 .50 .75 .90 .95 ## 5 12 26 44 61 69 ## ## lowest : 0 1 2 3 4, highest: 94 95 96 97 98 ## ----------------------------------------------------------------------- ## s02a_04a ## n missing distinct Info Mean Gmd .05 ## 39605 0 31 0.999 15.36 10.1 2 ## .10 .25 .50 .75 .90 .95 ## 3 8 15 23 28 29 ## ## lowest : 1 2 3 4 5, highest: 27 28 29 30 31 ## ----------------------------------------------------------------------- ## s02a_04b ## n missing distinct Info Mean Gmd .05 ## 39605 0 12 0.993 6.568 3.919 1 ## .10 .25 .50 .75 .90 .95 ## 2 4 7 10 11 12 ## ## lowest : 1 2 3 4 5, highest: 8 9 10 11 12 ## ## Value 1 2 3 4 5 6 7 8 9 10 ## Frequency 3190 2936 3258 3259 3489 3255 3337 3564 3348 3491 ## Proportion 0.081 0.074 0.082 0.082 0.088 0.082 0.084 0.090 0.085 0.088 ## ## Value 11 12 ## Frequency 3364 3114 ## Proportion 0.085 0.079 ## ----------------------------------------------------------------------- ## s02a_04c ## n missing distinct Info Mean Gmd .05 ## 39605 0 100 1 1989 23.77 1950 ## .10 .25 .50 .75 .90 .95 ## 1958 1974 1993 2007 2014 2016 ## ## lowest : 1920 1921 1922 1923 1924, highest: 2015 2016 2017 2018 2019 ## ----------------------------------------------------------------------- ## s02a_05 ## n missing distinct ## 39605 0 13 ## ## lowest : 1.JEFE O JEFA DEL HOGAR 2.ESPOSA/O O CONVIVIENTE 3.HIJO/A O ENTENADO/A 4.HIJO/A ADOPTADO/A 5.YERNO O NUERA ## highest: 9.NIETO/NIETA 10.OTRO PARIENTE 11.OTRO QUE NO ES PARIENTE 12.EMPLEADA/O DEL HOGAR CAMA ADENTRO 13.PARIENTE DE LA EMPLEADA/O DEL HOGAR ## ----------------------------------------------------------------------- summary(eh19p$ylab) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 10 1416 2598 3075 4000 32917 23816 by(eh19p[,1:10],eh19p$area,summary) ## eh19p$area: Urbana ## folio depto area nro ## Length:30748 La Paz :8616 Urbana:30748 Min. : 1.000 ## Class :character Cochabamba:6251 Rural : 0 1st Qu.: 1.000 ## Mode :character Santa Cruz:6043 Median : 2.000 ## Tarija :2313 Mean : 2.626 ## Chuquisaca:1820 3rd Qu.: 4.000 ## Oruro :1804 Max. :15.000 ## (Other) :3901 ## s02a_02 s02a_03 s02a_04a s02a_04b ## 1.Hombre:14893 Min. : 0.00 Min. : 1.00 Min. : 1.000 ## 2.Mujer :15855 1st Qu.:12.00 1st Qu.: 8.00 1st Qu.: 4.000 ## Median :26.00 Median :15.00 Median : 7.000 ## Mean :29.27 Mean :15.41 Mean : 6.553 ## 3rd Qu.:43.00 3rd Qu.:23.00 3rd Qu.:10.000 ## Max. :98.00 Max. :31.00 Max. :12.000 ## ## s02a_04c s02a_05 ## Min. :1920 3.HIJO/A O ENTENADO/A :13795 ## 1st Qu.:1976 1.JEFE O JEFA DEL HOGAR : 9086 ## Median :1993 2.ESPOSA/O O CONVIVIENTE: 5508 ## Mean :1990 9.NIETO/NIETA : 1055 ## 3rd Qu.:2006 6.HERMANO/A O CUÑADO/A : 379 ## Max. :2019 7.PADRES : 247 ## (Other) : 678 ## ----------------------------------------------------- ## eh19p$area: Rural ## folio depto area nro ## Length:8857 Chuquisaca:1313 Urbana: 0 Min. : 1.000 ## Class :character Cochabamba:1186 Rural :8857 1st Qu.: 1.000 ## Mode :character Potosí :1159 Median : 2.000 ## La Paz :1097 Mean : 2.671 ## Oruro : 967 3rd Qu.: 4.000 ## Tarija : 844 Max. :13.000 ## (Other) :2291 ## s02a_02 s02a_03 s02a_04a s02a_04b ## 1.Hombre:4402 Min. : 0.00 Min. : 1.00 Min. : 1.00 ## 2.Mujer :4455 1st Qu.:11.00 1st Qu.: 8.00 1st Qu.: 4.00 ## Median :27.00 Median :15.00 Median : 7.00 ## Mean :31.16 Mean :15.15 Mean : 6.62 ## 3rd Qu.:50.00 3rd Qu.:23.00 3rd Qu.:10.00 ## Max. :98.00 Max. :31.00 Max. :12.00 ## ## s02a_04c s02a_05 ## Min. :1921 3.HIJO/A O ENTENADO/A :3603 ## 1st Qu.:1969 1.JEFE O JEFA DEL HOGAR :2783 ## Median :1992 2.ESPOSA/O O CONVIVIENTE:1797 ## Mean :1988 9.NIETO/NIETA : 320 ## 3rd Qu.:2008 6.HERMANO/A O CUÑADO/A : 73 ## Max. :2019 7.PADRES : 71 ## (Other) : 210 by(eh19p[,c(&quot;depto&quot;,&quot;ylab&quot;)],eh19p$area,describe) ## eh19p$area: Urbana ## data[x, , drop = FALSE] ## ## 2 Variables 30748 Observations ## ----------------------------------------------------------------------- ## depto ## n missing distinct ## 30748 0 9 ## ## lowest : Chuquisaca La Paz Cochabamba Oruro Potosí ## highest: Potosí Tarija Santa Cruz Beni Pando ## ## Value Chuquisaca La Paz Cochabamba Oruro Potosí ## Frequency 1820 8616 6251 1804 1208 ## Proportion 0.059 0.280 0.203 0.059 0.039 ## ## Value Tarija Santa Cruz Beni Pando ## Frequency 2313 6043 1634 1059 ## Proportion 0.075 0.197 0.053 0.034 ## ----------------------------------------------------------------------- ## ylab ## n missing distinct Info Mean Gmd .05 ## 12417 18331 3796 1 3339 2454 600.0 ## .10 .25 .50 .75 .90 .95 ## 952.6 1792.0 2800.0 4200.1 6200.0 7903.3 ## ## lowest : 10.00000 11.66667 16.66667 25.00000 35.00000 ## highest: 24083.33398 24491.41602 24500.00000 30000.00000 32916.66797 ## ----------------------------------------------------------------------- ## ----------------------------------------------------- ## eh19p$area: Rural ## data[x, , drop = FALSE] ## ## 2 Variables 8857 Observations ## ----------------------------------------------------------------------- ## depto ## n missing distinct ## 8857 0 9 ## ## lowest : Chuquisaca La Paz Cochabamba Oruro Potosí ## highest: Potosí Tarija Santa Cruz Beni Pando ## ## Value Chuquisaca La Paz Cochabamba Oruro Potosí ## Frequency 1313 1097 1186 967 1159 ## Proportion 0.148 0.124 0.134 0.109 0.131 ## ## Value Tarija Santa Cruz Beni Pando ## Frequency 844 838 609 844 ## Proportion 0.095 0.095 0.069 0.095 ## ----------------------------------------------------------------------- ## ylab ## n missing distinct Info Mean Gmd .05 ## 3372 5485 1588 1 2103 2100 142.6 ## .10 .25 .50 .75 .90 .95 ## 233.3 600.0 1499.5 2994.8 4750.0 6158.1 ## ## lowest : 11.66667 20.00000 25.00000 26.66667 30.83333 ## highest: 14433.43262 14750.00000 18599.50781 22545.50000 24216.66602 ## ----------------------------------------------------------------------- eh19p %&gt;% group_by(depto) %&gt;% summarise(n(),mean(s02a_03)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 9 x 3 ## depto `n()` `mean(s02a_03)` ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Chuquisaca 3133 30.7 ## 2 La Paz 9713 30.9 ## 3 Cochabamba 7437 29.8 ## 4 Oruro 2771 32.8 ## 5 Potosí 2367 30.7 ## 6 Tarija 3157 29.2 ## 7 Santa Cruz 6881 28.6 ## 8 Beni 2243 25.9 ## 9 Pando 1903 24.8 2.1.2 Visualización La visualización es una herramienta importante para explorar y entender la base de datos. Los seres humanos son excelentes para capturar patrones visuales, y la visualización de datos intenta capitalizar en estas habilidades. Es útil diferenciar las visualizaciones por: Una sola variables Dos variables Multiples variables Los aspectos vinculados al uso de gráficos de origen de R y la librería ggplot pueden verse en el texto guía de EST-383 “BigData”. A continuación se introducen de forma directa funciones en R orientadas a la visualización univariante, bivariante y multivariante. Usando al EH-2019, para variables cualitativas. #Gráficos de origen de R barplot(table(eh19p$s03a_01a),main=&quot;Dónde vivia hace 5 años?&quot;) #GGPLOT library(ggplot2) ggplot(eh19p,aes(x=s03a_01a))+geom_bar()+ggtitle(&quot;Dónde vivia hace 5 años?&quot;) Para variables del tipo cuantitativas par(mfrow=c(1,2)) boxplot(eh19p$ylab) hist(eh19p$ylab) dev.off() ## null device ## 1 ggplot(eh19p,aes(ylab))+geom_boxplot() ## Warning: Removed 23816 rows containing non-finite values ## (stat_boxplot). ggplot(eh19p,aes(ylab))+geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 23816 rows containing non-finite values (stat_bin). Si ahora queremos comparar usar ambas variables cuanti y cuali boxplot(eh19p$ylab~eh19p$s03a_01a) ggplot(eh19p,aes(x=s03a_01a,y=ylab))+geom_boxplot() ## Warning: Removed 23816 rows containing non-finite values ## (stat_boxplot). ggplot(eh19p,aes(x=s03a_01a,y=ylab))+geom_violin() ## Warning: Removed 23816 rows containing non-finite values ## (stat_ydensity). Usando ambas variables cuantitativas plot(eh19p$tothrs,eh19p$ylab) plot(eh19p[,c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)]) #pairs(eh18p[,c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)]) similar al anterior library(GGally) ggpairs(eh19p,columns = c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;aestudio&quot;)) library(ggridges) ggplot(eh19p %&gt;% filter(s02a_03&gt;=15 &amp; ylab&lt;10000), aes(x = ylab, y = s02a_02, fill = s02a_02)) + geom_density_ridges() + theme_ridges() + theme(legend.position = &quot;none&quot;)+ ggtitle(&quot;Ingreso laboral por sexo, personas de 15 años o más&quot;) ggplot(eh19p %&gt;% filter(s02a_03&gt;=15 &amp; ylab&lt;10000), aes(x = ylab, y = depto, fill = depto)) + geom_density_ridges() + theme_ridges() + theme(legend.position = &quot;none&quot;)+ ggtitle(&quot;Ingreso laboral por departamento, personas de 15 años o más&quot;) ggplot(eh19p %&gt;% filter(s02a_03&gt;=15 &amp; ylab&lt;10000), aes(x = ylab, y = area, fill = area)) + geom_density_ridges() + theme_ridges() + theme(legend.position = &quot;none&quot;)+ ggtitle(&quot;Ingreso laboral por rural, personas de 15 años o más&quot;) Ahora si combinamos variables cuanti y cuali con ggpairs. ggpairs(eh19p,columns = c(&quot;ylab&quot;,&quot;tothrs&quot;,&quot;s03a_01a&quot;,&quot;area&quot;)) Alternativas Multivariantes, #trabajando a partir de una muestra de 100 individuos s&lt;-sample(1:dim(eh19p)[1],100) i&lt;-match(c(&quot;s02a_03&quot;,&quot;aestudio&quot;,&quot;ylab&quot;,&quot;tothrs&quot;),names(eh19p)) ggparcoord(eh19p[s,],columns = i,groupColumn = &quot;area&quot;,boxplot=T) library(&quot;TeachingDemos&quot;) faces(na.omit(eh19p[s,i])) 2.2 Componentes Principales El método de Análisis de Componentes Principales se ocupa de explicar la estructura de varianza y covarianza de un grupo de variables a través de unas pocas combinaciones lineales de este grupo de variables. En general sus objetivos son (1) la reducción de los datos y (2) la interpretación. Algebráicamente, los componentes principales son combinaciones lineales de \\(p\\) variables aleatorias \\(X_1\\), \\(X_2\\), , \\(X_p\\). Geométricamente, estas combinaciones lineales representan la selección de un nuevo sistema de coordenadas obtenido por rotación de del sistema original con \\(X_1\\), \\(X_2\\), , \\(X_p\\) como los ejes de coordenadas. Los nuevos ejes representan la dirección con la máxima variabilidad y provee una simple y más parsimoniosa descripción de la estructura de la covarianza. Los componentes principales dependen únicamente de la matriz de covarianza \\(\\Sigma\\) o la matriz de correlaciones \\(\\rho\\) (Matriz estandarizada de \\(\\Sigma\\)) de \\(X_1\\), \\(X_2\\), , \\(X_p\\). Su desarrollo no requiere de ningún supuesto de normalidad multivariada, sin embargo, componentes principales derivados de poblaciones normales multivariantes tienen un gran uso en la interpretación en términos de elipsoide de densidad constante. Sea la matriz \\(\\mathbf{X}\\) compuesta de \\(p\\) vectores aleatorios \\(\\mathbf{X}=[X_1, X_2, \\ldots, X_p ]\\) que tiene la matriz de covarianza \\(\\Sigma\\) con valores propios \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_p \\geq 0\\). Considere la combinación lineal: \\[\\begin{equation} \\begin{array}{rrr} Y_1 = &amp; a_1^{&#39;} \\mathbf{X} = &amp; a_{11} X_1 + a_{12} X_2 + \\ldots a_{1p} X_p \\\\ Y_2 = &amp; a_2^{&#39;} \\mathbf{X} = &amp; a_{21} X_1 + a_{22} X_2 + \\ldots a_{2p} X_p\\\\ \\vdots = &amp; \\vdots &amp; \\vdots \\\\ Y_p = &amp; a_p^{&#39;} \\mathbf{X} = &amp; a_{p1} X_1 + a_{p2} X_2 + \\ldots a_{pp} X_p\\\\ \\end{array} \\label{cp1} \\end{equation}\\] Equivalente a: \\[\\begin{equation} \\mathbf{Y}= \\left[ \\begin{array}{c} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_p\\\\ \\end{array} \\right] = \\left[ \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1p} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{21} &amp; a_{p2} &amp; \\ldots &amp; a_{pp} \\\\ \\end{array} \\right] \\left[ \\begin{array}{c} X_1\\\\ X_2\\\\ \\vdots\\\\ X_p\\\\ \\end{array} \\right] = \\mathbf{A X} \\label{cp2} \\end{equation}\\] La combinación lineal \\(\\mathbf{Y}=\\mathbf{AX}\\) tiene: \\[\\begin{equation} \\mu_y=E(\\mathbf{Y})=E(\\mathbf{AX})=A \\mu_x \\label{cp3} \\end{equation}\\] \\[\\begin{equation} \\Sigma_y=Cov(\\mathbf{Y})=Cov(\\mathbf{AX})=A \\Sigma A^{&#39;} \\label{cp4} \\end{equation}\\] En base a , se obtiene: \\[\\begin{equation} Var(Y_i)=a_i^{&#39;} \\Sigma a_i \\quad i=1, 2, \\ldots, p \\label{cp5} \\end{equation}\\] \\[\\begin{equation} Cov(Y_i,Y_k)=a_i^{&#39;} \\Sigma a_k \\quad i,k=1, 2, \\ldots, p \\label{cp6} \\end{equation}\\] Los componentes principales son combinaciones lineales incorrelacionadas, tal que es lo más grande posible. El primer componente principal es la combinación lineal con máxima varianza. Entonces se debe maximizar \\(Var(Y_1)=a_1^{&#39;} \\Sigma a_1\\). Es claro que \\(Var(Y_1)\\) puede ser incrementada multiplicando a \\(a_1\\) por alguna constante. Para eliminar esta indeterminación, es conveniente restringir los coeficientes del vector. Por lo tanto se define. \\[ \\begin{array}{rcl} \\text{Primer componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_1^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_1^{&#39;}X) \\quad \\text{sujeto a} \\quad a_1^{&#39;} a_1=1\\\\ \\text{Segundo componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_2^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_2^{&#39;}X) \\quad \\text{sujeto a} \\quad a_2^{&#39;} a_2=1 \\quad y \\\\ &amp; &amp; Cov(a_1^{&#39;}X,a_2^{&#39;}X)=0 \\end{array} \\] Para el \\(i-esimo\\) paso: \\[ \\begin{array}{rcl} i-esimo \\text{ componente principal} &amp; = &amp; \\text{Combinacion lineal} \\quad a_i^{&#39;}X \\quad \\text{que maximiza} \\\\ &amp; &amp; Var(a_i^{&#39;}X) \\quad \\text{sujeto a} \\quad a_i^{&#39;} a_i=1 \\quad y \\\\ &amp; &amp; Cov(a_i^{&#39;}X,a_k^{&#39;}X)=0 \\quad para \\quad k&lt;i \\end{array} \\] \\[\\Sigma_{(pxp)}= A_{(p*p)} \\Lambda_{(p*p)} A^t_{(p*p)} \\] El aspecto central de componentes principales es trabajar con menos variables, sea \\(m&lt;p\\), el método de componentes busca a partir de \\(m\\), los siguiente: \\[\\Sigma_{(pxp)} \\approx A_{(p*m)} \\Lambda_{(m*m)} A^t_{(m*p)}\\] Los pasos sugeridos para iniciar el análisis de componentes principales son: Identificar las variables de interés dentro de la matriz de datos, si las variables tienen las mismas escalas se recomienda emplear la matriz de covarianza, si las escalas son diferentes, se recomienda trabajar con la matriz de correlaciones. Obtener los componentes principales, los eigen valores y la matriz de eigen vectores (Opcional) Eliminar las variables redundantes, se sugiere identificar las variables del conjunto de datos correlacionadas con los últimos componentes. Calcular nuevamente los componentes principales excluyendo las variables identificadas en el paso previo Elegir el número de componentes a retener \\(m\\) (scree plot, tamaño de los eigen valores, etc) Analizar los resultados y usar los componentes Usos de componentes principales. Detección de valores atípicos Identificación de posibles factores Los primeros componentes se pueden usar como indicadores Eliminan la multicolinealidad de un modelo de regresión múltiple load(url(&quot;https://github.com/AlvaroLimber/EST-384/blob/master/data/oct20.RData?raw=true&quot;)) names(computo)[18]&lt;-&quot;MAS&quot; bd&lt;-computo %&gt;% filter(País==&quot;Bolivia&quot; &amp; Elección==&quot;Presidente y Vicepresidente&quot;) bdmun&lt;-bd %&gt;% group_by(Departamento,Provincia,Municipio) %&gt;% summarise_at(vars(Inscritos:Nulos),sum) bdmun[,5:13]&lt;-prop.table(as.matrix(bdmun[,5:13]),1)*100 cov(bdmun[,5:13]) ## CC FPV MTS UCS ## CC 229.38080031 -2.09801049 -16.59325886 -0.390870255 ## FPV -2.09801049 0.18638642 0.50970626 0.092315798 ## MTS -16.59325886 0.50970626 8.23441135 0.234687473 ## UCS -0.39087026 0.09231580 0.23468747 0.196770971 ## MAS -249.50296353 1.41367645 10.40910089 -0.402938082 ## 21F 35.49784986 -0.57462867 -5.41710138 -0.182375753 ## PDC -1.86605304 0.49553362 3.48928083 0.409571826 ## MNR 5.64850407 -0.05669375 -0.88593478 -0.006117697 ## PAN-BOL -0.07599806 0.03171437 0.01910823 0.048955719 ## MAS 21F PDC MNR PAN-BOL ## CC -249.5029635 35.4978499 -1.8660530 5.648504075 -0.07599806 ## FPV 1.4136764 -0.5746287 0.4955336 -0.056693755 0.03171437 ## MTS 10.4091009 -5.4171014 3.4892808 -0.885934778 0.01910823 ## UCS -0.4029381 -0.1823758 0.4095718 -0.006117697 0.04895572 ## MAS 333.1052479 -69.5776991 -13.4371126 -11.567664567 -0.43964740 ## 21F -69.5776991 42.7813484 -8.2966775 6.115120503 -0.34583640 ## PDC -13.4371126 -8.2966775 20.2437456 -1.658049785 0.61976111 ## MNR -11.5676646 6.1151205 -1.6580498 2.465499159 -0.05466316 ## PAN-BOL -0.4396474 -0.3458364 0.6197611 -0.054663155 0.19660559 sigma&lt;-cor(bdmun[,5:13]) library(corrplot) corrplot(sigma) #desc. espectral Lambda&lt;-diag(eigen(sigma)$values) A&lt;-eigen(sigma)$vectors A %*% Lambda %*% t(A) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.00000000 -0.32086474 -0.3818008 -0.058179981 -0.90262309 ## [2,] -0.32086474 1.00000000 0.4114306 0.482045890 0.17941228 ## [3,] -0.38180084 0.41143057 1.0000000 0.184371214 0.19874957 ## [4,] -0.05817998 0.48204589 0.1843712 1.000000000 -0.04976993 ## [5,] -0.90262309 0.17941228 0.1987496 -0.049769934 1.00000000 ## [6,] 0.35834082 -0.20349462 -0.2886180 -0.062857825 -0.58284368 ## [7,] -0.02738420 0.25510593 0.2702553 0.205212648 -0.16363258 ## [8,] 0.23752126 -0.08363261 -0.1966224 -0.008783245 -0.40364774 ## [9,] -0.01131686 0.16567275 0.0150178 0.248900051 -0.05432701 ## [,6] [,7] [,8] [,9] ## [1,] 0.35834082 -0.0273842 0.237521258 -0.01131686 ## [2,] -0.20349462 0.2551059 -0.083632612 0.16567275 ## [3,] -0.28861802 0.2702553 -0.196622422 0.01501780 ## [4,] -0.06285783 0.2052126 -0.008783245 0.24890005 ## [5,] -0.58284368 -0.1636326 -0.403647735 -0.05432701 ## [6,] 1.00000000 -0.2819237 0.595422656 -0.11924647 ## [7,] -0.28192367 1.0000000 -0.234692765 0.31065686 ## [8,] 0.59542266 -0.2346928 1.000000000 -0.07851350 ## [9,] -0.11924647 0.3106569 -0.078513499 1.00000000 #round(A %*% t(A),2) #componentes principales #componentes a retener barplot(eigen(sigma)$values) abline(h=1,col=&quot;red&quot;) plot(1:9,eigen(sigma)$values,type=&quot;b&quot;) aux&lt;-round(eigen(sigma)$values/sum(eigen(sigma)$values),4)*100 cumsum(aux) ## [1] 33.12 53.36 67.22 77.29 85.31 90.96 95.75 100.01 100.01 sigma3&lt;-A[,1:3] %*% Lambda[1:3,1:3] %*% t(A[,1:3]) sigma2&lt;-A[,1:2] %*% Lambda[1:2,1:2] %*% t(A[,1:2]) par(mfrow=c(1,3)) colnames(sigma)&lt;-1:9 rownames(sigma)&lt;-1:9 corrplot(sigma) corrplot(sigma3) corrplot(sigma2) dev.off() ## null device ## 1 as.matrix(bdmun[,5:13])%*%A[,1] ## [,1] ## [1,] -5.3879940 ## [2,] 9.8581564 ## [3,] -1.5069018 ## [4,] 9.5957026 ## [5,] -6.0727426 ## [6,] 10.2036950 ## [7,] -4.5777091 ## [8,] 7.2265148 ## [9,] 9.1327336 ## [10,] 3.0186036 ## [11,] 4.9851015 ## [12,] 11.6909217 ## [13,] -12.6842356 ## [14,] -13.6968262 ## [15,] -16.2162176 ## [16,] 12.3414047 ## [17,] 14.4096106 ## [18,] -14.8981304 ## [19,] 13.3536094 ## [20,] -28.0012634 ## [21,] -27.4405125 ## [22,] -21.3330370 ## [23,] -10.6524174 ## [24,] -3.0214342 ## [25,] -15.8435323 ## [26,] -12.3914245 ## [27,] -4.3952446 ## [28,] -12.4443213 ## [29,] -29.9990727 ## [30,] -35.4245622 ## [31,] -29.9140946 ## [32,] -32.5619743 ## [33,] 13.4092851 ## [34,] -18.2720500 ## [35,] -27.2945428 ## [36,] -29.5427858 ## [37,] -27.2671925 ## [38,] -20.3482866 ## [39,] -32.3094742 ## [40,] -14.7264115 ## [41,] -31.2043390 ## [42,] -31.8980892 ## [43,] -30.2074258 ## [44,] -29.3667784 ## [45,] -29.3379440 ## [46,] -29.4721459 ## [47,] -31.7284278 ## [48,] -29.0107244 ## [49,] -22.5250197 ## [50,] -39.8307602 ## [51,] -41.0969095 ## [52,] -41.6207036 ## [53,] -35.6189208 ## [54,] -38.8706782 ## [55,] -40.3937045 ## [56,] -36.1674086 ## [57,] -27.6935240 ## [58,] -32.6865200 ## [59,] -13.4113036 ## [60,] -24.4487864 ## [61,] -32.6298672 ## [62,] -39.4663503 ## [63,] -36.3782592 ## [64,] -39.9744544 ## [65,] -39.9810357 ## [66,] -39.4998171 ## [67,] -39.8329196 ## [68,] -38.5728304 ## [69,] 1.5685691 ## [70,] -33.8526575 ## [71,] -7.4235962 ## [72,] -40.8601326 ## [73,] -36.1002928 ## [74,] -32.6412171 ## [75,] -37.9093953 ## [76,] -27.7756703 ## [77,] -25.1065115 ## [78,] -35.6165447 ## [79,] -23.8411157 ## [80,] -40.0180967 ## [81,] -30.3381198 ## [82,] -42.4024884 ## [83,] -41.2632414 ## [84,] -18.7457137 ## [85,] -25.5523195 ## [86,] -23.7548518 ## [87,] -32.3757839 ## [88,] -31.9818037 ## [89,] 2.2938813 ## [90,] -6.1658502 ## [91,] -26.1580147 ## [92,] -3.6390996 ## [93,] -15.2341034 ## [94,] -41.4407766 ## [95,] -37.2122458 ## [96,] -38.4407716 ## [97,] -7.0568021 ## [98,] -11.0126507 ## [99,] -37.3246419 ## [100,] -36.2109035 ## [101,] -23.9223557 ## [102,] -34.9009966 ## [103,] -29.8146639 ## [104,] -35.3174108 ## [105,] -34.0978395 ## [106,] -34.8825805 ## [107,] -35.1971706 ## [108,] -35.1678037 ## [109,] -35.5789758 ## [110,] -35.0373872 ## [111,] -32.4570900 ## [112,] -32.4419557 ## [113,] -33.4898312 ## [114,] -26.8277117 ## [115,] -10.1047042 ## [116,] -24.9468945 ## [117,] -38.1303050 ## [118,] -37.6057514 ## [119,] -38.4853227 ## [120,] -37.0010204 ## [121,] -40.2290550 ## [122,] -17.7265777 ## [123,] -37.2184939 ## [124,] -37.9420055 ## [125,] -31.6877403 ## [126,] -36.2836734 ## [127,] -34.4431782 ## [128,] -25.8223735 ## [129,] -24.2148767 ## [130,] -29.8715610 ## [131,] -37.3810976 ## [132,] -35.7426051 ## [133,] -26.5910644 ## [134,] -34.3505089 ## [135,] -32.6145043 ## [136,] -18.2360380 ## [137,] -18.2432574 ## [138,] -30.9911014 ## [139,] -29.1385693 ## [140,] -25.4349664 ## [141,] -24.1915053 ## [142,] -19.0204117 ## [143,] -31.7311148 ## [144,] -38.5147148 ## [145,] -37.0846187 ## [146,] -37.8787157 ## [147,] -37.4807212 ## [148,] -32.8741260 ## [149,] -34.9910638 ## [150,] -33.8188993 ## [151,] -37.3440974 ## [152,] -31.1069810 ## [153,] -25.8212289 ## [154,] -26.2395090 ## [155,] -33.4967587 ## [156,] -33.3820752 ## [157,] -35.3449648 ## [158,] -29.7917915 ## [159,] -17.5489357 ## [160,] -30.4101526 ## [161,] 3.0564052 ## [162,] -39.1509579 ## [163,] -2.1069118 ## [164,] -12.4523420 ## [165,] -22.6541782 ## [166,] -32.3124013 ## [167,] -31.4982931 ## [168,] -27.1146780 ## [169,] -33.5231794 ## [170,] -35.0088178 ## [171,] -36.6951659 ## [172,] -35.5348259 ## [173,] -15.4604170 ## [174,] -36.2120410 ## [175,] -34.3170142 ## [176,] -36.5857828 ## [177,] -34.0517266 ## [178,] -37.6546898 ## [179,] 10.1730707 ## [180,] -7.4603047 ## [181,] -5.4264362 ## [182,] -29.6837047 ## [183,] -16.0444123 ## [184,] -23.1385472 ## [185,] -30.3241663 ## [186,] -36.7275467 ## [187,] -28.3135517 ## [188,] -28.6365149 ## [189,] -32.2535510 ## [190,] -0.9116476 ## [191,] -30.7276427 ## [192,] -36.0823173 ## [193,] -33.0059806 ## [194,] 4.9269802 ## [195,] -0.8641924 ## [196,] -0.4758113 ## [197,] -11.1225902 ## [198,] 0.9303554 ## [199,] -0.1338435 ## [200,] -12.1972536 ## [201,] 9.0320757 ## [202,] -30.9954762 ## [203,] -21.3172641 ## [204,] -19.0047628 ## [205,] -12.8761910 ## [206,] -24.4031095 ## [207,] -24.6021190 ## [208,] -34.1423876 ## [209,] -1.2988197 ## [210,] 8.3946994 ## [211,] -32.1263476 ## [212,] -30.0990131 ## [213,] -38.4424639 ## [214,] -27.1992587 ## [215,] -26.6730862 ## [216,] -36.4671882 ## [217,] -32.6905601 ## [218,] -28.5862038 ## [219,] 4.4185207 ## [220,] -15.2504511 ## [221,] -11.1634786 ## [222,] 0.7305325 ## [223,] -4.3561980 ## [224,] -8.3785171 ## [225,] -10.5448133 ## [226,] -3.2154940 ## [227,] -7.8953066 ## [228,] -10.3254753 ## [229,] 7.8938375 ## [230,] -8.6329288 ## [231,] 5.0356261 ## [232,] 3.2351315 ## [233,] -6.6563079 ## [234,] -37.2416372 ## [235,] -37.5239707 ## [236,] 9.9836077 ## [237,] -21.7393729 ## [238,] -7.1246449 ## [239,] -35.9819133 ## [240,] -36.7826681 ## [241,] -34.6955397 ## [242,] -30.9828833 ## [243,] -35.7856729 ## [244,] -32.9800349 ## [245,] -29.6737813 ## [246,] -19.9581488 ## [247,] -29.7882623 ## [248,] -26.9243743 ## [249,] -27.8424468 ## [250,] -27.3234881 ## [251,] -37.2245672 ## [252,] -39.5340804 ## [253,] -21.6272997 ## [254,] -35.6306536 ## [255,] -25.9575731 ## [256,] -23.5190648 ## [257,] -29.8975877 ## [258,] -30.6559925 ## [259,] -15.5745296 ## [260,] -16.6778772 ## [261,] -31.6594618 ## [262,] -34.7818243 ## [263,] -12.4898857 ## [264,] -17.8690953 ## [265,] -24.3279529 ## [266,] -15.5840870 ## [267,] -38.1694016 ## [268,] -37.4327963 ## [269,] -26.3399328 ## [270,] 20.2781277 ## [271,] -30.9265705 ## [272,] -29.5600596 ## [273,] -8.2777515 ## [274,] 4.5896583 ## [275,] -10.2339725 ## [276,] 1.3836692 ## [277,] 21.4925562 ## [278,] 14.6741470 ## [279,] -0.7844150 ## [280,] 2.8319042 ## [281,] 17.2455682 ## [282,] 13.6897997 ## [283,] -1.0772254 ## [284,] -6.4769526 ## [285,] 12.7192358 ## [286,] -12.4088390 ## [287,] -2.7848645 ## [288,] -22.0559062 ## [289,] -12.9869448 ## [290,] -7.7673430 ## [291,] -5.8168102 ## [292,] 5.8302470 ## [293,] 7.3227940 ## [294,] -0.2483735 ## [295,] 4.5372606 ## [296,] 15.0610187 ## [297,] -0.8787057 ## [298,] -15.7075010 ## [299,] 1.1971226 ## [300,] -3.4708909 ## [301,] -5.8044182 ## [302,] -7.9239877 ## [303,] -24.8868601 ## [304,] -13.8763433 ## [305,] -15.1706755 ## [306,] -8.9847194 ## [307,] -20.9846343 ## [308,] -10.2188924 ## [309,] 7.3083232 ## [310,] -27.3347937 ## [311,] 4.1131126 ## [312,] -16.6478408 ## [313,] -3.2389921 ## [314,] 3.7037605 ## [315,] 3.9028753 ## [316,] -15.5961318 ## [317,] 17.2832730 ## [318,] 25.8263118 ## [319,] -7.3465568 ## [320,] 9.4576133 ## [321,] 20.4977083 ## [322,] -2.8713831 ## [323,] 5.6009689 ## [324,] 14.8737372 ## [325,] 15.0745800 ## [326,] -2.3079776 ## [327,] 12.3451573 ## [328,] 13.3613824 ## [329,] 1.1002287 ## [330,] -5.3290524 ## [331,] -19.6727311 ## [332,] -11.0124148 ## [333,] -33.5659735 ## [334,] 8.1077561 ## [335,] -10.1705556 ## [336,] 1.5149081 ## [337,] -5.5069444 ## [338,] -21.1801154 ## [339,] -2.7832261 ## [340,] -6.5915367 Y&lt;-as.matrix(scale(bdmun[,5:13]))%*%A plot(density(Y[,1])) plot(Y[,1],Y[,2]) #1. Seleccione una base de datos de interés del repositorio load(url(&quot;https://github.com/AlvaroLimber/EST-384/blob/master/data/oct20.RData?raw=true&quot;)) #2. Seleccione las variables para el PCA (Según la motivación) vv&lt;-c(14:22,24,25) #2A TRANFORMAR vval&lt;-apply(computo[,vv],1,sum) aux&lt;-computo[,vv]/vval #2B LIMPIEZA #3. Calcule el PCA aux1&lt;-na.omit(aux) cp1&lt;-eigen(cov(aux1)) cp2&lt;-eigen(cor(aux1)) #4. Identifique el número de CPs que explican hasta el 90% de la varianza cumsum(cp1$values)/sum(cp1$values) ## [1] 0.6437488 0.8650934 0.9435882 0.9722772 0.9839750 0.9903314 ## [7] 0.9952079 0.9977119 0.9991579 1.0000000 1.0000000 cumsum(cp2$values)/sum(cp2$values) ## [1] 0.2060107 0.3791103 0.4915437 0.5920446 0.6774866 0.7601857 ## [7] 0.8319256 0.8991550 0.9562490 1.0000000 1.0000000 #5. Identifique las variables correlacionadas con la cantidad de #componentes fuera del 90% del paso anterior cp11cov&lt;-as.matrix(aux1)%*%cp1$vectors[,11] cp11cor&lt;-as.matrix(aux1)%*%cp2$vectors[,11] cor(cbind(aux1,cp11cov,cp11cor))[1:11,12:13] ## cp11cov cp11cor ## CC 0.53979440 -0.30842297 ## FPV -0.55376059 0.21633850 ## MTS 0.13333915 0.40515623 ## UCS -0.34677314 0.35428285 ## MAS - IPSP -0.21854796 -0.41971569 ## 21F -0.38433159 0.53263368 ## PDC -0.12174043 -0.01216537 ## MNR -0.21628303 0.41542663 ## PAN-BOL 0.11710134 0.09577694 ## Blancos -0.07666553 0.50453525 ## Nulos -0.11039942 0.18712933 cp11&lt;-eigen(cov(aux1[,-2])) cumsum(cp11$values)/sum(cp11$values) ## [1] 0.6443329 0.8658697 0.9444240 0.9730984 0.9847943 0.9911498 ## [7] 0.9960295 0.9984949 0.9999206 1.0000000 cp10cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,10] #6. Calcule nuevamente el componente principal eliminando las variables rebundantes #7. Determine la cantidad de componentes principales a retener cp1cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,1] cp2cov&lt;-as.matrix(aux1[,-2])%*%cp11$vectors[,2] plot(cp1cov,cp2cov) #8. Defina un indicador a partir de estos cor(cbind(aux1,cp1cov,cp2cov))[1:11,12:13] ## cp1cov cp2cov ## CC 0.92113465 0.35759774 ## FPV -0.08074887 -0.10802564 ## MTS -0.16786500 -0.30855895 ## UCS 0.14139049 -0.22216012 ## MAS - IPSP -0.92337643 0.36777758 ## 21F 0.32455320 -0.47209833 ## PDC -0.09883288 0.43937850 ## MNR 0.19561852 -0.25724983 ## PAN-BOL 0.04776390 -0.01901819 ## Blancos -0.17330606 -0.88168972 ## Nulos -0.08874645 0.06391096 bd&lt;-cbind(aux1,cp1cov,cp2cov) names(bd)[5]&lt;-&quot;MAS&quot; #9. Modele un modelo lineal empleando los CPs retenidos. summary(lm(MAS~cp1cov,data=bd)) ## ## Call: ## lm(formula = MAS ~ cp1cov, data = bd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.39544 -0.05881 0.01417 0.06187 0.16168 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3407735 0.0003141 1085.0 &lt;0.0000000000000002 *** ## cp1cov -0.7053686 0.0011240 -627.5 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07868 on 68069 degrees of freedom ## Multiple R-squared: 0.8526, Adjusted R-squared: 0.8526 ## F-statistic: 3.938e+05 on 1 and 68069 DF, p-value: &lt; 0.00000000000000022 2.3 Análisis de correspondencia El análisis de correspondencia esta orientado a encontrar relaciones entre las categorías de variables cualitativas. Esta técnica es un método visual que va más allá del test de independencia Chi-cuadrado. load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/endsa.RData&quot;)) #tarea 10 min, explorar la base de datos library(Hmisc) describe(endsa) names(endsa) aux&lt;-attributes(endsa) names(endsa) #aeXX aux$var.labels[7:15] #nupXX aux$var.labels[23:26] #antXX aux$var.labels[16:20] #vfXX aux$var.labels[50:59] table(endsa$year,endsa$sexo) table(endsa$edad,endsa$sexo) library(dplyr) N&lt;-endsa %&gt;% filter(year==2008) %&gt;% select(ae01,ae08) %&gt;% table() N&lt;-t1[1:4,] chisq.test(N) N n&lt;-sum(N) P&lt;-N/n addmargins(N) Para resumir la teoría, primero divida la matriz de datos \\(I × J\\), denotada por \\(N\\), por su gran total \\(n\\) para obtener la llamada matriz de correspondencia \\(P = N / n\\). Deje que los totales marginales de fila y columna de \\(P\\) sean los vectores \\(r\\) y \\(c\\) respectivamente, es decir, los vectores de masas de fila y columna, y \\(Dr\\) y \\(Dc\\) sean las matrices diagonales de estas matrices. El algoritmo computacional para obtener coordenadas de los perfiles de fila y columna con respecto a los ejes principales, usando el SVD, es el siguiente: Calcular la matriz de residuos estadarizados: \\(S=D_r^{-1/2}(P-rc^t)D_c^{-1/2}\\) Calcular la descomposición SVD de \\(S\\): \\(S=UD_{\\alpha}V^t\\), donde \\(U^T U=V^T V=I\\) Coordenadas principales de filas: \\(F=D_r^{-1/2} U D_{\\alpha}\\) Coordenadas principales de columnas: \\(G=D_c^{-1/2} V D_{\\alpha}\\) Coordenadas estándar de filas: \\(X=D_r^{-1/2} U\\) Coordenadas estándar de columnas: \\(Y=D_c^{-1/2} V\\) Calcular la Inercia: \\[\\phi^2=\\sum_i^I\\sum_j^J{\\frac{(p_{ij}-r_i c_j)^2}{r_i c_j}}\\] Graficar las coordenadas de F y G según la la inercia contenida en la matriz \\(D_{\\alpha}\\) En R, existe la libreria ca que permite acceder a las coordenadas del método de correspondencia. #ejemplo CA #install.packages(&quot;ca&quot;) library(dplyr) library(ca) data(&quot;smoke&quot;) model&lt;-ca(smoke) plot(model) names(model) summary(model) #ejemplo ENDSA load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/endsa.RData&quot;)) ll&lt;-attributes(endsa) ll$var.labels t1&lt;-endsa %&gt;% filter(year==2008) %&gt;% select(7,14) %&gt;% table() t1&lt;-t1[1:4,] #test chi2 chisq.test(t1) model&lt;-ca(t1) model plot(model) #programando el ca lcol&lt;-colnames(t1) lrow&lt;-rownames(t1) P&lt;-prop.table(t1) r&lt;-margin.table(P,1) c&lt;-margin.table(P,2) Dr&lt;-diag(r) Dc&lt;-diag(c) ##Paso 1 P-r%*%t(c) #error en las matriz diagonales Dr^(-0.5)%*%(P-r%*%t(c))%*% Dc^(-0.5) # corrigiendo el problema S&lt;-diag(r^(-0.5))%*%(P-r%*%t(c))%*% diag(c^(-0.5)) # 2 descomposición SVD svd(S) U&lt;-svd(S)$u V&lt;-svd(S)$v Da&lt;-diag(svd(S)$d) #verificando las propiedades U %*% t(U) t(V) %*% V U %*% Da %*% t(V) S # 3 Coordenadas principales filas FF&lt;- diag(r^(-0.5)) %*% U %*% Da # 4 Coordenadas principales columnas G&lt;- diag(c^(-0.5)) %*% V %*% Da # 5 Coordenadas estandar filas X&lt;- diag(r^(-0.5)) %*% U # 6 Coordenadas estandar columnas Y&lt;- diag(c^(-0.5)) %*% V # 7 inercia sum(((P-r%*%t(c))**2)/(r%*%t(c))) #graficando xmin&lt;-min(c(FF[,1],G[,1])) xmax&lt;-max(c(FF[,1],G[,1])) ymin&lt;-min(c(FF[,2],G[,2])) ymax&lt;-max(c(FF[,2],G[,2])) plot(FF[,1],FF[,2],col=&quot;red&quot;,xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1) points(G[,1],G[,2],col=&quot;blue&quot;) abline(h=0,v=0,lty=2) #incluyendo el texto plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = &quot;n&quot;) text(FF[,1],FF[,2],labels = lrow,col=&quot;red&quot;,cex=0.7) text(G[,1],G[,2],labels = lcol,col=&quot;blue&quot;,cex=0.7) abline(h=0,v=0,lty=2) #incluyendo mas información plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = &quot;n&quot;) text(FF[,1],FF[,2],labels = lrow,col=&quot;red&quot;,cex=0.5+r*2) text(G[,1],G[,2],labels = lcol,col=&quot;blue&quot;,cex=0.5+c*2) abline(h=0,v=0,lty=2) #viendo solo una dimensión plot(rep(0,dim(FF)[1]),FF[,1],type = &quot;n&quot;, axes = F) axis(2) text(rep(0,dim(FF)[1]),FF[,1],labels = lrow,col=&quot;red&quot;,cex=0.5+r*2) text(rep(0,dim(G)[1]),G[,1],labels = lcol,col=&quot;blue&quot;,cex=0.5+c*2) abline(h=0,v=0,lty=2) 2.3.1 Un ejemplo de correspondencia simple y múltiple library(FactoMineR) library(explor) endsa08&lt;-endsa %&gt;% filter(year==2008) model1&lt;-MCA(endsa08[,c(&quot;ae01&quot;,&quot;ae05&quot;,&quot;ae08&quot;,&quot;ae09&quot;)]) explor(model1) model1$eig model1$call model2&lt;-PCA(endsa08[,c(&quot;nup02&quot;,&quot;nup03&quot;,&quot;edad&quot;)]) explor(model2) "],
["clustering.html", "3 Clustering 3.1 Medidas de Disimilaridad 3.2 Métodos de clustering 3.3 K-center Clustering (no jerárquicos) 3.4 Cluster Jerárquico 3.5 Ejercicios", " 3 Clustering El clustering es un método cuyo objetivo es el de crear grupos en base a las relaciones multivariantes que existen en los datos, este método es un método previo a las técnicas de clasificación que existen. La base del clustering es la definición de la similaridad entre las filas. Similaridad es definida como una función de distancia entre un par de filas. Es importante distinguir la existencia de grupos naturales dentro de los datos, normalmente estos grupos son características naturales de las observaciones de interés. 3.1 Medidas de Disimilaridad Dado el objetivo del clustering, el aspecto mas importante dentro de estos métodos es utilizar de forma correcta la medida de (di)similaridad entre un para de casos dentro de la base de datos. La definición de las medidas de distancia es crucial para aplicar estos modelos. Funciones de distancia incorrecta pueden generar sesgos en los resultados y ser un problema para etapas posteriores de la mineria de datos. Debemos distinguir las funciones de distancia según la naturaleza de las variables. Sean las filas \\(x\\) e \\(y\\) dentro de una base de datos, estos vectores tienen una dimensión \\(p\\), es decir, se observan \\(p\\) variables para las 2 observaciones. 3.1.1 Distancia Euclideana: Variables numéricas \\[d(x,y)=\\sqrt{\\sum_{i=1}^p{(x_i-y_i)^2}}\\] Donde los \\(x_i\\) y \\(_y_i\\) son los valores para la variable \\(i\\) de las observaciones \\(x\\) e \\(y\\). 3.1.2 Distancia Manhattan: \\(p\\) grande \\[d(x,y)=\\sum_{i=1}^p{|x_i-y_i|}\\] 3.1.3 Distancia Minkowski \\[d(x,y)=\\left(\\sum_{i=1}^p{|x_i-y_i|^d}\\right)^{1/d}\\] x&lt;-rnorm(10) y&lt;-rnorm(10) sqrt(sum((x-y)^2)) #euclideana ## [1] 4.828883 sum(abs(x-y)) #manhattan ## [1] 13.65275 mink&lt;-function(x,y,d){ aux&lt;-sum(abs(x-y)^d)^(1/d) return(aux) } x1&lt;-rnorm(10) y1&lt;-rnorm(10) mink(x=x1,y=y1,d=0.5) ## [1] 85.76253 # se recomienda que d -&gt; 0, p-&gt;grande aux&lt;-matrix(rnorm(100),nrow=5) dist(aux) #euclideana ## 1 2 3 4 ## 2 5.620481 ## 3 4.999371 5.545902 ## 4 5.276834 5.224085 5.739026 ## 5 4.897327 6.035392 5.696445 4.539914 dist(aux, method=&quot;manhattan&quot;) ## 1 2 3 4 ## 2 22.15789 ## 3 17.53638 22.29123 ## 4 20.06987 17.65211 20.32808 ## 5 15.99389 23.53313 18.68370 15.97795 dist(aux, method=&quot;minkowski&quot;, p=3) ## 1 2 3 4 ## 2 3.764412 ## 3 3.512567 3.630056 ## 4 3.541395 3.860047 4.161141 ## 5 3.557810 4.074700 4.166427 3.093807 3.1.4 programando minkowski&lt;-function(x,y,d){ dd&lt;-(sum(abs(x-y)**d))**(1/d) return(dd) } minkowski(c(1,2,3),c(4,2,1),d=2) ## [1] 3.605551 x&lt;-c(1,2,3) y&lt;-c(4,2,1) sum(abs(x-y)) # manhattan ## [1] 5 sqrt(sum(abs(x-y)**2)) # euclideana ## [1] 3.605551 # la función de distancia distancia&lt;-function(bd,d=2){ nf&lt;-dim(bd)[1] DD&lt;-matrix(NA,nf-1,nf-1) colnames(DD)&lt;-1:(nf-1) rownames(DD)&lt;-2:nf for(i in 1:(nf-1)){ for(j in (i+1):nf){ DD[j-1,i]&lt;-minkowski(bd[i,],bd[j,],d) } } return(DD) } distancia(aux,d=2) ## 1 2 3 4 ## 2 5.620481 NA NA NA ## 3 4.999371 5.545902 NA NA ## 4 5.276834 5.224085 5.739026 NA ## 5 4.897327 6.035392 5.696445 4.539914 dist(aux) ## 1 2 3 4 ## 2 5.620481 ## 3 4.999371 5.545902 ## 4 5.276834 5.224085 5.739026 ## 5 4.897327 6.035392 5.696445 4.539914 3.1.5 Variables cualitativas Para las variables cualitativas se debe considerar los casos cuando estas son nominales y ordinales, distinguir tambien los casos de variables binarias. install.packages(&quot;vegan&quot;) library(vegan) 3.1.6 Datos mixtos Una de los mayores desafios es cuando las variables son mixtas, es decir cuantitativas y cualitativas. install.packages(&quot;cluster&quot;) library(cluster) 3.2 Métodos de clustering Partición (k-center) Jerárquicos (dendograma) Basados en densidad Basados en cuadrículas (grid) load(url(&#39;https://raw.githubusercontent.com/AlvaroLimber/EST-383/master/data/oct20.RData&#39;)) aux&lt;-matrix(rnorm(100),20,5) aux1&lt;-cbind(aux,rep(c(1,2,3,4),5)) aux2&lt;-cbind(aux,c(rep(1,5),rep(2,5),rep(3,5),rep(4,5))) aux3&lt;-cbind(aux,sample(1:4,20,replace = T)) vmean&lt;-rbind(colMeans(aux1[aux1[,6]==1,1:5]), colMeans(aux1[aux1[,6]==2,1:5]), colMeans(aux1[aux1[,6]==3,1:5]), colMeans(aux1[aux1[,6]==4,1:5])) colMeans(aux2[aux2[,6]==1,1:5]) ## [1] -0.3141031 0.6941139 0.2416709 -0.5686672 -0.8930634 colMeans(aux2[aux2[,6]==2,1:5]) ## [1] 0.1447913 -0.7786705 0.5137054 -0.4384427 -0.7212081 colMeans(aux2[aux2[,6]==3,1:5]) ## [1] -0.73616555 -0.21000035 -0.02794208 1.18010861 0.56784309 colMeans(aux2[aux2[,6]==4,1:5]) ## [1] 0.9101678 0.2680695 0.1813769 -0.9772059 0.1565090 colMeans(aux3[aux3[,6]==1,1:5]) ## [1] -0.2091440089 -0.0778936759 0.0518331112 -0.0000816898 ## [5] 0.1453905505 colMeans(aux3[aux3[,6]==2,1:5]) ## [1] 0.07980738 -0.28949932 0.15094232 -0.19189242 -0.32074190 colMeans(aux3[aux3[,6]==3,1:5]) ## [1] 0.4066638 0.8438366 1.2227273 -0.7058193 -0.4189945 colMeans(aux3[aux3[,6]==4,1:5]) ## [1] -0.6378747 -0.2564106 -0.9822498 0.2648409 -0.3069473 dist(rbind(aux[1,],vmean)) ## 1 2 3 4 ## 2 2.804768 ## 3 2.441447 1.779359 ## 4 2.418812 1.876594 1.058048 ## 5 2.605279 1.328966 1.491997 1.287586 3.3 K-center Clustering (no jerárquicos) Algoritmo Definición del valor de \\(k\\). Partición de las observaciones en \\(k\\) grupos, obtener el vector de centros de cada grupo (centroides). Se puede trabajar con la media, la mediana o el medoide. Para cada observación calcular las distancia euclidiana (u otra) a los centroides y reasignar la observación en base a la menor distancia, re calcular los centroides en base a la re asignación de cada observación Repetir el paso 2 hasta que que ya no existan más re asignaciones library(dplyr) load(url(&#39;https://raw.githubusercontent.com/AlvaroLimber/EST-383/master/data/oct20.RData&#39;)) # preparación de la base de datos unique(computo$Elección) bd&lt;-computo %&gt;% filter(País==&quot;Bolivia&quot; &amp; Elección==&quot;Presidente y Vicepresidente&quot;)#cobertura #municipios bdmun&lt;-bd %&gt;% group_by(Departamento, Provincia, Municipio) %&gt;% summarise_at(vars(Inscritos:Nulos),sum) #prop.table(as.matrix(bdmun[,5:13]),1) bdmun[,5:13]&lt;-bdmun[,5:13]/apply(bdmun[,5:13],1,sum) #recintos bdrec&lt;-bd %&gt;% group_by(Departamento, Provincia, Municipio,Localidad,Recinto) %&gt;% summarise_at(vars(Inscritos:Nulos),sum) # Modelado: K-center mod1&lt;-kmeans(bdmun[5:13],4) bdmun&lt;-cbind(bdmun,cl1=mod1$cluster) mod2&lt;-kmeans(bdmun[5:13],5) bdmun&lt;-cbind(bdmun,cl2=mod2$cluster) table(bdmun$cl1) table(bdmun$cl2) table(bdmun$Departamento,bdmun$cl2) bdmun %&gt;% filter(cl1==1) bdmun %&gt;% filter(cl1==2) bdmun %&gt;% filter(cl1==3) bdmun %&gt;% filter(cl1==4) mod1$centers mod2$centers library(ggplot2) ggplot(bdmun,aes(x=`MAS - IPSP`,y=CC,size=Inscritos))+geom_point(aes(shape=as.factor(cl2))) ggplot(bdmun,aes(x=`MAS - IPSP`,y=CC, colour=as.factor(cl2),size=Inscritos))+geom_point() ggplot(bdmun,aes(x=`MAS - IPSP`,y=`21F`, colour=as.factor(cl2),size=Inscritos))+geom_point() Hmisc::describe(t(mod2$centers)) scale() Ejemplo: Implementar el algoritmo para el k-center, con la distancia de Minkowski, para la media y mediana. rm(list=ls()) library(dplyr) set.seed(12345) bd&lt;-data.frame(matrix(rnorm(1000),100,10)) kcenter&lt;-function(bd,k=2,distancia=&quot;euclidean&quot;,centro=&quot;media&quot;,semilla=12345){ #0. Definición del valor de $k$. #k=6;distancia=&quot;euclidean&quot;;centro=&quot;media&quot;;semilla=12345 #1. Partición de las observaciones en $k$ grupos, obtener el vector de centros de cada grupo (centroides). Se puede trabajar con la media, la mediana o el medoide. nf&lt;-nrow(bd) nc&lt;-ncol(bd) #bd$k&lt;-rep(seq(1:3),ceiling(nf/k))[1:nf] set.seed(semilla) bd$k&lt;-c(1:k,sample(1:k,nf-k,replace = T)) if(centro==&quot;media&quot;){ centros&lt;-bd %&gt;% group_by(k) %&gt;% summarise_at(vars(1:nc),mean) } else if(centro==&quot;mediana&quot;){ centros&lt;-bd %&gt;% group_by(k) %&gt;% summarise_at(vars(1:nc),median) } #2. Para cada observación calcular las distancia euclidiana (u otra) a los centroides y reasignar la observación en base a la menor distancia, re calcular los centroides en base a la re asignación de cada observación reasig&lt;-1 while(reasig&gt;0){ reasig&lt;-0 for(i in 1:nf){ #i&lt;-30 aux&lt;-as.matrix(dist(rbind(bd[i,1:nc],centros[,-1]),method =distancia ))[-1,1] rk&lt;-as.numeric(which(aux==min(aux))) #re asignación if(rk!=bd$k[i]){ bd$k[i]&lt;-rk reasig&lt;-reasig+1 if(centro==&quot;media&quot;){ centros&lt;-bd %&gt;% group_by(k) %&gt;% summarise_at(vars(1:nc),mean) } else if(centro==&quot;mediana&quot;){ centros&lt;-bd %&gt;% group_by(k) %&gt;% summarise_at(vars(1:nc),median) } } } } #3. Repetir el paso 2 hasta que que ya no existan más re asignaciones return(list(k=bd$k,centros=centros)) } set.seed(12345) bdtest&lt;-data.frame(matrix(rnorm(1000),100,10)) mod1&lt;-kcenter(bdtest,k=5)# media mod2&lt;-kcenter(bdtest,k=5,centro = &quot;mediana&quot;) mod3&lt;-kmeans(bdtest,5) table(mod1$k) ## ## 1 2 3 4 5 ## 15 23 20 24 18 table(mod2$k) ## ## 1 2 3 4 5 ## 19 12 27 16 26 table(mod3$cluster) ## ## 1 2 3 4 5 ## 16 25 14 31 14 #Nota: La entrada de la funcion es un data frame kcenter&lt;-function(bd,k=3,d=2,tipo=&quot;media&quot;,seed=123456){ nf&lt;-dim(bd)[1] nc&lt;-dim(bd)[2] #paso1: asignar las k (nf&gt;=k) set.seed(seed) bd$k&lt;-sample(1:k,nf,replace=T) centroide&lt;-NULL for(i in 1:k){ if(tipo==&quot;media&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, mean)) } else if(tipo==&quot;mediana&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, median)) } } #paso2 (recalcular los centroides al final) cc&lt;-1 while(cc!=0){ #paso3 cc&lt;-0 for(i in 1:nf){ auxd&lt;-NULL for(j in 1:k){ auxd&lt;-c(auxd,minkowski(bd[i,1:nc],centroide[j,1:nc],d=d)) } newk&lt;-which(auxd==min(auxd)) if(newk!=bd$k[i]){ bd$k[i] &lt;- newk cc&lt;-cc+1 } } centroide&lt;-NULL for(i in 1:k){ if(tipo==&quot;media&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, mean)) } else if(tipo==&quot;mediana&quot;){ centroide&lt;-rbind(centroide,apply(bd[bd$k==i,],2, median)) } } } return(bd) } Pensar en un gráfico que permita ver como se asignaron los cluster bdtest$k&lt;-mod1$k head(bdtest) ## X1 X2 X3 X4 X5 X6 ## 1 0.5855288 0.2239254 -1.4361457 0.52228217 0.627965113 -1.4203239 ## 2 0.7094660 -1.1562233 -0.6292596 0.00979376 0.002143951 -2.4669386 ## 3 -0.1093033 0.4224185 0.2435218 -0.44052620 0.284377723 0.4847158 ## 4 -0.4534972 -1.3247553 1.0583622 1.19948953 -1.001779086 -0.9379723 ## 5 0.6058875 0.1410843 0.8313488 -0.11746849 -0.617221929 3.3307333 ## 6 -1.8179560 -0.5360480 0.1052118 0.03820979 0.828194239 -0.1629455 ## X7 X8 X9 X10 k ## 1 -1.6366291 2.30362701 -0.8174921 -0.78486098 1 ## 2 0.2115626 2.02089590 -0.2492659 -2.56005244 1 ## 3 -0.4648317 -0.05787852 0.4629986 0.07280078 4 ## 4 -0.6623572 0.44209338 0.6673264 0.75024358 4 ## 5 -0.1329536 -1.30333114 0.4881699 -0.12824888 5 ## 6 -1.3217017 -0.03522043 1.0764874 -0.48786673 3 #componentes principales 3.3.1 Validación cluster La estructura de los cluster es aleatoria (¿funciona?) ¿Cómo definimos el valor de \\(K\\)? Silhouette coefficient: Se obtiene para la observación \\(i\\) el promedio de distancia a todos los objetos en el mismo cluster (\\(a_i\\)) Se obtiene para la observación \\(i\\) el promedio de distancia a todos los objetos de los otros clusters (\\(b_i\\)) Se define a \\(s_i\\) como el coeficiente, con un recorrido entre \\([-1,1]\\), para cada observación \\(i\\) \\[s_i=\\frac{b_i-a_i}{max(a_i,b_i)}\\] Idealmente se espera que \\(a_i &lt; b_i\\) y los \\(a_i\\) cercanos a \\(0\\). library(cluster) aux&lt;-dist(bdtest[,1:10]) #mod1 s1&lt;-silhouette(mod1$k,aux) #mod2 s2&lt;-silhouette(mod2$k,aux) #mod1 s3&lt;-silhouette(mod3$cluster,aux) ############################## mean(s1[,3]);mean(s2[,3]);mean(s3[,3]) ## [1] 0.06552794 ## [1] 0.07477204 ## [1] 0.09692723 plot(s1) plot(s2) plot(s3) #sobre la base IRIS data(&quot;iris&quot;) aux&lt;-kmeans(iris[,-5],3) s &lt;- silhouette(aux$cluster, dist(iris[,-5])) plot(s) Medoide: es el punto de datos que es “menos diferente” de todos los otros puntos de datos. A diferencia del centroide, el medoide tiene que ser uno de los puntos originales. mod4&lt;-pam(bdtest,k=5) mod4$medoids ## X1 X2 X3 X4 X5 ## [1,] 0.8881394 0.63301734 0.04170917 0.3102550 -1.00572873 ## [2,] -0.1093033 0.42241853 0.24352177 -0.4405262 0.28437772 ## [3,] -0.6443284 0.32215158 -0.15019029 1.0196741 -0.56265483 ## [4,] 1.0431436 -0.04904469 0.16428100 -0.1025805 -0.12739034 ## [5,] 0.8168998 -0.50508981 0.31561282 -0.3157325 -0.01991974 ## X6 X7 X8 X9 X10 k ## [1,] -0.03716272 -0.94568862 0.96937088 -0.4014458 -1.57807666 1 ## [2,] 0.48471584 -0.46483173 -0.05787852 0.4629986 0.07280078 4 ## [3,] 0.72606595 -0.05611997 -0.23970673 -0.8424122 -0.06072724 3 ## [4,] -1.35020157 1.13604965 -1.00523739 0.1198710 -0.88433132 2 ## [5,] 0.76745466 0.80289084 0.99795424 0.6830538 0.01196105 2 mod4$clustering ## [1] 1 1 2 2 2 2 2 3 4 3 2 4 3 5 3 5 2 2 1 2 4 1 3 2 2 4 3 1 5 4 1 1 4 ## [34] 5 2 3 4 3 5 2 3 3 3 2 3 5 2 1 4 3 2 5 5 2 2 2 2 5 1 2 2 2 3 5 2 2 ## [67] 1 5 1 3 2 3 3 4 2 2 2 2 2 2 2 4 2 1 2 5 1 3 5 1 3 2 5 3 3 2 3 1 2 ## [100] 2 aux&lt;-dist(bdtest[,1:10]) s4&lt;-silhouette(mod4$clustering,aux) plot(s4) #iris aux2&lt;-pam(iris[,-5],3) ss &lt;- silhouette(aux2$clustering, dist(iris[,-5])) plot(ss) ¿Cuál es el número óptimo de \\(k\\)? library(fpc)# Flexible Procedures for Clustering sol &lt;- pamk(iris[,-5], krange=2:10, criterion=&quot;asw&quot;, usepam=TRUE) sol$nc ## [1] 2 pamk(bdtest,krange=2:10,usepam = T) ## $pamobject ## Medoids: ## ID X1 X2 X3 X4 X5 ## [1,] 93 1.8869469 0.7867958 -0.1903841 0.1195602 -0.2420959 ## [2,] 3 -0.1093033 0.4224185 0.2435218 -0.4405262 0.2843777 ## X6 X7 X8 X9 X10 k ## [1,] 0.8008613 -0.2500888 0.87061707 0.1199820 0.16194465 2 ## [2,] 0.4847158 -0.4648317 -0.05787852 0.4629986 0.07280078 4 ## Clustering vector: ## [1] 1 1 2 2 2 2 2 2 2 2 2 1 2 1 2 1 2 2 1 2 1 1 2 2 2 1 2 1 2 2 1 1 1 ## [34] 1 2 2 2 2 1 2 2 2 2 2 2 1 2 1 2 2 2 1 1 2 2 2 2 1 1 2 2 2 2 1 2 2 ## [67] 1 1 1 2 2 2 2 1 2 2 2 2 2 2 2 1 2 1 2 1 1 2 1 1 2 2 1 2 2 2 2 1 2 ## [100] 2 ## Objective function: ## build swap ## 3.2616 3.2616 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; &quot;isolation&quot; ## [6] &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; &quot;data&quot; ## ## $nc ## [1] 2 ## ## $crit ## [1] 0.00000000 0.16134389 0.08751267 0.06872608 0.06846076 0.08024272 ## [7] 0.07514533 0.06674595 0.07538168 0.09449311 3.3.2 Distancias para variables nominales (todas nominales) En este caso la mejor estrategia es llevar las variables con sus categorias a variables binarias. Existen múltiples medidas de distancia para variables binarias, muchas de estas medidas son aproximaciones a las medidas mas conocidas. Entre ellas: Sean las filas \\(i\\), \\(j\\) que contienen los valores binarios de las variables de estudio. Sea \\(A\\) el total de \\(1\\) que existe en \\(i\\), \\(B\\) el total de \\(1\\) que existe en \\(j\\) y sea \\(J\\) el total de casos en los que los \\(1\\) ocurren simultaneamente en \\(i\\) y \\(j\\). Euclideana \\[d_{ij}=\\sqrt{A+B-2J}\\] * Manhattan \\[d_{ij}=A+B-2J\\] * Bray \\[d_{ij}=\\frac{A+B-2J}{A+B}\\] Binomial \\[d_{ij}=log(2)(A+B-2J)\\] aux&lt;-rbind(c(0,0,0,0,1,1,1),c(1,0,1,0,0,1,1)) A&lt;-sum(aux[1,]) B&lt;-sum(aux[2,]) J&lt;-sum(apply(aux, 2, sum)==2) #euclideana sqrt(A+B-2*J) ## [1] 1.732051 #manhathan A+B-2*J ## [1] 3 #bray (A+B-2*J)/(A+B) ## [1] 0.4285714 #binomial log(2)*(A+B-2*J) ## [1] 2.079442 library(vegan) vegdist(aux,binary = T) ## 1 ## 2 0.4285714 vegdist(aux,binary = F) ## 1 ## 2 0.4285714 #una base de datos mas grandes set.seed(999) aux1&lt;-matrix(rbinom(200,1,0.4),nrow = 20) vegdist(aux1,method = &quot;binomial&quot;,binary = T) ## 1 2 3 4 5 6 ## 2 3.4657359 ## 3 3.4657359 2.7725887 ## 4 2.7725887 4.8520303 4.8520303 ## 5 4.1588831 3.4657359 4.8520303 2.7725887 ## 6 2.7725887 3.4657359 3.4657359 4.1588831 1.3862944 ## 7 3.4657359 2.7725887 2.7725887 2.0794415 3.4657359 4.8520303 ## 8 2.0794415 1.3862944 2.7725887 4.8520303 3.4657359 2.0794415 ## 9 2.7725887 3.4657359 4.8520303 4.1588831 2.7725887 1.3862944 ## 10 2.7725887 4.8520303 3.4657359 2.7725887 2.7725887 2.7725887 ## 11 3.4657359 2.7725887 4.1588831 3.4657359 4.8520303 4.8520303 ## 12 2.7725887 4.8520303 3.4657359 2.7725887 4.1588831 2.7725887 ## 13 4.1588831 4.8520303 3.4657359 2.7725887 2.7725887 2.7725887 ## 14 4.8520303 4.1588831 4.1588831 3.4657359 3.4657359 3.4657359 ## 15 1.3862944 3.4657359 2.0794415 2.7725887 4.1588831 2.7725887 ## 16 3.4657359 4.1588831 5.5451774 2.0794415 0.6931472 2.0794415 ## 17 3.4657359 2.7725887 4.1588831 3.4657359 3.4657359 3.4657359 ## 18 2.7725887 4.8520303 3.4657359 4.1588831 4.1588831 4.1588831 ## 19 3.4657359 4.1588831 1.3862944 4.8520303 3.4657359 2.0794415 ## 20 4.8520303 2.7725887 4.1588831 3.4657359 3.4657359 3.4657359 ## 7 8 9 10 11 12 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 4.1588831 ## 9 6.2383246 2.0794415 ## 10 3.4657359 3.4657359 2.7725887 ## 11 2.7725887 4.1588831 3.4657359 3.4657359 ## 12 3.4657359 4.8520303 2.7725887 2.7725887 2.0794415 ## 13 4.8520303 4.8520303 2.7725887 2.7725887 3.4657359 2.7725887 ## 14 4.1588831 4.1588831 2.0794415 2.0794415 2.7725887 2.0794415 ## 15 3.4657359 2.0794415 2.7725887 1.3862944 3.4657359 2.7725887 ## 16 4.1588831 4.1588831 2.0794415 2.0794415 4.1588831 3.4657359 ## 17 5.5451774 2.7725887 2.0794415 4.8520303 4.1588831 4.8520303 ## 18 3.4657359 4.8520303 4.1588831 2.7725887 3.4657359 2.7725887 ## 19 4.1588831 2.7725887 3.4657359 2.0794415 4.1588831 3.4657359 ## 20 4.1588831 4.1588831 2.0794415 3.4657359 1.3862944 2.0794415 ## 13 14 15 16 17 18 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 3.4657359 ## 15 2.7725887 3.4657359 ## 16 2.0794415 2.7725887 3.4657359 ## 17 2.0794415 4.1588831 3.4657359 2.7725887 ## 18 4.1588831 3.4657359 4.1588831 3.4657359 4.8520303 ## 19 2.0794415 4.1588831 2.0794415 4.1588831 4.1588831 3.4657359 ## 20 2.0794415 1.3862944 3.4657359 2.7725887 2.7725887 4.8520303 ## 19 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 ## 17 ## 18 ## 19 ## 20 4.1588831 dist(aux1) ## 1 2 3 4 5 6 7 ## 2 2.236068 ## 3 2.236068 2.000000 ## 4 2.000000 2.645751 2.645751 ## 5 2.449490 2.236068 2.645751 2.000000 ## 6 2.000000 2.236068 2.236068 2.449490 1.414214 ## 7 2.236068 2.000000 2.000000 1.732051 2.236068 2.645751 ## 8 1.732051 1.414214 2.000000 2.645751 2.236068 1.732051 2.449490 ## 9 2.000000 2.236068 2.645751 2.449490 2.000000 1.414214 3.000000 ## 10 2.000000 2.645751 2.236068 2.000000 2.000000 2.000000 2.236068 ## 11 2.236068 2.000000 2.449490 2.236068 2.645751 2.645751 2.000000 ## 12 2.000000 2.645751 2.236068 2.000000 2.449490 2.000000 2.236068 ## 13 2.449490 2.645751 2.236068 2.000000 2.000000 2.000000 2.645751 ## 14 2.645751 2.449490 2.449490 2.236068 2.236068 2.236068 2.449490 ## 15 1.414214 2.236068 1.732051 2.000000 2.449490 2.000000 2.236068 ## 16 2.236068 2.449490 2.828427 1.732051 1.000000 1.732051 2.449490 ## 17 2.236068 2.000000 2.449490 2.236068 2.236068 2.236068 2.828427 ## 18 2.000000 2.645751 2.236068 2.449490 2.449490 2.449490 2.236068 ## 19 2.236068 2.449490 1.414214 2.645751 2.236068 1.732051 2.449490 ## 20 2.645751 2.000000 2.449490 2.236068 2.236068 2.236068 2.449490 ## 8 9 10 11 12 13 14 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 1.732051 ## 10 2.236068 2.000000 ## 11 2.449490 2.236068 2.236068 ## 12 2.645751 2.000000 2.000000 1.732051 ## 13 2.645751 2.000000 2.000000 2.236068 2.000000 ## 14 2.449490 1.732051 1.732051 2.000000 1.732051 2.236068 ## 15 1.732051 2.000000 1.414214 2.236068 2.000000 2.000000 2.236068 ## 16 2.449490 1.732051 1.732051 2.449490 2.236068 1.732051 2.000000 ## 17 2.000000 1.732051 2.645751 2.449490 2.645751 1.732051 2.449490 ## 18 2.645751 2.449490 2.000000 2.236068 2.000000 2.449490 2.236068 ## 19 2.000000 2.236068 1.732051 2.449490 2.236068 1.732051 2.449490 ## 20 2.449490 1.732051 2.236068 1.414214 1.732051 1.732051 1.414214 ## 15 16 17 18 19 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 ## 10 ## 11 ## 12 ## 13 ## 14 ## 15 ## 16 2.236068 ## 17 2.236068 2.000000 ## 18 2.449490 2.236068 2.645751 ## 19 1.732051 2.449490 2.449490 2.236068 ## 20 2.236068 2.000000 2.000000 2.645751 2.449490 # creando variables binarias (dummy) library(fastDummies) dummy_cols() crime &lt;- data.frame(city = c(&quot;SF&quot;, &quot;SF&quot;, &quot;NYC&quot;), year = c(1990, 2000, 1990), crime = 1:3) dummy_cols(crime, select_columns = c(&quot;city&quot;, &quot;year&quot;)) 3.3.3 Distancias para variables mixtas (cuantitativas, nominales, ordinales) library(cluster) data(&quot;flower&quot;) str(flower) dd&lt;-daisy(flower,metric = &quot;gower&quot;) summary(dd) Ejercicios Busque funciones en R que permitan calcular los k-center para variables mixtas y medoides Crear una función k-center para variables mixtas y alternativas para incluir el medoide. 3.4 Cluster Jerárquico El objetivo es obtener una jerarquía de posibles soluciones que van desde un solo grupo a \\(n\\) grupos, donde \\(n\\) es el número de observaciones en el conjunto de datos. 3.4.1 Algoritmo (Johnson) Se inicia con \\(n\\) grupos y se genera una matriz de \\(nxn\\) de distancias, \\(D=\\{d_{ik}\\}\\) Buscar en la matriz de distancia los pares de cluster más cercanos entre ellos, “los cluster mas similares”, si definimos los clusters \\(V\\) y \\(U\\), estamos interesados en encontrar \\(d_{UV}\\) Unir los cluster \\(U\\) y \\(V\\), re etiquetar el nuevo cluster como \\(UV\\). Actualizar la matriz de distancias a) remover las filas y columnas correspondientes a \\(U\\) y \\(V\\) b) incluimos las nuevas filas y columnas para el nuevo cluster \\(UV\\). Repetimos el paso 2 y 3 un total de \\(n-1\\) veces. El momento de definir el cluster más cercano, se puede emplear los siguientes enlaces: Single linkage (Enlace simple): Se elige al cluster más cercano, con la regla de que las distincia individual entre las observaciones dentro de los clusters es la más corta Complete linkage (Enlace completo): Se elige el cluster mas cercano, con la regla que las distancias individuales entre las observaciones dentro de los cluster es la más larga Average linkage (Enlace promedio): Se elige el cluster mas cercano, considerando el promedio de las distancias entre los cluster. Nota: Se debe elegir la matriz de distancias acorde a la naturaleza de los datos, se recomienda: Todas Numéricas: Euclideana o Manhatan Todas nominales: Transformación a binarias y usar la distancia binomial Mixtas: Distancia de Gower aux2&lt;-matrix(rbinom(200,1,0.4),nrow = 20) md&lt;-vegdist(aux2,binary = T) mod1&lt;-hclust(md,method = &quot;single&quot;) mod2&lt;-hclust(md,method = &quot;complete&quot;) mod3&lt;-hclust(md,method = &quot;average&quot;) #dendograma plot(mod1,main=&quot;single&quot;) plot(mod2,main=&quot;complete&quot;) plot(mod3,main=&quot;average&quot;) plot(mod1,main=&quot;single&quot;,hang = -0.1,cex=0.8) amod1&lt;-cutree(mod1,5) amod2&lt;-cutree(mod2,5) amod3&lt;-cutree(mod3,5) plot(mod1,main=&quot;single&quot;,hang = -0.1,cex=0.8) rect.hclust(mod1,3) plot(mod2,main=&quot;complete&quot;,hang = -0.1,cex=0.8) rect.hclust(mod2,3) plot(mod3,main=&quot;average&quot;,hang = -0.1,cex=0.8) rect.hclust(mod3,4) plot(silhouette(amod1,md)) plot(silhouette(amod2,md)) plot(silhouette(amod3,md)) d &lt;- dist(scale(iris[,-5]))#euclideana h &lt;- hclust(d) plot(h,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) #elegir la cantidad de grupos clus3 &lt;- cutree(h, 5) plot(h,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(h,k=5) Probando los tres tipos de enlaces para \\(k=3\\) hs &lt;- hclust(d,method = &quot;single&quot;) hc &lt;- hclust(d,method = &quot;complete&quot;) ha &lt;- hclust(d,method = &quot;average&quot;) cs&lt;-cutree(hs,3) cc&lt;-cutree(hc,3) ca&lt;-cutree(ha,3) table(cs,cc) table(cs,ca) table(cc,ca) par(mfrow=c(1,3)) plot(hs,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(hs,k=3) plot(hc,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(hc,k=3) plot(ha,hang=-0.1,labels=iris[[&quot;Species&quot;]],cex=0.5) rect.hclust(ha,k=3) Nota: El dendograma es muy útil para ver las relaciones que existen basadas en las distancias y la creación de las jerarquias, a partir de estos se puede definir un \\(k\\) (de forma visual) Nota: El dendograma pierde su utilidad cuando la cantidad de observaciones es muy alta, Ejemplo, Usar los datos de las elecciones del 20 de octubre, agregar los resultados en términos relativos para los municipios y generar el dendograma para los tres tipos de enlaces, de forma visual sugerir un valor de \\(k\\) para cada tipo de enlace. library(dplyr) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\oct20.RData&quot;) #filtrar los casos aux&lt;-c(&quot;Número departamento&quot;,&quot;Departamento&quot; ,&quot;Número municipio&quot;,&quot;Municipio&quot; ,&quot;CC&quot;,&quot;FPV&quot;,&quot;MTS&quot;,&quot;UCS&quot;,&quot;MAS - IPSP&quot;,&quot;21F&quot;,&quot;PDC&quot;,&quot;MNR&quot;,&quot;PAN-BOL&quot;,&quot;Votos Válidos&quot;,&quot;Blancos&quot;,&quot;Nulos&quot;) names(computo)[1]&lt;-&quot;pais&quot; names(computo)[12]&lt;-&quot;eleccion&quot; bd&lt;-computo %&gt;% filter(pais==&quot;Bolivia&quot; &amp; eleccion==&quot;Presidente y Vicepresidente&quot;) %&gt;% select(aux) names(bd)[1:4]&lt;-c(&quot;idep&quot;,&quot;ddep&quot;,&quot;imun&quot;,&quot;dmun&quot;) bdmun&lt;-aggregate(bd[,5:16],bd[,1:4],sum) bdmun&lt;-bdmun[,-14] bdmun[,5:15]&lt;-prop.table(as.matrix(bdmun[,5:15]),1) #cluster jerarquico d&lt;-dist(bdmun[,5:15]) plot(hclust(d),hang=-0.1,label=bdmun$dmun,cex=0.5) # determinar el mejor k y el mejor enlace mm&lt;-c(&quot;single&quot;, &quot;complete&quot;, &quot;average&quot;) # método k&lt;-2:20 # cantidad de cluster d&lt;-dist(bdmun[,5:15]) # matriz de distancia # matriz de resultados res&lt;-matrix(NA, nrow = 19,ncol=3) colnames(res)&lt;-mm rownames(res)&lt;-k ####################### for(i in k){ for(j in 1:3){ h&lt;-hclust(d,method = mm[j]) c&lt;-cutree(h,i) s&lt;-silhouette(c,d) res[i-1,j]&lt;-median(s[,3]) } } #la mejor opción es k=2 con el método average h&lt;-hclust(d,method = &quot;average&quot;) c&lt;-cutree(h,2) plot(h,hang=-0.1,labels=bdmun$dmun,cex=0.4) rect.hclust(h,k=2) bdmun$cluster&lt;-c group_by(bdmun,cluster) %&gt;% summarise(mean(CC),mean(`MAS - IPSP`)) Algunas alternativas para la visualización son: library(ape) h$labels&lt;-bdmun$dmun plot(as.phylo(h),type=&quot;fan&quot;) library(dendextend) library(circlize) dend &lt;- as.dendrogram(h) # modify the dendrogram to have some colors in the branches and labels dend &lt;- dend %&gt;% color_branches(k=4) %&gt;% color_labels # plot the radial plot png(&quot;dendo.png&quot;,width = 1500,height = 1500) par(mar = rep(0,4)) # circlize_dendrogram(dend, ) circlize_dendrogram(dend,dend_track_height = 0.8) dev.off() 3.5 Ejercicios Pensar en un gráfico que permita ver como se asignaron los cluster Pensar en optimizar el código empleado para el k-center Hacer que la función desarrollada para el k-center retorne también los centroides Utilizando la base de datos de las elecciones del 20 de octubre, crear una base de datos a nivel municipal, aplicar el método k-center con medoides para los resultados a nivel municipal y terminar el \\(k\\) óptimo en un rango de \\(k=2:10\\) (Usar datos relativos). "],
["regresión.html", "4 Regresión 4.1 Regresión lineal 4.2 Probit y Logit", " 4 Regresión \\[y=f(x_1,x_2, \\ldots)\\] \\(y\\) Variable de resultado, dependiente, solo tenemos a una \\(y\\). \\(x_1, x_2, \\ldots\\), variables de control, independientes. A partir de estas variables: ¿Cuál es la relación de \\(x\\) sobre \\(y\\)? Lineal \\[y_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+\\ldots+\\epsilon_i\\] \\[E[y_i]=E[\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i1}+\\hat{\\beta}_2 x_{i2}+\\ldots]\\] \\[\\frac{dy}{d x_1}= \\beta_1\\] Nota: Diferenciar que la regresión busca establecer relaciones basadas en los datos y no asi un proceso causal. Polinomial Etc; No lineal, Conocer la naturaleza de \\(y\\) y las variables \\(x\\) \\(Y\\) es cuanti (real), \\(X\\) mixtas. (Modelos lineales, MCO) \\(Y\\) es cuanti (discreta &gt;= 0), \\(X\\) cuanti. (Poisson) \\(Y\\) es cuali nominal binario, \\(X\\) mixtas. (LOGIT/PROBIT) \\(Y\\) es cuali ordinal, \\(X\\) mixtas. (Logit/probit ordenados) 4.1 Regresión lineal Base datos lista para el modelo (Unidad de investigación) Establecer la relación interés Definir el modelo de interés Optimizar el modelo Validar el modelo Predecir a partir del modelo 4.1.1 Paso 1: Base de datos Dos poblaciones de estudio: Personas de 18 a 35 años ocupadas, EH-2019 (bd1) Personas de 18 años o más, jefes de hogar y ocupados, EH-2018 (bd2) library(dplyr) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\eh19.Rdata&quot;) bd1&lt;-eh19p %&gt;% filter(s02a_03&gt;=18 &amp; s02a_03&lt;=35 &amp; ocupado==&quot;Si&quot;) %&gt;% select(s02a_02,s02a_03,aestudio,tothrs,ylab,ynolab,factor,estrato, upm,area,permanente,cob_op) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\eh18.Rdata&quot;) bd2&lt;-eh18p %&gt;% filter(s02a_03&gt;=18 &amp; s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot; &amp; ocupado==&quot;Si&quot;) %&gt;% select(s02a_02,s02a_03,aestudio,tothrs,ylab,ynolab,factor,estrato, upm,area,permanente,cob_op) 4.1.2 Paso 2: Establecer la relación de interés. \\(Y\\) Ingreso laboral puede ser un buena opción \\(X\\) el resto, pueden ser basadas en un modelo teórico o buscadas a partir de un proceso de minería de datos \\[IngresoLaboral=f(edad,sexo,educación,...)\\] Para definir el vector de covariables \\(X\\), una práctica recomendada es identificar variables desde el unidad de análisis hacia otras unidades agregadas. 4.1.3 Paso 3: Definir el modelo a utilizar OLS, MCO. Modelos lineales # regresión lineal simple y=f(x) m1_1&lt;-lm(ylab~aestudio,data=bd1) m1_2&lt;-lm(ylab~aestudio,data=bd2) m1_1 m1_2 \\[ylab_i=\\beta_0+\\beta_1 educacion_i+\\epsilon_i\\] * Para personas de 18 a 35 \\[E[ylab_i]=1231.7-136.7*educacion_i\\] * Para personas de 18 años o más (jefes de hogar) \\[E[ylab_i]=1241.9-194.7*educacion_i\\] plot(bd1$aestudio,bd1$ylab) abline(m1_1,col=&quot;red&quot;,lwd=3) summary(m1_1) plot(bd2$aestudio,bd2$ylab) abline(m1_2,col=&quot;red&quot;,lwd=3) summary(m1_2) #en los betas coefficients(m1_1) coefficients(m1_2) confint(m1_1,level=0.95) confint(m1_2,level=0.95) #mejorar el modelo 4.1.4 Paso 4: Optimizar el modelo m5&lt;-lm(log(ylab)~.,data=bd[,-c(7,8,9,11)]) summary(m5) m6&lt;-step(m5) summary(m6) #un ejemplo de laboratorio bd2&lt;-as.data.frame(matrix(rnorm(1000),ncol = 8)) names(bd2)[1]&lt;-&quot;y&quot; bd2$x&lt;-bd2$y+runif(125) plot(bd2$x,bd2$y) plot(bd2) p1&lt;-lm(y~.,data=bd2) summary(p1) p2&lt;-step(p1) summary(p2) Nota: Se debe tener en cuenta siempre la fuente de la información summary(m4) summary(lm(ylab~factor(aestudio)+s02a_02+s02a_03,data=bd)) summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03,data=bd)) #estamos suponiendo una relación lineal #las relaciones que encontramos son a nivel de la muestra #para hacer inferencia el mejor camino es la libreria survey summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03,data=bd))#MCO,OLS summary(lm(log(ylab)~factor(aestudio)+s02a_02+s02a_03 ,weights = factor ,data=bd))#MCP 4.1.5 Paso 5: Validar el modelo #Supuestos del modelo ## los errores se distribuyen normal(media=0, varianza=constante) ## Independencia entre los X del modelo # los errores se distribuyen normal model&lt;-lm(log(ylab)~s02a_03+aestudio+tothrs+ynolab+s02a_02+area+cob_op,data=bd) summary(model) par(mfrow=c(2,2)) plot(model) dev.off() #errores ee&lt;-residuals(model) plot(density(ee)) #prueba de normalidad #H0 x ~ normal() library(normtest) library(nortest) ad.test(ee) lillie.test(ee) boxplot(bd$ylab) #La normalidad de los errores plot(density(ee)) curve(dnorm(x,mean(ee),sd(ee)),add=T,col=&quot;red&quot;) #limitar los datos hasta el percentil punto&lt;-c(0.01,0.90) puntonl&lt;-c(0.99) z&lt;-quantile(bd$ylab,punto,na.rm=T) znl&lt;-quantile(bd$ynolab,puntonl,na.rm=T) znl bd2&lt;-bd %&gt;% filter((ylab&lt;z[2] &amp; ylab&gt;z[1])) boxplot(bd2$ynolab) #definiendo el modelo sin atípicos model1&lt;-lm(log(ylab)~s02a_03+aestudio+tothrs+s02a_02+area,data=bd2) summary(model1) boxplot(bd2$ylab) ee1&lt;-residuals(model1) ad.test(ee1) lillie.test(ee1) ks.test(ee1,&quot;pnorm&quot;,mean(ee1),sd(ee1))#kolmogorov Smirnofv plot(density(ee1)) curve(dnorm(x,mean(ee1),sd(ee1)),add=T,col=&quot;red&quot;) plot(model1) # Distancia de Cook plot(cooks.distance(model1)) cc&lt;-cooks.distance(model1) bd3&lt;-bd2[cc&lt;quantile(cc,0.90),] model3&lt;-lm(log(ylab)~s02a_03+aestudio+s02a_02+area,data=bd3) summary(model3) ad.test(residuals(model3)) lillie.test(residuals(model3)) plot(density(residuals(model3))) curve(dnorm(x,mean(residuals(model3)),sd(residuals(model3))),add=T,col=&quot;red&quot;) plot(model3) plot(cooks.distance(model3)) #ajustando polinomios bd3&lt;-na.omit(bd3) model4&lt;-lm(log(ylab)~poly(s02a_03,2)+poly(aestudio,3)+s02a_02+area,data=bd3) summary(model4) ad.test(residuals(model4)) ## interacciones entre variables model5&lt;-lm(log(ylab)~poly(s02a_03,2)+poly(aestudio,3)+s02a_02+area+s02a_02:aestudio+area:aestudio+exp(aestudio)+I(aestudio^4),data=bd3) summary(model5) ad.test(residuals(model5)) #trabajando sobre los valores atípicos desde R library(MASS) modela&lt;-rlm(ylab~s02a_02+s02a_03+area,data=bd2) modelb&lt;-lm(ylab~s02a_02+s02a_03+area,data=bd2) summary(modela) summary(modelb) ad.test(residuals(model)) # Colinealidad (X1=f(X2) library(car) vif(model3) sqrt(vif(model3))&gt;2 ##Variance Inflation Factors # Verificar si la varianza es constante (homocedástico) o no (heterocedástico) library(lmtest) bptest(model3) # H0: Homocedsticidad https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test #corrigiendo library(rms) model1 = ols(ylab~s02a_02+s02a_03+area,data=bd3,x=T,y=T) bptest(model1) aux&lt;-robcov(model1) aux # H0: Homocedasticidad, implica que los EE de B no son los correctos 4.1.6 Paso 6: Predicir a partir del modelo test&lt;-bd3 yest&lt;-predict(model3,test) plot(log(bd3$ylab),yest) plot(bd3$ylab,exp(yest)) 4.2 Probit y Logit Estrategia, llevar valores binarios a valores continuos. Mediante una función de enlace (\\(F(Y)\\)). \\[F(Y)=Y&#39;=X \\beta +\\epsilon\\] Probit: \\[Y=\\Phi (X \\beta +\\epsilon)\\] \\[\\phi^{-1}(Y)=X \\beta +\\epsilon\\] \\[Y&#39;=X \\beta +\\epsilon\\] El enlace \\(F(Y)=\\Phi^{-1}(Y)\\), es conocida como probit. Logit: \\[logit(Y)=log(\\frac{Y}{1-Y})=X\\beta+\\epsilon\\] \\[Y=\\frac{e^{X\\beta+\\epsilon}}{1+e^{X\\beta+\\epsilon}}\\] 4.2.1 En R: library(dplyr) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\eh18.Rdata&quot;) aux&lt;-levels(eh18p$s04a_01a) eh18p&lt;-eh18p %&gt;% mutate(diabetes=(s04a_01a==aux[1] | s04a_01b==aux[1]),corazon=(s04a_01a==aux[4] | s04a_01b==aux[4]),hiper=(s04a_01a==aux[10] | s04a_01b==aux[10])) eh18p$diabetes&lt;-(eh18p$diabetes==T); eh18p$diabetes[is.na(eh18p$diabetes)]&lt;-F eh18p$corazon&lt;-(eh18p$corazon==T); eh18p$corazon[is.na(eh18p$corazon)]&lt;-F eh18p$hiper&lt;-(eh18p$hiper==T); eh18p$hiper[is.na(eh18p$hiper)]&lt;-F eh18p$cronicas&lt;-(eh18p$diabetes+eh18p$corazon+eh18p$hiper) #modelo para las enfermedades crónicas eh18p$cronicas&lt;-(eh18p$cronicas!=0) #probit logit logit&lt;-glm(cronicas~s02a_02+s02a_03,data=eh18p,family = binomial(link=&quot;logit&quot;)) probit&lt;-glm(cronicas~s02a_02+s02a_03,data=eh18p,family = binomial(link=&quot;probit&quot;)) lineal&lt;-lm(cronicas~s02a_02+s02a_03,data=eh18p) #resumen summary(logit) summary(probit) summary(lineal) #score probabilidades lres&lt;-predict(logit,eh18p,type=&quot;response&quot;) pres&lt;-predict(probit,eh18p,type=&quot;response&quot;) plot(density(lres)) points(density(pres),type = &quot;l&quot;,col=&quot;red&quot;) #efectos marginales library(mfx) probitmfx(cronicas~s02a_02+s02a_03,data=eh18p) logitmfx(cronicas~s02a_02+s02a_03,data=eh18p) #ajuste library(DescTools) PseudoR2(logit) PseudoR2(probit) summary(lineal) #comparando library(memisc) mtable(logit,probit,lineal) summary(glm(cronicas~s02a_02+s02a_03,data=eh18p)) summary(lm(cronicas~s02a_02+s02a_03,data=eh18p)) "],
["clasificación.html", "5 Clasificación 5.1 Logit Probit 5.2 Arboles de clasificación (CART) 5.3 Naive Bayes 5.4 Ejercicios.", " 5 Clasificación Pasos sugeridos Preparar la base de datos; base de entrenamiento (trainbd) y de testeo (testbd). 70/30 Definir el modelo de clasificación * Regresión logística (logit) * Árbol de clasificación (CART) * Naive bayes Base de datos, se empleara la base de datos de covid de México. library(dplyr) covid&lt;-read.csv(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\covid_mx\\\\200627COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,na.strings = c(99,98)) object.size(covid)/10^6 ## 123.5 bytes vv&lt;-c(&quot;SEXO&quot;,&quot;FECHA_DEF&quot;,&quot;NEUMONIA&quot;,&quot;EDAD&quot;,&quot;HABLA_LENGUA_INDIG&quot;,&quot;DIABETES&quot;,&quot;EPOC&quot;,&quot;ASMA&quot;,&quot;INMUSUPR&quot;,&quot;HIPERTENSION&quot;,&quot;OTRA_COM&quot;,&quot;CARDIOVASCULAR&quot;,&quot;OBESIDAD&quot;,&quot;RENAL_CRONICA&quot;,&quot;TABAQUISMO&quot;,&quot;RESULTADO&quot;) covid&lt;-covid[,vv] covid&lt;-covid %&gt;% filter(EDAD&lt;=90) #Descripción Hmisc::describe(covid) ## covid ## ## 16 Variables 549476 Observations ## ----------------------------------------------------------------------- ## SEXO ## n missing distinct Info Mean Gmd ## 549476 0 2 0.75 1.507 0.4999 ## ## Value 1 2 ## Frequency 270904 278572 ## Proportion 0.493 0.507 ## ----------------------------------------------------------------------- ## FECHA_DEF ## n missing distinct ## 549476 0 122 ## ## lowest : 2020-01-13 2020-01-14 2020-01-15 2020-01-29 2020-01-30 ## highest: 2020-06-24 2020-06-25 2020-06-26 2020-06-27 9999-99-99 ## ----------------------------------------------------------------------- ## NEUMONIA ## n missing distinct Info Mean Gmd ## 549465 11 2 0.392 1.845 0.2615 ## ## Value 1 2 ## Frequency 84969 464496 ## Proportion 0.155 0.845 ## ----------------------------------------------------------------------- ## EDAD ## n missing distinct Info Mean Gmd .05 ## 549476 0 91 1 42.45 18.47 19 ## .10 .25 .50 .75 .90 .95 ## 24 31 41 53 65 72 ## ## lowest : 0 1 2 3 4, highest: 86 87 88 89 90 ## ----------------------------------------------------------------------- ## HABLA_LENGUA_INDIG ## n missing distinct Info Mean Gmd ## 531854 17622 2 0.029 1.99 0.0196 ## ## Value 1 2 ## Frequency 5264 526590 ## Proportion 0.01 0.99 ## ----------------------------------------------------------------------- ## DIABETES ## n missing distinct Info Mean Gmd ## 547557 1919 2 0.328 1.875 0.2187 ## ## Value 1 2 ## Frequency 68437 479120 ## Proportion 0.125 0.875 ## ----------------------------------------------------------------------- ## EPOC ## n missing distinct Info Mean Gmd ## 547788 1688 2 0.047 1.984 0.03108 ## ## Value 1 2 ## Frequency 8650 539138 ## Proportion 0.016 0.984 ## ----------------------------------------------------------------------- ## ASMA ## n missing distinct Info Mean Gmd ## 547781 1695 2 0.093 1.968 0.06216 ## ## Value 1 2 ## Frequency 17589 530192 ## Proportion 0.032 0.968 ## ----------------------------------------------------------------------- ## INMUSUPR ## n missing distinct Info Mean Gmd ## 547565 1911 2 0.047 1.984 0.03133 ## ## Value 1 2 ## Frequency 8715 538850 ## Proportion 0.016 0.984 ## ----------------------------------------------------------------------- ## HIPERTENSION ## n missing distinct Info Mean Gmd ## 547713 1763 2 0.409 1.837 0.2724 ## ## Value 1 2 ## Frequency 89094 458619 ## Proportion 0.163 0.837 ## ----------------------------------------------------------------------- ## OTRA_COM ## n missing distinct Info Mean Gmd ## 546958 2518 2 0.088 1.97 0.05884 ## ## Value 1 2 ## Frequency 16596 530362 ## Proportion 0.03 0.97 ## ----------------------------------------------------------------------- ## CARDIOVASCULAR ## n missing distinct Info Mean Gmd ## 547716 1760 2 0.066 1.978 0.04372 ## ## Value 1 2 ## Frequency 12246 535470 ## Proportion 0.022 0.978 ## ----------------------------------------------------------------------- ## OBESIDAD ## n missing distinct Info Mean Gmd ## 547756 1720 2 0.411 1.836 0.274 ## ## Value 1 2 ## Frequency 89749 458007 ## Proportion 0.164 0.836 ## ----------------------------------------------------------------------- ## RENAL_CRONICA ## n missing distinct Info Mean Gmd ## 547749 1727 2 0.058 1.98 0.03897 ## ## Value 1 2 ## Frequency 10889 536860 ## Proportion 0.02 0.98 ## ----------------------------------------------------------------------- ## TABAQUISMO ## n missing distinct Info Mean Gmd ## 547637 1839 2 0.234 1.915 0.156 ## ## Value 1 2 ## Frequency 46713 500924 ## Proportion 0.085 0.915 ## ----------------------------------------------------------------------- ## RESULTADO ## n missing distinct Info Mean Gmd ## 549476 0 3 0.821 1.736 0.688 ## ## Value 1 2 3 ## Frequency 212222 270355 66899 ## Proportion 0.386 0.492 0.122 ## ----------------------------------------------------------------------- #variable muerte covid$muerte&lt;-(covid$FECHA_DEF!=&quot;9999-99-99&quot;) covid&lt;-covid %&gt;% dplyr::select(-FECHA_DEF) covid&lt;-na.omit(covid) 5.1 Logit Probit Se usa para realizar clasificaciones basadas en probabilidades Las clasificaciones son del tipo 1/0 Existen variaciones para clasificar considerando más grupos, empleando el logit y probit ordenado. 5.1.1 Pasos Identificar la variable (1/0) que se requiere clasificar, definir covariables para construir el modelos table(covid$muerte) ## ## FALSE TRUE ## 494841 33611 prop.table(table(covid$muerte)) ## ## FALSE TRUE ## 0.93639725 0.06360275 ################################################## #a factor ################################################## #sexo covid$SEXO&lt;-factor(covid$SEXO,levels=1:2,labels=c(&quot;Mujer&quot;,&quot;Hombre&quot;)) #resultado covid$RESULTADO&lt;-factor(covid$RESULTADO,levels = 1:3,labels=c(&quot;COVID +&quot;,&quot;COVID -&quot;,&quot;COVID pendiende&quot;)) covid2&lt;-covid#para cart #si/no aux&lt;-c(&quot;NEUMONIA&quot;,&quot;HABLA_LENGUA_INDIG&quot;,&quot;DIABETES&quot;,&quot;EPOC&quot;,&quot;ASMA&quot;,&quot;INMUSUPR&quot;,&quot;HIPERTENSION&quot;,&quot;OTRA_COM&quot;,&quot;CARDIOVASCULAR&quot;,&quot;OBESIDAD&quot;,&quot;RENAL_CRONICA&quot;,&quot;TABAQUISMO&quot;) for(i in aux){ covid[[i]]&lt;-covid[[i]]==1 } str(covid) ## &#39;data.frame&#39;: 528452 obs. of 16 variables: ## $ SEXO : Factor w/ 2 levels &quot;Mujer&quot;,&quot;Hombre&quot;: 2 2 2 2 2 1 2 1 2 2 ... ## $ NEUMONIA : logi FALSE FALSE TRUE FALSE FALSE FALSE ... ## $ EDAD : int 63 39 62 86 46 40 52 46 73 61 ... ## $ HABLA_LENGUA_INDIG: logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ DIABETES : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ EPOC : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ ASMA : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ INMUSUPR : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ HIPERTENSION : logi TRUE FALSE FALSE TRUE FALSE FALSE ... ## $ OTRA_COM : logi FALSE FALSE FALSE FALSE TRUE FALSE ... ## $ CARDIOVASCULAR : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ OBESIDAD : logi TRUE FALSE FALSE FALSE FALSE FALSE ... ## $ RENAL_CRONICA : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ TABAQUISMO : logi FALSE TRUE FALSE FALSE FALSE FALSE ... ## $ RESULTADO : Factor w/ 3 levels &quot;COVID +&quot;,&quot;COVID -&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ muerte : logi FALSE FALSE TRUE TRUE FALSE FALSE ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:21024] 23 184 230 236 363 642 737 820 831 915 ... ## ..- attr(*, &quot;names&quot;)= chr [1:21024] &quot;23&quot; &quot;184&quot; &quot;230&quot; &quot;236&quot; ... ## Bases: trainbd, testbd set.seed(123) index = sample(1:2, nrow(covid), replace = TRUE, prob=c(0.7, 0.3)) prop.table(table(index)) ## index ## 1 2 ## 0.7001544 0.2998456 trainbd&lt;-covid[index==1,] testbd&lt;-covid[index==2,] Especificar el modelo (logit/probit) m1&lt;-glm(muerte~.,data=trainbd,family = binomial(link=&quot;logit&quot;)) Identificar las variables significativas Construir el modelo con variables significativas summary(m1) ## ## Call: ## glm(formula = muerte ~ ., family = binomial(link = &quot;logit&quot;), ## data = trainbd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2988 -0.2468 -0.1466 -0.0944 3.8471 ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -6.0453094 0.0351339 -172.065 ## SEXOHombre 0.4528000 0.0167200 27.081 ## NEUMONIATRUE 2.1601129 0.0166393 129.820 ## EDAD 0.0503957 0.0005735 87.881 ## HABLA_LENGUA_INDIGTRUE 0.1299336 0.0636812 2.040 ## DIABETESTRUE 0.3568379 0.0185218 19.266 ## EPOCTRUE 0.0606881 0.0396321 1.531 ## ASMATRUE -0.2211289 0.0539182 -4.101 ## INMUSUPRTRUE 0.3809713 0.0467945 8.141 ## HIPERTENSIONTRUE 0.1546319 0.0187518 8.246 ## OTRA_COMTRUE 0.5776581 0.0358269 16.124 ## CARDIOVASCULARTRUE -0.0544158 0.0376414 -1.446 ## OBESIDADTRUE 0.1697086 0.0194157 8.741 ## RENAL_CRONICATRUE 0.7287407 0.0354132 20.578 ## TABAQUISMOTRUE -0.0631770 0.0281778 -2.242 ## RESULTADOCOVID - -1.1795001 0.0193564 -60.936 ## RESULTADOCOVID pendiende -1.3541807 0.0317650 -42.631 ## Pr(&gt;|z|) ## (Intercept) &lt; 0.0000000000000002 *** ## SEXOHombre &lt; 0.0000000000000002 *** ## NEUMONIATRUE &lt; 0.0000000000000002 *** ## EDAD &lt; 0.0000000000000002 *** ## HABLA_LENGUA_INDIGTRUE 0.0413 * ## DIABETESTRUE &lt; 0.0000000000000002 *** ## EPOCTRUE 0.1257 ## ASMATRUE 0.000041102762764731 *** ## INMUSUPRTRUE 0.000000000000000391 *** ## HIPERTENSIONTRUE &lt; 0.0000000000000002 *** ## OTRA_COMTRUE &lt; 0.0000000000000002 *** ## CARDIOVASCULARTRUE 0.1483 ## OBESIDADTRUE &lt; 0.0000000000000002 *** ## RENAL_CRONICATRUE &lt; 0.0000000000000002 *** ## TABAQUISMOTRUE 0.0250 * ## RESULTADOCOVID - &lt; 0.0000000000000002 *** ## RESULTADOCOVID pendiende &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 175579 on 369997 degrees of freedom ## Residual deviance: 113042 on 369981 degrees of freedom ## AIC: 113076 ## ## Number of Fisher Scoring iterations: 7 m2&lt;-step(m1) ## Start: AIC=113075.8 ## muerte ~ SEXO + NEUMONIA + EDAD + HABLA_LENGUA_INDIG + DIABETES + ## EPOC + ASMA + INMUSUPR + HIPERTENSION + OTRA_COM + CARDIOVASCULAR + ## OBESIDAD + RENAL_CRONICA + TABAQUISMO + RESULTADO ## ## Df Deviance AIC ## &lt;none&gt; 113042 113076 ## - CARDIOVASCULAR 1 113044 113076 ## - EPOC 1 113044 113076 ## - HABLA_LENGUA_INDIG 1 113046 113078 ## - TABAQUISMO 1 113047 113079 ## - ASMA 1 113059 113091 ## - INMUSUPR 1 113105 113137 ## - HIPERTENSION 1 113109 113141 ## - OBESIDAD 1 113117 113149 ## - OTRA_COM 1 113286 113318 ## - DIABETES 1 113406 113438 ## - RENAL_CRONICA 1 113445 113477 ## - SEXO 1 113789 113821 ## - RESULTADO 2 118350 118380 ## - EDAD 1 121441 121473 ## - NEUMONIA 1 131172 131204 summary(m2) ## ## Call: ## glm(formula = muerte ~ SEXO + NEUMONIA + EDAD + HABLA_LENGUA_INDIG + ## DIABETES + EPOC + ASMA + INMUSUPR + HIPERTENSION + OTRA_COM + ## CARDIOVASCULAR + OBESIDAD + RENAL_CRONICA + TABAQUISMO + ## RESULTADO, family = binomial(link = &quot;logit&quot;), data = trainbd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2988 -0.2468 -0.1466 -0.0944 3.8471 ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -6.0453094 0.0351339 -172.065 ## SEXOHombre 0.4528000 0.0167200 27.081 ## NEUMONIATRUE 2.1601129 0.0166393 129.820 ## EDAD 0.0503957 0.0005735 87.881 ## HABLA_LENGUA_INDIGTRUE 0.1299336 0.0636812 2.040 ## DIABETESTRUE 0.3568379 0.0185218 19.266 ## EPOCTRUE 0.0606881 0.0396321 1.531 ## ASMATRUE -0.2211289 0.0539182 -4.101 ## INMUSUPRTRUE 0.3809713 0.0467945 8.141 ## HIPERTENSIONTRUE 0.1546319 0.0187518 8.246 ## OTRA_COMTRUE 0.5776581 0.0358269 16.124 ## CARDIOVASCULARTRUE -0.0544158 0.0376414 -1.446 ## OBESIDADTRUE 0.1697086 0.0194157 8.741 ## RENAL_CRONICATRUE 0.7287407 0.0354132 20.578 ## TABAQUISMOTRUE -0.0631770 0.0281778 -2.242 ## RESULTADOCOVID - -1.1795001 0.0193564 -60.936 ## RESULTADOCOVID pendiende -1.3541807 0.0317650 -42.631 ## Pr(&gt;|z|) ## (Intercept) &lt; 0.0000000000000002 *** ## SEXOHombre &lt; 0.0000000000000002 *** ## NEUMONIATRUE &lt; 0.0000000000000002 *** ## EDAD &lt; 0.0000000000000002 *** ## HABLA_LENGUA_INDIGTRUE 0.0413 * ## DIABETESTRUE &lt; 0.0000000000000002 *** ## EPOCTRUE 0.1257 ## ASMATRUE 0.000041102762764731 *** ## INMUSUPRTRUE 0.000000000000000391 *** ## HIPERTENSIONTRUE &lt; 0.0000000000000002 *** ## OTRA_COMTRUE &lt; 0.0000000000000002 *** ## CARDIOVASCULARTRUE 0.1483 ## OBESIDADTRUE &lt; 0.0000000000000002 *** ## RENAL_CRONICATRUE &lt; 0.0000000000000002 *** ## TABAQUISMOTRUE 0.0250 * ## RESULTADOCOVID - &lt; 0.0000000000000002 *** ## RESULTADOCOVID pendiende &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 175579 on 369997 degrees of freedom ## Residual deviance: 113042 on 369981 degrees of freedom ## AIC: 113076 ## ## Number of Fisher Scoring iterations: 7 Predecir la clase de pertenencia en la base de test (\\(prob&gt;0.5\\)) clase&lt;-predict(m2,testbd,type=&quot;response&quot;)&gt;0.5 Observar la clasificación dada en base a la probabilidad fijada table(clase) ## clase ## FALSE TRUE ## 153984 4470 Comparar lo observado y lo predicho table(testbd$muerte,clase) ## clase ## FALSE TRUE ## FALSE 146404 2041 ## TRUE 7580 2429 Generar la matriz de confusión (librería caret) library(caret) ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:vegan&#39;: ## ## tolerance ## The following object is masked from &#39;package:survival&#39;: ## ## cluster confusionMatrix(table(testbd$muerte,clase)) ## Confusion Matrix and Statistics ## ## clase ## FALSE TRUE ## FALSE 146404 2041 ## TRUE 7580 2429 ## ## Accuracy : 0.9393 ## 95% CI : (0.9381, 0.9405) ## No Information Rate : 0.9718 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.3086 ## ## Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002 ## ## Sensitivity : 0.9508 ## Specificity : 0.5434 ## Pos Pred Value : 0.9863 ## Neg Pred Value : 0.2427 ## Prevalence : 0.9718 ## Detection Rate : 0.9240 ## Detection Prevalence : 0.9368 ## Balanced Accuracy : 0.7471 ## ## &#39;Positive&#39; Class : FALSE ## Efectos marginales library(mfx) ## Loading required package: sandwich ## Loading required package: lmtest ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:srvyr&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## Loading required package: betareg info&lt;-logitmfx(formula(m2),data=testbd) barplot(info$mfxest[,1],horiz = T,las=1,cex.names = 0.5,xlim=c(-0.1,0.1),pos=-0.02) ## Warning in plot.window(xlim, ylim, log = log, ...): &quot;pos&quot; is not a ## graphical parameter ## Warning in title(main = main, sub = sub, xlab = xlab, ylab = ## ylab, ...): &quot;pos&quot; is not a graphical parameter 5.2 Arboles de clasificación (CART) El método CART uso condiciones basadas en cortes sobre covariables para realizar la clasificación (predicción) de una clase. El proceso de clasificación comienza desde el nodo raíz del árbol; en cada nodo, el proceso verificará si el valor de entrada debe continuar de forma recursiva hacia la sub-rama derecha o izquierda de acuerdo con la condición de división, y se detiene al encontrar cualquier nodo hoja (terminal) del árbol de decisión. 5.2.1 Pasos Crear el modelo de clasificación Cargar la librería rpart library(rpart) Usar la función rpart para construir el modelo de clasificación covid2$muerte&lt;-factor(covid2$muerte,c(T,F),labels = c(&quot;Muerte&quot;,&quot;No muerte&quot;)) #si/no aux&lt;-c(&quot;NEUMONIA&quot;,&quot;HABLA_LENGUA_INDIG&quot;,&quot;DIABETES&quot;,&quot;EPOC&quot;,&quot;ASMA&quot;,&quot;INMUSUPR&quot;,&quot;HIPERTENSION&quot;,&quot;OTRA_COM&quot;,&quot;CARDIOVASCULAR&quot;,&quot;OBESIDAD&quot;,&quot;RENAL_CRONICA&quot;,&quot;TABAQUISMO&quot;) for(i in aux){ covid2[[i]]&lt;-factor(covid2[[i]],1:2,c(&quot;SI&quot;,&quot;NO&quot;)) } str(covid2) ## &#39;data.frame&#39;: 528452 obs. of 16 variables: ## $ SEXO : Factor w/ 2 levels &quot;Mujer&quot;,&quot;Hombre&quot;: 2 2 2 2 2 1 2 1 2 2 ... ## $ NEUMONIA : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 1 2 2 2 2 2 2 2 ... ## $ EDAD : int 63 39 62 86 46 40 52 46 73 61 ... ## $ HABLA_LENGUA_INDIG: Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ DIABETES : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 1 2 ... ## $ EPOC : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ ASMA : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ INMUSUPR : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ HIPERTENSION : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 1 2 2 1 2 2 2 2 2 2 ... ## $ OTRA_COM : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 1 2 2 1 2 2 ... ## $ CARDIOVASCULAR : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 1 2 2 ... ## $ OBESIDAD : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 1 2 2 2 2 2 2 1 2 2 ... ## $ RENAL_CRONICA : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ TABAQUISMO : Factor w/ 2 levels &quot;SI&quot;,&quot;NO&quot;: 2 1 2 2 2 2 2 2 2 2 ... ## $ RESULTADO : Factor w/ 3 levels &quot;COVID +&quot;,&quot;COVID -&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ muerte : Factor w/ 2 levels &quot;Muerte&quot;,&quot;No muerte&quot;: 2 2 1 1 2 2 2 2 2 2 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:21024] 23 184 230 236 363 642 737 820 831 915 ... ## ..- attr(*, &quot;names&quot;)= chr [1:21024] &quot;23&quot; &quot;184&quot; &quot;230&quot; &quot;236&quot; ... ## Bases: trainbd, testbd set.seed(123) index = sample(1:2, nrow(covid2), replace = TRUE, prob=c(0.7, 0.3)) prop.table(table(index)) ## index ## 1 2 ## 0.7001544 0.2998456 trainbd&lt;-covid2[index==1,] testbd&lt;-covid2[index==2,] mod1&lt;-rpart(muerte~.,data=trainbd) Explorar los nodos creados por rpart mod1 ## n= 369998 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 369998 23602 No muerte (0.06378953 0.93621047) ## 2) NEUMONIA=SI 57116 16946 No muerte (0.29669445 0.70330555) ## 4) RESULTADO=COVID + 34442 13362 No muerte (0.38795656 0.61204344) ## 8) EDAD&gt;=51.5 20669 10039 No muerte (0.48570323 0.51429677) ## 16) EDAD&gt;=64.5 9808 4363 Muerte (0.55515905 0.44484095) * ## 17) EDAD&lt; 64.5 10861 4594 No muerte (0.42298131 0.57701869) * ## 9) EDAD&lt; 51.5 13773 3323 No muerte (0.24126915 0.75873085) * ## 5) RESULTADO=COVID -,COVID pendiende 22674 3584 No muerte (0.15806651 0.84193349) * ## 3) NEUMONIA=NO 312882 6656 No muerte (0.02127320 0.97872680) * Examinar los parámetros del árbol con printcp printcp(mod1) ## ## Classification tree: ## rpart(formula = muerte ~ ., data = trainbd) ## ## Variables actually used in tree construction: ## [1] EDAD NEUMONIA RESULTADO ## ## Root node error: 23602/369998 = 0.06379 ## ## n= 369998 ## ## CP nsplit rel error xerror xstd ## 1 0.011461 0 1.00000 1.0000 0.0062981 ## 2 0.010000 4 0.95416 0.9567 0.0061694 Usar el comando plotcp para explorar los parámetros de forma gráfica plotcp(mod1) Usar la función summary para para examinar el modelo summary(mod1) ## Call: ## rpart(formula = muerte ~ ., data = trainbd) ## n= 369998 ## ## CP nsplit rel error xerror xstd ## 1 0.01146089 0 1.0000000 1.0000000 0.006298140 ## 2 0.01000000 4 0.9541564 0.9566986 0.006169353 ## ## Variable importance ## NEUMONIA RESULTADO EDAD ## 72 14 13 ## ## Node number 1: 369998 observations, complexity param=0.01146089 ## predicted class=No muerte expected loss=0.06378953 P(node) =1 ## class counts: 23602 346396 ## probabilities: 0.064 0.936 ## left son=2 (57116 obs) right son=3 (312882 obs) ## Primary splits: ## NEUMONIA splits as LR, improve=7327.636, (0 missing) ## EDAD &lt; 57.5 to the right, improve=3636.657, (0 missing) ## RESULTADO splits as LRR, improve=1662.835, (0 missing) ## DIABETES splits as LR, improve=1643.543, (0 missing) ## HIPERTENSION splits as LR, improve=1431.782, (0 missing) ## ## Node number 2: 57116 observations, complexity param=0.01146089 ## predicted class=No muerte expected loss=0.2966944 P(node) =0.1543684 ## class counts: 16946 40170 ## probabilities: 0.297 0.703 ## left son=4 (34442 obs) right son=5 (22674 obs) ## Primary splits: ## RESULTADO splits as LRR, improve=1445.2040, (0 missing) ## EDAD &lt; 49.5 to the right, improve=1368.6730, (0 missing) ## HIPERTENSION splits as LR, improve= 357.6824, (0 missing) ## DIABETES splits as LR, improve= 303.1878, (0 missing) ## SEXO splits as RL, improve= 104.5722, (0 missing) ## Surrogate splits: ## EDAD &lt; 30.5 to the right, agree=0.631, adj=0.072, (0 split) ## INMUSUPR splits as RL, agree=0.609, adj=0.015, (0 split) ## OTRA_COM splits as RL, agree=0.607, adj=0.010, (0 split) ## EPOC splits as RL, agree=0.604, adj=0.002, (0 split) ## CARDIOVASCULAR splits as RL, agree=0.604, adj=0.002, (0 split) ## ## Node number 3: 312882 observations ## predicted class=No muerte expected loss=0.0212732 P(node) =0.8456316 ## class counts: 6656 306226 ## probabilities: 0.021 0.979 ## ## Node number 4: 34442 observations, complexity param=0.01146089 ## predicted class=No muerte expected loss=0.3879566 P(node) =0.09308699 ## class counts: 13362 21080 ## probabilities: 0.388 0.612 ## left son=8 (20669 obs) right son=9 (13773 obs) ## Primary splits: ## EDAD &lt; 51.5 to the right, improve=987.67290, (0 missing) ## HIPERTENSION splits as LR, improve=276.65030, (0 missing) ## DIABETES splits as LR, improve=194.20950, (0 missing) ## RENAL_CRONICA splits as LR, improve= 89.58991, (0 missing) ## EPOC splits as LR, improve= 51.97436, (0 missing) ## ## Node number 5: 22674 observations ## predicted class=No muerte expected loss=0.1580665 P(node) =0.06128141 ## class counts: 3584 19090 ## probabilities: 0.158 0.842 ## ## Node number 8: 20669 observations, complexity param=0.01146089 ## predicted class=No muerte expected loss=0.4857032 P(node) =0.05586246 ## class counts: 10039 10630 ## probabilities: 0.486 0.514 ## left son=16 (9808 obs) right son=17 (10861 obs) ## Primary splits: ## EDAD &lt; 64.5 to the right, improve=180.08500, (0 missing) ## SEXO splits as RL, improve= 26.65526, (0 missing) ## RENAL_CRONICA splits as LR, improve= 23.86824, (0 missing) ## HIPERTENSION splits as LR, improve= 20.12766, (0 missing) ## DIABETES splits as LR, improve= 13.55075, (0 missing) ## Surrogate splits: ## HIPERTENSION splits as LR, agree=0.573, adj=0.100, (0 split) ## EPOC splits as LR, agree=0.547, adj=0.044, (0 split) ## CARDIOVASCULAR splits as LR, agree=0.543, adj=0.037, (0 split) ## OTRA_COM splits as LR, agree=0.531, adj=0.011, (0 split) ## RENAL_CRONICA splits as LR, agree=0.529, adj=0.007, (0 split) ## ## Node number 9: 13773 observations ## predicted class=No muerte expected loss=0.2412691 P(node) =0.03722453 ## class counts: 3323 10450 ## probabilities: 0.241 0.759 ## ## Node number 16: 9808 observations ## predicted class=Muerte expected loss=0.4448409 P(node) =0.02650825 ## class counts: 5445 4363 ## probabilities: 0.555 0.445 ## ## Node number 17: 10861 observations ## predicted class=No muerte expected loss=0.4229813 P(node) =0.02935421 ## class counts: 4594 6267 ## probabilities: 0.423 0.577 Visualizar el árbol Usar la función plot y text(,all=T, n=T) plot(mod1) text(mod1,all=T,use.n=T) #install.packages(&quot;rpart.plot&quot;) library(rpart.plot) rpart.plot(mod1) Ajustes en el layout plot(…,uniform=TRUE, branch=0.6, margin=0.1) plot(mod1,uniform=T, branch=1, margin=0.1) text(mod1,all=T,use.n=T) Predicción de la clasificación predict(…, testbd, type=“class”), predicción sobre la base de test clase&lt;-predict(mod1,testbd,type = &quot;class&quot;) Elaborar una tabla de contingencia de la clasificación table(clase,testbd$muerte) ## ## clase Muerte No muerte ## Muerte 2405 1862 ## No muerte 7604 146583 Emplear el comando confusionMatrix sobre la tabla del paso anterior, para evaluar la calidad de la clasificación. Mcnemar’s Test H0: \\(ij=ji\\) library(caret) confusionMatrix(table(clase,testbd$muerte)) ## Confusion Matrix and Statistics ## ## ## clase Muerte No muerte ## Muerte 2405 1862 ## No muerte 7604 146583 ## ## Accuracy : 0.9403 ## 95% CI : (0.9391, 0.9414) ## No Information Rate : 0.9368 ## P-Value [Acc &gt; NIR] : 0.00000000806 ## ## Kappa : 0.3109 ## ## Mcnemar&#39;s Test P-Value : &lt; 0.00000000000000022 ## ## Sensitivity : 0.24028 ## Specificity : 0.98746 ## Pos Pred Value : 0.56363 ## Neg Pred Value : 0.95068 ## Prevalence : 0.06317 ## Detection Rate : 0.01518 ## Detection Prevalence : 0.02693 ## Balanced Accuracy : 0.61387 ## ## &#39;Positive&#39; Class : Muerte ## 5.2.2 Proceso de pruning (podado) El objetivo es eliminar variables redundantes y crear un modelo de clasificación mas robusto Pasos: Encuentre el valor mínimo en cross-validation error. (xerror) min(mod1$cptable[,&quot;xerror&quot;]) ## [1] 0.9566986 Encontrar el registro que contiene el valor del anterior paso. (which.min, cptable) which.min(mod1$cptable[,&quot;xerror&quot;]) ## 2 ## 2 Obtenga el “cost complexity parameter” del valor mínimo encontrado (CP) mod1.cp&lt;-mod1$cptable[2,&quot;CP&quot;] Realizar el podado con la función prune, empleando el modelo original y el CP del valor mínimo en xerror (paso anterior) mod1.prune&lt;-prune(mod1,cp=mod1.cp) Visualice el nuevo árbol rpart.plot(mod1.prune) Realice la predicción a partir del árbol podado clase&lt;-predict(mod1.prune,testbd,type = &quot;class&quot;) Evalúe los resultados con la matriz de confusión confusionMatrix(table(clase,testbd$muerte)) ## Confusion Matrix and Statistics ## ## ## clase Muerte No muerte ## Muerte 2405 1862 ## No muerte 7604 146583 ## ## Accuracy : 0.9403 ## 95% CI : (0.9391, 0.9414) ## No Information Rate : 0.9368 ## P-Value [Acc &gt; NIR] : 0.00000000806 ## ## Kappa : 0.3109 ## ## Mcnemar&#39;s Test P-Value : &lt; 0.00000000000000022 ## ## Sensitivity : 0.24028 ## Specificity : 0.98746 ## Pos Pred Value : 0.56363 ## Neg Pred Value : 0.95068 ## Prevalence : 0.06317 ## Detection Rate : 0.01518 ## Detection Prevalence : 0.02693 ## Balanced Accuracy : 0.61387 ## ## &#39;Positive&#39; Class : Muerte ## 5.3 Naive Bayes Es un modelo basado en probabilidad, su base teórica aplica el teorema de Bayes (fuerte supuesto de independencia). nota \\[P(muerte/edad,neumonia, ...)&lt;&gt;P(\\sim muerte/edad,neumonia,...)\\] \\[P(C/X)=\\frac{P(C)*P(X/C)}{P(X)}\\] * \\(P(C/X)\\) Probabilidad Posterior * \\(P(C)\\) Probabilidad a Priori * \\(P(X/C)\\) Verosimilitud * \\(P(X)\\) Marginal Si se tiene varios predictores (\\(X\\)) se supone independencia, esto es: \\[P(C/X) = \\frac{P(X_1/C)*P(X_2/C)*\\ldots *P(X_n/C)*P(C)}{P(X)}\\] Pasos, Cargar la librería e1071 y emplear la función naiveBayes para construir el clasificador library(e1071) ## ## Attaching package: &#39;e1071&#39; ## The following object is masked from &#39;package:Hmisc&#39;: ## ## impute mod1&lt;-naiveBayes(muerte~.,data=trainbd) Explorar los resultados mod1 ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## Muerte No muerte ## 0.06378953 0.93621047 ## ## Conditional probabilities: ## SEXO ## Y Mujer Hombre ## Muerte 0.3513685 0.6486315 ## No muerte 0.5045959 0.4954041 ## ## NEUMONIA ## Y SI NO ## Muerte 0.7179900 0.2820100 ## No muerte 0.1159655 0.8840345 ## ## EDAD ## Y [,1] [,2] ## Muerte 60.42543 15.03884 ## No muerte 41.26280 15.78475 ## ## HABLA_LENGUA_INDIG ## Y SI NO ## Muerte 0.017710364 0.982289636 ## No muerte 0.009295719 0.990704281 ## ## DIABETES ## Y SI NO ## Muerte 0.3705618 0.6294382 ## No muerte 0.1089822 0.8910178 ## ## EPOC ## Y SI NO ## Muerte 0.05448691 0.94551309 ## No muerte 0.01310927 0.98689073 ## ## ASMA ## Y SI NO ## Muerte 0.02076095 0.97923905 ## No muerte 0.03311239 0.96688761 ## ## INMUSUPR ## Y SI NO ## Muerte 0.03694602 0.96305398 ## No muerte 0.01450074 0.98549926 ## ## HIPERTENSION ## Y SI NO ## Muerte 0.4184815 0.5815185 ## No muerte 0.1461045 0.8538955 ## ## OTRA_COM ## Y SI NO ## Muerte 0.06448606 0.93551394 ## No muerte 0.02853670 0.97146330 ## ## CARDIOVASCULAR ## Y SI NO ## Muerte 0.05982544 0.94017456 ## No muerte 0.01967113 0.98032887 ## ## OBESIDAD ## Y SI NO ## Muerte 0.232904 0.767096 ## No muerte 0.159589 0.840411 ## ## RENAL_CRONICA ## Y SI NO ## Muerte 0.07825608 0.92174392 ## No muerte 0.01579695 0.98420305 ## ## TABAQUISMO ## Y SI NO ## Muerte 0.09397509 0.90602491 ## No muerte 0.08496345 0.91503655 ## ## RESULTADO ## Y COVID + COVID - COVID pendiende ## Muerte 0.74934328 0.19455978 0.05609694 ## No muerte 0.36267162 0.51143200 0.12589637 Predecir los resultados en la base de testeo clase&lt;-predict(mod1,testbd,type = &quot;class&quot;) Realizar la matriz de confusión confusionMatrix(table(clase,testbd$muerte)) ## Confusion Matrix and Statistics ## ## ## clase Muerte No muerte ## Muerte 5261 8124 ## No muerte 4748 140321 ## ## Accuracy : 0.9188 ## 95% CI : (0.9174, 0.9201) ## No Information Rate : 0.9368 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.4069 ## ## Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002 ## ## Sensitivity : 0.52563 ## Specificity : 0.94527 ## Pos Pred Value : 0.39305 ## Neg Pred Value : 0.96727 ## Prevalence : 0.06317 ## Detection Rate : 0.03320 ## Detection Prevalence : 0.08447 ## Balanced Accuracy : 0.73545 ## ## &#39;Positive&#39; Class : Muerte ## 5.4 Ejercicios. Usando la ENDSA para un año en particular, defina clases de violencia en base a las variables de violencia y aplique los métodos de clasificación con las variables que considere relevante. Usando la Encuesta a hogares, para la clase nivel de educación (ninguno, primaria, secundaria, superior) aplique los métodos de clasificación considerando las variables relevantes. "],
["minería-de-texto.html", "6 Minería de Texto 6.1 Introducción 6.2 Recolección de texto 6.3 Nubes de palabras 6.4 Análisis de sentimiento (sentimental scoring)", " 6 Minería de Texto La minería de texto es el proceso de destilar información procesable del texto. (Ted 2017) Minería de texto puede ser sinónimo de análisis de texto, sin embargo, el uso de minería de texto describe de forma más adecuada el descubrimiento de ideas y el uso de algorítmos específicos más alla del análisis estadístico básico. 6.1 Introducción 6.1.1 En la práctica Análisis de mercado (impacto en los consumidores, marca, etc.) Análisis político (percepción, sentimientos, etc.) Match: CV y expectativas de una empresa Inteligencia de negocios 6.1.2 ¿Por qué importa? Las redes sociales continúan evolucionando y afectan los esfuerzos públicos de una organización. El contenido en línea de una organización, sus competidores y fuentes externas, como los blogs, continúa creciendo. La digitalización de los registros en papel se está produciendo en muchas industrias. 6.1.3 Las consecuencias de ignorarlo Ignorar el texto no es una respuesta adecuada de un esfuerzo analítico. La exploración científica y analítica rigurosa requiere investigar fuentes de información que puedan explicar los fenómenos. No realizar minería de texto puede conducir a un análisis o resultado falso. Algunos problemas se basan casi exclusivamente en texto, por lo que no usar estos métodos significaría una reducción significativa en la efectividad o incluso no poder realizar el análisis. 6.1.4 Beneficios La confianza se genera entre las partes interesadas ya que se necesita poco o ningún muestreo para extraer información. Las metodologías se pueden aplicar rápidamente. El uso de R permite métodos auditables y repetibles. La minería de texto identifica nuevas ideas o refuerza las percepciones existentes basadas en toda la información relevante. Posibles usos 6.1.5 Flujo de trabajo en la minería de texto Flujo de trabajo Definir el problema y establecer las metas Identificar el texto que se quiere recolectar Organizar el texto (corpus, colección de documentos) Extraer características Analizar el texto Llegar a una idea o una recomendación 6.1.6 Librerías en R para texto stringi stringr qdap tm Comandos para texto nchar paste, paste0 sub, gsub, grep mgsub (qdap) library(qdap) fake.text&lt;-&#39;R text mining is good but text mining in python is also&#39; patterns&lt;-c(&#39;good&#39;,&#39;also&#39;,&#39;text mining&#39;) replacements&lt;-c(&#39;great&#39;,&#39;just as suitable&#39;,&#39;tm&#39;) mgsub(patterns,replacements,fake.text) tolower removePunctuation stripWhitespace removeNumbers removeWords (stopwords) stemDocument 6.2 Recolección de texto La recolección de texto puede provenir de: Base de datos en csv u otro similar Colección de documentos Scraping Web, API 6.2.1 CSV library(tm) library(dplyr) setwd(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data&quot;) fb&lt;-read.csv(&quot;bd_sc.csv&quot;) fb&lt;-read.csv(&quot;bd_sc.csv&quot;,encoding = &quot;UTF-8&quot;) #fb&lt;-read.csv(&quot;bd_sc.csv&quot;,encoding = &quot;Latin-1&quot;) fb$post_text[5] 6.2.2 Colección de documentos library(pdftools) dir&lt;-&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-384\\\\data\\\\pdf&quot; pdfdocs &lt;- VCorpus(DirSource(dir, pattern = &quot;.pdf&quot;), readerControl = list(reader = readPDF)) 6.2.3 Twitter (API) library(rtweet) tw&lt;-search_tweets(&quot;Dioxido de cloro&quot;,n=10000,include_rts = F) 6.3 Nubes de palabras library(wordcloud2) docs&lt;-Corpus(VectorSource(fb$post_text)) docs &lt;- docs %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(stripWhitespace) docs &lt;- tm_map(docs, content_transformer(tolower)) docs &lt;- tm_map(docs, removeWords, stopwords(&quot;sp&quot;)) dtm &lt;- TermDocumentMatrix(docs) matrix &lt;- as.matrix(dtm) words &lt;- sort(rowSums(matrix),decreasing=TRUE) df &lt;- data.frame(word = names(words),freq=words) ##funciones nube&lt;-function(aux){ docs&lt;-Corpus(VectorSource(aux)) docs &lt;- docs %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(stripWhitespace) docs &lt;- tm_map(docs, content_transformer(tolower)) docs &lt;- tm_map(docs, removeWords, stopwords(&quot;sp&quot;)) dtm &lt;- TermDocumentMatrix(docs) matrix &lt;- as.matrix(dtm) words &lt;- sort(rowSums(matrix),decreasing=TRUE) df &lt;- data.frame(word = names(words),freq=words) return(df) } nube2&lt;-function(aux){ docs &lt;- aux %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(stripWhitespace) docs &lt;- tm_map(docs, content_transformer(tolower)) docs &lt;- tm_map(docs, removeWords, stopwords(&quot;sp&quot;)) dtm &lt;- TermDocumentMatrix(docs) matrix &lt;- as.matrix(dtm) words &lt;- sort(rowSums(matrix),decreasing=TRUE) df &lt;- data.frame(word = names(words),freq=words) return(df) } Sobre los tipos de datos #csv df&lt;-nube(fb$post_text) wordcloud2(data=df[df$freq&gt;5,],color=&#39;random-dark&#39;,size = 0.4,shape = &#39;star&#39;) #colección de documentos df&lt;-nube2(pdfdocs) wordcloud2(data=df,color=&#39;random-dark&#39;,size = 0.4,shape = &#39;pentagon&#39;) #scrape df&lt;-nube(tw$text) wordcloud2(data=df[df$freq&gt;1,],color=&#39;random-dark&#39;,shape = &#39;pentagon&#39;) library(wordcloud) wordcloud(df$word,freq=df$freq,min.freq = 5) Gráfico de correlaciones library(ggplot2) library(ggthemes) docs&lt;-VCorpus(VectorSource(fb$text)) docs &lt;- docs %&gt;% tm_map(removeNumbers) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(stripWhitespace) docs &lt;- tm_map(docs, content_transformer(tolower)) docs &lt;- tm_map(docs, removeWords, stopwords(&quot;sp&quot;)) tdm&lt;-TermDocumentMatrix(docs) associations&lt;-findAssocs(tdm, &#39;evo&#39;, 0.55) associations&lt;-as.data.frame(associations) associations$terms&lt;-row.names(associations) associations$terms&lt;-factor(associations$terms, levels=associations$terms) names(associations)[1]&lt;-&quot;palabra&quot; ggplot(associations, aes(y=terms)) + geom_point(aes(x=palabra), data=associations, size=5)+ theme_gdocs()+ geom_text(aes(x=palabra, label=palabra), colour=&quot;darkred&quot;,hjust=-.25,size=8)+ theme(text=element_text(size=20), axis.title.y=element_blank()) 6.4 Análisis de sentimiento (sentimental scoring) El análisis de sentimientos es el proceso de extraer la intención emocional del autor de un texto. Se debe tener en cuenta: Aspectos culturales diferencias demográficas texto con sentimientos compuestos Hay varios marcos de referencias de emociones que se pueden considerar. Uno de los más usados es el creado en 1980 por Robert Plutchik (psicólogo), se establecen 8 emociones: (-) ira (anger) (-) miedo (fear) (-) tristeza (sadness) (-) asco (disgust) (+) sorpresa (surprise) (+) anticipación (anticipation) (+) confianza (trust) (+) alegría (joy) Espectro de emociones de Plutchik’s a partir de las primarias 6.4.1 Polarización y léxico subjetivo El análisis de sentimientos en R es bueno pero desafiante El análisis de sentimiento en R es muy bueno El análisis de sentimiento en Python no es bueno 6.4.2 Polarización en QDAP #solo ingles library(qdap) library(rtweet) tw&lt;-search_tweets(&quot;Bolivia&quot;,n=100,include_rts = F,lang=&quot;en&quot;) detach(package:dplyr, unload=TRUE) detach(package:rtweet, unload=TRUE) detach(package:qdap, unload=TRUE) `[[.qdap_hash` &lt;- `[[.data.frame` tw$text&lt;-removePunctuation(tw$text) score&lt;-polarity(tw$text[1:2]) 6.4.3 Librería syuzhet library(syuzhet) library(rtweet) tw&lt;-search_tweets(&quot;coronavirus&quot;,n=1000,include_rts = F,lang=&quot;es&quot;) ww&lt;-get_sentiment_dictionary(&quot;nrc&quot;,language = &quot;spanish&quot;) aa&lt;-get_nrc_sentiment(tw$text,language = &quot;spanish&quot;) barplot(apply(aa,2,sum),horiz = T,las=1) #ampliar el léxico ww&lt;-rbind(ww,c(&quot;spanish&quot;,&quot;xxxx&quot;,&quot;negative&quot;,&quot;1&quot;)) tail(ww) get_nrc_sentiment #tarea "]
]
