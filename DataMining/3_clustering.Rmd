---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Clustering
El clustering es un método cuyo objetivo es el de crear grupos en base a las relaciones multivariantes que existen en los datos, este método es un método previo a las técnicas de clasificación que existen. La base del clustering es la definición de la similaridad entre las filas. Similaridad es definida como una función de distancia entre un par de filas.

Es importante distinguir la existencia de grupos naturales dentro de los datos, normalmente estos grupos son características naturales de las observaciones de interés. 

## Medidas de Disimilaridad
Dado el objetivo del clustering, el aspecto mas importante dentro de estos métodos es utilizar de forma correcta la medida de (di)similaridad entre un para de casos dentro de la base de datos.

La definición de las medidas de distancia es crucial para aplicar estos modelos. Funciones de distancia incorrecta pueden generar sesgos en los resultados y ser un problema para etapas posteriores de la mineria de datos. Debemos distinguir las funciones de distancia segun la naturaleza de las variables.

Sean las filas $x$ e $y$ dentro de una base de datos, estos vectores tienen una dimensión $p$, es decir, se observan  $p$ variables para las 2 observaciones.

### Distancia Euclideana: Variables numéricas

$$d(x,y)=\sqrt{\sum_{i=1}^p{(x_i-y_i)^2}}$$

Donde los $x_i$ y $_y_i$ son los valores para la variable $i$ de las observaciones $x$ e $y$.

### Distancia Manhattan: $p$ grande

$$d(x,y)=\sum_{i=1}^p{|x_i-y_i|}$$

### Distancia Minkowski 

$$d(x,y)=\left(\sum_{i=1}^p{|x_i-y_i|^d}\right)^{1/d}$$

```{r}
aux<-matrix(rnorm(100),nrow=5)
dist(aux) #euclideana
dist(aux, method="manhattan")
dist(aux, method="minkowski", p=3)
```

### programando

```{r}
minkowski<-function(x,y,d){
  dd<-(sum(abs(x-y)**d))**(1/d)
  return(dd)
}
minkowski(c(1,2,3),c(4,2,1),d=2)
x<-c(1,2,3)
y<-c(4,2,1)
sum(abs(x-y)) # manhattan
sqrt(sum(abs(x-y)**2)) # euclideana
# la funcion de distancia
distancia<-function(bd,d=2){
  nf<-dim(bd)[1]
  DD<-matrix(NA,nf-1,nf-1)
  colnames(DD)<-1:(nf-1)
  rownames(DD)<-2:nf
  for(i in 1:(nf-1)){
    for(j in (i+1):nf){
      DD[j-1,i]<-minkowski(bd[i,],bd[j,],d)
    }
  }
  return(DD)
}
distancia(aux,d=2)
dist(aux)
```

### Variables cualitativas
Para las variables cualitativas se debe considerar los casos cuando estas son nominales y ordinales, distinguir tambien los casos de variables binarias.

```{r,eval=F}
install.packages("vegan")
library(vegan)
```

### Datos mixtos 
Una de los mayores desafios es cuando las variables son mixtas, es decir cuantitativas y cualitativas.

```{r,eval=FALSE}
install.packages("cluster")
library(cluster)
```

## Métodos de clustering

* Partición (k-center)
* Jerárquicos (dendograma)
* Basados en densidad
* Basados en cuadrículas (grid)

## K-center Clustering (no jerárquicos)

Algoritmo

1. Partición de las observaciones en $k$ grupos, obtener el vector de promedios de cada grupo (centroides). Se puede trabajar con la media o la mediana.
2. Para cada observación calcular las distancia euclideana a los centroides y reasignar lo observación en base a la menor distancia, recalcular los centroides en base a la reasignación de cada observación
3. Repetir el paso 2 hasta que que ya no existan más reasignaciones

```{r}
bd<-data.frame(x=rnorm(100),y=rnorm(100))
kmeans(bd,2)
```

Ejemplo: 

* Implementar el algoritmo para el k-center, con la distancia de Minkowski y para la media y mediana.

```{r}
#Nota: La entrada de la funcion es un data frame
kcenter<-function(bd,k=3,d=2,tipo="media",seed=123456){
  nf<-dim(bd)[1]
  nc<-dim(bd)[2]
  #paso1: asignar las k (nf>=k)
  set.seed(seed)
  bd$k<-sample(1:k,nf,replace=T)
  centroide<-NULL
  for(i in 1:k){
    if(tipo=="media"){  
      centroide<-rbind(centroide,apply(bd[bd$k==i,],2, mean))
    } else if(tipo=="mediana"){
      centroide<-rbind(centroide,apply(bd[bd$k==i,],2, median))
    }
  }
  #paso2 (recalcular loo centroides al final)
  cc<-1
  while(cc!=0){ #paso3
    cc<-0
    for(i in 1:nf){
      auxd<-NULL
      for(j in 1:k){
        auxd<-c(auxd,minkowski(bd[i,1:nc],centroide[j,1:nc],d=d))
      }
      newk<-which(auxd==min(auxd))
        if(newk!=bd$k[i]){
          bd$k[i] <- newk
          cc<-cc+1
        }
    }
    centroide<-NULL
    for(i in 1:k){
      if(tipo=="media"){  
        centroide<-rbind(centroide,apply(bd[bd$k==i,],2, mean))
      } else if(tipo=="mediana"){
        centroide<-rbind(centroide,apply(bd[bd$k==i,],2, median))
      }
    }
  }
  return(bd)
}
```

* Pensar en un gráfico que permita ver como se asignaron los cluster 

```{r}
bd<-data.frame(x=rnorm(100),y=rnorm(100),z=rnorm(100))
kcenter(bd,k=4,d=1,tipo="mediana")
```


### Validación cluster

* La estructura de los cluster es aleatoria (¿funciona?)
* ¿Cómo definimos el valor de $K$? 

> Silhouette coefficient: 

1. Se obtiene para la observación $i$ el promedio de distancia a todos los objetos en el mismo cluster ($a_i$)
2. Se obtiene para la observación $i$ el promedio de distancia a todos los objetos de los otros clusters ($b_i$)
3. Se define a $s_i$ como el coeficiente, con un recorrido entre $[-1,1]$, para cada observación $i$

$$s_i=\frac{b_i-a_i}{max(a_i,b_i)}$$

Idealmente se espera que $a_i < b_i$ y los $a_i$ cercanos a $0$. 

```{r}
library(cluster)
kk<-kmeans(bd,3)
s <- silhouette(kk$cluster, dist(bd))
plot(s)
#sobre la base IRIS
data("iris")
aux<-kmeans(iris[,-5],3)
s <- silhouette(aux$cluster, dist(iris[,-5]))
plot(s)
```

> Medoide: es el punto de datos que es "menos diferente" de todos los otros puntos de datos. A diferencia del centroide, el medoide tiene que ser uno de los puntos originales.

```{r}
pam(bd,k=3)
kmeans(bd,3)
```

* ¿Cúal es el número óptimo de $k$? 

```{r}
library(fpc)# Flexible Procedures for Clustering
sol <- pamk(iris[,-5], krange=2:10, criterion="asw", usepam=TRUE)
sol
pamk(bd,krange=2:10,usepam = T)
```


### Distancias para variables nominales (todas nominales)
En este caso la mejor estrategia es llevar las variables con sus categorias a variables binarias. Existen múltiples medidas de distancia para variables binarias, muchas de estas medidas son aproximaciones a las medidas mas conocidas. Entre ellas:

Sean las filas $i$, $j$ que contienen los valores binarios de las variables de estudio. Sea $A$ el total de $1$ que existe en $i$, $B$ el total de $1$ que existe en $j$ y sea $J$ el total de casos en los que los $1$ ocurren simultaneamente en $i$ y $j$.

  * Euclideana
  
  $$d_{ij}=\sqrt{A+B-2J}$$
  * Manhattan
  
  $$d_{ij}=A+B-2J$$
  * Bray
  
  $$d_{ij}=\frac{A+B-2J}{A+B}$$

  * Binomial
  
  $$d_{ij}=log(2)(A+B-2J)$$

```{r}
aux<-rbind(c(0,0,0,0,1,1,1),c(1,0,1,0,0,1,1))
A<-sum(aux[1,])
B<-sum(aux[2,])
J<-sum(apply(aux, 2, sum)==2)
#euclideana
sqrt(A+B-2*J)
#manhathan
A+B-2*J
#bray
(A+B-2*J)/(A+B)
#binomial
log(2)*(A+B-2*J)
library(vegan)
vegdist(aux,binary = T)
vegdist(aux,binary = F)
#una base de datos mas grandes
set.seed(999)
aux1<-matrix(rbinom(200,1,0.4),nrow = 20)
vegdist(aux1,method = "binomial",binary = T)
dist(aux1)
```


### Distancias para variables mixtas (cuantitativas, nominales, ordinales)

```{r}
library(cluster)
data("flower")
str(flower)
dd<-daisy(flower,metric = "gower")
summary(dd)
```

Ejercicios

1. Busque funciones en R que permitan calcular los k-center para variables mixtas y medoides 
2. Crear una función k-center para variables mixtas y alternativas para incluir el medoide.

## Cluster Jerárquico

El objetivo es obtener una jerarquía de posibles soluciones que van desde un solo grupo a $n$ grupos, donde $n$ es el número de observaciones en el conjunto de datos.

### Algoritmo (Johnson)

1. Se inicia con $n$ grupos y se genera una matriz de $nxn$ de distancias, $D=\{d_{ik}\}$
2. Buscar en la matriz de distancia los pares de cluster más cercanos entre ellos, "los cluster mas similares", si defifinimos los clusters $V$ y $U$, estamos interesados en encontrar $d_{UV}$
3. Unir los cluster $U$ y $V$, re etiquetar el nuevo cluster como $UV$. Actualizar la matriz de distancias a) remover las filas y columnas correspondientes a $U$ y $V$ b) incluimos las nuevas filas y columnas para el nuevo cluster $UV$.
4. Repetimos el paso 2 y 3 un total de $n-1$ veces.

El momento de definir el cluster más cercano, se puede emplear los siguientes enlaces:

* Single linkage (Enlace simple): Se elige al cluster más cercano, con la regla de que las distincia individual entre las observaciones dentro de los clusters es la más corta 
* Complete linkage (Enlace completo): Se elige el cluster mas cercano, con la regla que las distnaicas individuales entre las observaciones dentro de los cluster es la más larga
* Average linkage (Enlace promedio): Se elige el cluster mas cercano, considerando el promedio de las distancias entre los cluster.

>Nota: Se debe elegir la matriz de distancias acorde a la naturaleza de los datos, se recomienda:

  * Todas Numéricas: Euclideana o Manhatan
  * Todas nominales: Transformación a binarias y usar la distancia binomial
  * Mixtas: Distancia de Gower


```{r}
d <- dist(scale(iris[,-5]))#euclideana
h <- hclust(d)
plot(h,hang=-0.1,labels=iris[["Species"]],cex=0.5)
#elegir la cantidad de grupos
clus3 <- cutree(h, 5)
plot(h,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(h,k=5)
```

Probando los tres tipos de enlaces para $k=3$

```{r}
hs <- hclust(d,method = "single")
hc <- hclust(d,method = "complete")
ha <- hclust(d,method = "average")
cs<-cutree(hs,3)
cc<-cutree(hc,3)
ca<-cutree(ha,3)
table(cs,cc)
table(cs,ca)
table(cc,ca)
par(mfrow=c(1,3))
plot(hs,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(hs,k=3)
plot(hc,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(hc,k=3)
plot(ha,hang=-0.1,labels=iris[["Species"]],cex=0.5)
rect.hclust(ha,k=3)
```

Nota: El dendograma es muy útil para ver las relaciones que existen basadas en las distancias y la creación de las jerarquias, a partir de estos se puede definir un $k$ (de forma visual)

Nota: El dendograma pierde su utilidad cuando la cantidad de observaciones es muy alta, 

Ejemplo, Usar los datos de las elecciones del 20 de octubre, agregar los resultados en términos relativos para los municipios y generar el dendograma para los tres tipos de enlaces, de forma visual sugerir un valor de $k$ para cada tipo de enlace.

```{r}
library(dplyr)
load("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-384\\data\\oct20.RData")
#filtrar los casos
aux<-c("Número departamento","Departamento" ,"Número municipio","Municipio"   ,"CC","FPV","MTS","UCS","MAS - IPSP","21F","PDC","MNR","PAN-BOL","Votos Válidos","Blancos","Nulos")               
names(computo)[1]<-"pais"
names(computo)[12]<-"eleccion"
bd<-computo %>% filter(pais=="Bolivia" & eleccion=="Presidente y Vicepresidente") %>% select(aux)
names(bd)[1:4]<-c("idep","ddep","imun","dmun")
bdmun<-aggregate(bd[,5:16],bd[,1:4],sum)
bdmun<-bdmun[,-14]
bdmun[,5:15]<-prop.table(as.matrix(bdmun[,5:15]),1)
#cluster jerarquico
d<-dist(bdmun[,5:15])
plot(hclust(d),hang=-0.1,label=bdmun$dmun,cex=0.5)
# determinar el mejor k y el mejor enlace
mm<-c("single", "complete", "average") # método
k<-2:20 # cantidad de cluster
d<-dist(bdmun[,5:15]) # matriz de distancia
# matriz de resultados
res<-matrix(NA, nrow = 19,ncol=3)
colnames(res)<-mm
rownames(res)<-k
#######################
for(i in k){
  for(j in 1:3){
    h<-hclust(d,method = mm[j])
    c<-cutree(h,i)
    s<-silhouette(c,d)
    res[i-1,j]<-median(s[,3])
  }
}
#la mejor opción es k=2 con el método average
h<-hclust(d,method = "average")
c<-cutree(h,2)
plot(h,hang=-0.1,labels=bdmun$dmun,cex=0.4)
rect.hclust(h,k=2)
bdmun$cluster<-c
group_by(bdmun,cluster) %>% summarise(mean(CC),mean(`MAS - IPSP`))
```


## Ejercicios
1. Pensar en un gráfico que permita ver como se asignaron los cluster 
2. Pensar en optimizar el código empleado para el k-center
3. Hacer que la función desarrollada para el k-center retorne también los centroides
4. Utilizando la base de datos de las elecciones del 20 de octubre, crear una base de datos a nivel municipal, aplicar el método k-center con medoides para los resultados a nivel municipal y terminar el $k$ óptimo en un rango de $k=2:10$ (Usar datos relativos).