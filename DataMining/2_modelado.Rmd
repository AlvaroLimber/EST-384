# Modelado en Minería de datos

## Explorando los datos

Existen dos aproximaciones para empezar a explorar la información existente en una base de datos:

* Resumen de los datos
* Visualización de los datos

### Resumen de los datos

Dado el tamaño de las bases de datos resulta imposible o muy difícil conocer todas sus propiedad, el resumen de los datos intenta brindar propiedades claves de los datos, estas propiedades podrían ser:

* Cual es el valor más común
* Cuan variable o dispersa esta la información
* Existen valores extraños o inesperados en la base de datos
* Los datos siguen alguna distribución

A continuación se emplea la encuesta a hogares 2019 para ir respondiendo estas preguntas.

En cuanto a los valores mas comunes, la media y la mediana de los datos son suficientes para las variables cuantitativas, mientra que para variables cualitativas, las categorías mas frecuentes son una buena opción.

```{r}
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData"))
# media de edad
mean(eh19p$s02a_03)
#mediana de edad
median(eh19p$s02a_03)
# para las categorías más frecuentes
library(DMwR2)
library(dplyr)
#para la edad
centralValue(eh19p$s02a_03)
#para el sexo
centralValue(eh19p$s02a_02)
#para el departamento 
centralValue(eh19p$depto)
```

A veces es mejor ver los resultados por grupos, por ejemplo, podemos verlo por departamento y área.

```{r}
eh19p %>% group_by(depto,area,p0) %>% summarise(media_edad=mean(s02a_03),mediana_edad=median(s02a_03),centralValue(s02a_02),n())
```

La función n() realiza un proceso de conteo.

De forma similar podemos hacer para las medidas de variabilidad, las mas comunes la desviación estándar, el rango, el rango intercuartil y los cuantiles. Usando la eh19 para la edad y el ingreso laboral.

```{r}
##EDAD
# Desviacion estandar
sd(eh19p$s02a_03)
# Rango
range(eh19p$s02a_03)
# Rango intercuartil
IQR(eh19p$s02a_03)
# Quantiles
quantile(eh19p$s02a_03)
quantile(eh19p$s02a_03,c(0.10,0.90))
##INGRESO LABORAL
# Desviacion estandar
sd(eh19p$ylab,na.rm = T)
# Rango
range(eh19p$ylab,na.rm = T)
# Rango intercuartil
IQR(eh19p$ylab,na.rm = T)
# Quantiles
quantile(eh19p$ylab,na.rm = T)
quantile(eh19p$ylab,probs=c(0.10,0.90),na.rm = T)
##por departamento y area
tapply(eh19p$ylab,list(eh19p$depto,eh19p$area),sd,na.rm=T)#opcion1
eh19p %>% group_by(depto,area) %>% summarise(sd(ylab,na.rm=T),n())

t1<-eh19p %>% group_by(depto,area) %>% summarise(quantile(ylab,na.rm=T))
View(t1)

tapply(eh19p$ylab,list(eh19p$depto,eh19p$area),quantile,na.rm=T)#con problemas
aggregate(eh19p$ylab,list(depto=eh19p$depto,area=eh19p$area),quantile,na.rm=T)
```

Finalmente, para explorar a fondo las variables la función describe es bastante útil, también, el comando summary.

```{r}
#analizando las 5 primeras variables de la base de datos
library(Hmisc)
describe(eh19p[,1:10])

summary(eh19p$ylab)
by(eh19p[,1:10],eh19p$area,summary)

by(eh19p[,c("depto","ylab")],eh19p$area,describe)

eh19p %>% group_by(depto) %>% summarise(n(),mean(s02a_03))
```

### Visualización

La visualización es una herramienta importante para explorar y entender la base de datos. Los seres humanos son excelentes para capturar patrones visuales, y la visualización de datos intenta capitalizar en estas habilidades. Es útil diferenciar las visualizaciones por:

* Una sola variables
* Dos variables
* Multiples variables

Los aspectos vinculados al uso de gráficos de origen de R y la librería ggplot pueden verse en el texto guía de EST-383 "BigData". A continuación se introducen de forma directa funciones en R orientadas a la visualización univariante, bivariante y multivariante.

Usando al EH-2019, para variables cualitativas.
```{r}
#Gráficos de origen de R
barplot(table(eh19p$s03a_01a),main="Dónde vivia hace 5 años?")
#GGPLOT
library(ggplot2)
ggplot(eh19p,aes(x=s03a_01a))+geom_bar()+ggtitle("Dónde vivia hace 5 años?")
```

Para variables del tipo cuantitativas

```{r}
par(mfrow=c(1,2))
boxplot(eh19p$ylab)
hist(eh19p$ylab)
dev.off()

ggplot(eh19p,aes(ylab))+geom_boxplot()
ggplot(eh19p,aes(ylab))+geom_histogram()
```

Si ahora queremos comparar usar ambas variables cuanti y cuali

```{r}
boxplot(eh19p$ylab~eh19p$s03a_01a)
ggplot(eh19p,aes(x=s03a_01a,y=ylab))+geom_boxplot()
ggplot(eh19p,aes(x=s03a_01a,y=ylab))+geom_violin()
```

Usando ambas variables cuantitativas

```{r,warning=F,message=FALSE}
plot(eh19p$tothrs,eh19p$ylab)
plot(eh19p[,c("ylab","tothrs","aestudio")])
#pairs(eh18p[,c("ylab","tothrs","aestudio")]) similar al anterior
library(GGally)
ggpairs(eh19p,columns = c("ylab","tothrs","aestudio"))

library(ggridges)
ggplot(eh19p %>% filter(s02a_03>=15 & ylab<10000), aes(x = ylab, y = s02a_02, fill = s02a_02)) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")+
  ggtitle("Ingreso laboral por sexo, personas de 15 años o más")

ggplot(eh19p %>% filter(s02a_03>=15 & ylab<10000), aes(x = ylab, y = depto, fill = depto)) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")+
  ggtitle("Ingreso laboral por departamento, personas de 15 años o más")

ggplot(eh19p %>% filter(s02a_03>=15 & ylab<10000), aes(x = ylab, y = area, fill = area)) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")+
  ggtitle("Ingreso laboral por rural, personas de 15 años o más")

```

Ahora si combinamos variables cuanti y cuali con ggpairs.

```{r,warning=F,message=FALSE}
ggpairs(eh19p,columns = c("ylab","tothrs","s03a_01a","area"))
```

Alternativas Multivariantes,

```{r}
#trabajando a partir de una muestra de 100 individuos
s<-sample(1:dim(eh19p)[1],100)
i<-match(c("s02a_03","aestudio","ylab","tothrs"),names(eh19p))
ggparcoord(eh19p[s,],columns = i,groupColumn = "area",boxplot=T)

library("TeachingDemos")
faces(na.omit(eh19p[s,i]))
```

## Componentes Principales

El método de Análisis de Componentes Principales se ocupa de explicar la estructura de varianza y covarianza de un grupo de variables a través de unas pocas combinaciones lineales de este grupo de variables. En general sus objetivos son (1) la reducción de los datos y (2) la interpretación.

Algebráicamente, los componentes principales son combinaciones lineales de $p$ variables aleatorias $X_1$, $X_2$, \ldots, $X_p$. Geométricamente, estas combinaciones lineales representan la selección de un nuevo sistema de coordenadas obtenido por rotación de del sistema original con $X_1$, $X_2$, \ldots, $X_p$ como los ejes de coordenadas. Los nuevos ejes representan la dirección con la máxima variabilidad y provee una simple y más parsimoniosa descripción de la estructura de la covarianza.

Los componentes principales dependen únicamente de la matriz de covarianza $\Sigma$ o la matriz de correlaciones $\rho$ (Matriz estandarizada de $\Sigma$) de $X_1$, $X_2$, \ldots, $X_p$. Su desarrollo no requiere de ningún supuesto de normalidad multivariada, sin embargo, componentes principales derivados de poblaciones normales multivariantes tienen un gran uso en la interpretación en términos de elipsoide de densidad constante.

Sea la matriz $\mathbf{X}$ compuesta de $p$ vectores aleatorios $\mathbf{X}=[X_1, X_2, \ldots, X_p ]$ que tiene la matriz de covarianza $\Sigma$ con valores propios $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p \geq 0$.

Considere la combinación lineal:

\begin{equation}
\begin{array}{rrr}
Y_1		= & a_1^{'} \mathbf{X} = & a_{11} X_1 + a_{12} X_2 + \ldots a_{1p} X_p \\
Y_2		= & a_2^{'} \mathbf{X} = & a_{21} X_1 + a_{22} X_2 + \ldots a_{2p} X_p\\
\vdots	= & \vdots & \vdots \\
Y_p		= & a_p^{'} \mathbf{X} = & a_{p1} X_1 + a_{p2} X_2 + \ldots a_{pp} X_p\\
\end{array}
\label{cp1}
\end{equation}

Equivalente a:

\begin{equation}
\mathbf{Y}= \left[  
\begin{array}{c}
Y_1\\
Y_2\\
\vdots\\
Y_p\\
\end{array}
\right] = \left[  
\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1p} \\
a_{21} & a_{22} & \ldots & a_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
a_{21} & a_{p2} & \ldots & a_{pp} \\
\end{array}
\right] \left[ 
\begin{array}{c}
X_1\\
X_2\\
\vdots\\
X_p\\
\end{array}
\right] = \mathbf{A X}
\label{cp2}
\end{equation}

La combinación lineal $\mathbf{Y}=\mathbf{AX}$ tiene:

\begin{equation}
\mu_y=E(\mathbf{Y})=E(\mathbf{AX})=A \mu_x
\label{cp3}
\end{equation}

\begin{equation}
\Sigma_y=Cov(\mathbf{Y})=Cov(\mathbf{AX})=A \Sigma A^{'}
\label{cp4}
\end{equation}

En base a \ref{cp4}, se obtiene:

\begin{equation}
Var(Y_i)=a_i^{'} \Sigma a_i \quad i=1, 2, \ldots, p
\label{cp5}
\end{equation}

\begin{equation}
Cov(Y_i,Y_k)=a_i^{'} \Sigma a_k \quad i,k=1, 2, \ldots, p
\label{cp6}
\end{equation}

Los componentes principales son combinaciones lineales incorrelacionadas, tal que \ref{cp5} es lo más grande posible.

El primer componente principal es la combinación lineal con máxima varianza. Entonces se debe maximizar $Var(Y_1)=a_1^{'} \Sigma a_1$. Es claro que $Var(Y_1)$ puede ser incrementada multiplicando a $a_1$ por alguna constante. Para eliminar esta indeterminación, es conveniente restringir los coeficientes del vector. Por lo tanto se define.

$$
\begin{array}{rcl}
\text{Primer componente principal} & = & \text{Combinacion lineal} \quad a_1^{'}X \quad \text{que maximiza} \\
								   &   & Var(a_1^{'}X) \quad \text{sujeto a} \quad a_1^{'} a_1=1\\
\text{Segundo componente principal} & = & \text{Combinacion lineal} \quad a_2^{'}X \quad \text{que maximiza} \\
								   &   & Var(a_2^{'}X) \quad \text{sujeto a} \quad a_2^{'} a_2=1 \quad y \\													   &   & Cov(a_1^{'}X,a_2^{'}X)=0			   
\end{array}
$$
 
Para el $i-esimo$ paso:

$$
\begin{array}{rcl}
i-esimo \text{ componente principal} & = & \text{Combinacion lineal} \quad a_i^{'}X \quad \text{que maximiza} \\
								   &   & Var(a_i^{'}X) \quad \text{sujeto a} \quad a_i^{'} a_i=1 \quad y \\													   &   & Cov(a_i^{'}X,a_k^{'}X)=0 \quad	para \quad k<i			   
\end{array}
$$

$$\Sigma_{(pxp)}= A_{(p*p)} \Lambda_{(p*p)} A^t_{(p*p)}  $$
El aspecto central de componentes principales es trabajar con menos variables, sea $m<p$, el método de componentes busca a partir de $m$, los siguiente:

$$\Sigma_{(pxp)} \approx  A_{(p*m)} \Lambda_{(m*m)} A^t_{(m*p)}$$

Los pasos sugeridos para iniciar el análisis de componentes principales son:

1. Identificar las variables de interés dentro de la matriz de datos, si las variables tienen las mismas escalas se recomienda emplear la matriz de covarianza, si las escalas son diferentes, se recomienda trabajar con la matriz de correlaciones.
2. Obtener los componentes principales, los eigen valores y la matriz de eigen vectores
3. (Opcional) Eliminar las variables redundantes, 
  * se sugiere identificar las variables del conjunto de datos correlacionadas con los últimos componentes.
4. Calcular nuevamente los componentes principales excluyendo las variables identificadas en el paso previo
5. Elegir el número de componentes a retener $m$ (scree plot, tamaño de los eigen valores, etc)
6. Analizar los resultados y usar los componentes

Usos de componentes principales.

  * Detección de valores atípicos
  * Identificación de posibles factores 
  * Los primeros componentes se pueden usar como indicadores
  * Eliminan la multicolinealidad de un modelo de regresión múltiple

```{r}
load(url("https://github.com/AlvaroLimber/EST-384/blob/master/data/oct20.RData?raw=true"))
names(computo)[18]<-"MAS"

bd<-computo %>% filter(País=="Bolivia" & Elección=="Presidente y Vicepresidente")

bdmun<-bd %>% group_by(Departamento,Provincia,Municipio) %>% summarise_at(vars(Inscritos:Nulos),sum)

bdmun[,5:13]<-prop.table(as.matrix(bdmun[,5:13]),1)*100

cov(bdmun[,5:13])

sigma<-cor(bdmun[,5:13])

library(corrplot)
corrplot(sigma)
#desc. espectral

Lambda<-diag(eigen(sigma)$values)
A<-eigen(sigma)$vectors

A %*% Lambda %*% t(A)

#round(A %*% t(A),2)

#componentes principales 

#componentes a retener
barplot(eigen(sigma)$values)
abline(h=1,col="red")
plot(1:9,eigen(sigma)$values,type="b")

aux<-round(eigen(sigma)$values/sum(eigen(sigma)$values),4)*100
cumsum(aux)

sigma3<-A[,1:3] %*% Lambda[1:3,1:3] %*% t(A[,1:3])
sigma2<-A[,1:2] %*% Lambda[1:2,1:2] %*% t(A[,1:2])

par(mfrow=c(1,3))

colnames(sigma)<-1:9
rownames(sigma)<-1:9

corrplot(sigma)
corrplot(sigma3)
corrplot(sigma2)
dev.off()
as.matrix(bdmun[,5:13])%*%A[,1]

Y<-as.matrix(scale(bdmun[,5:13]))%*%A
plot(density(Y[,1]))

plot(Y[,1],Y[,2])
```



```{r}
#1. Seleccione una base de datos de interés del repositorio 
load(url("https://github.com/AlvaroLimber/EST-384/blob/master/data/oct20.RData?raw=true"))
#2. Seleccione las variables para el PCA (Según la motivación) 
vv<-c(14:22,24,25)
#2A TRANFORMAR 
vval<-apply(computo[,vv],1,sum)
aux<-computo[,vv]/vval
#2B LIMPIEZA 
#3. Calcule el PCA
aux1<-na.omit(aux)
cp1<-eigen(cov(aux1))
cp2<-eigen(cor(aux1))
#4. Identifique el número de CPs que explican hasta el 90% de la varianza
cumsum(cp1$values)/sum(cp1$values) 
cumsum(cp2$values)/sum(cp2$values) 
#5. Identifique las variables correlacionadas con la cantidad de 
#componentes fuera del 90% del paso anterior
cp11cov<-as.matrix(aux1)%*%cp1$vectors[,11] 
cp11cor<-as.matrix(aux1)%*%cp2$vectors[,11] 
cor(cbind(aux1,cp11cov,cp11cor))[1:11,12:13]
cp11<-eigen(cov(aux1[,-2]))
cumsum(cp11$values)/sum(cp11$values)
cp10cov<-as.matrix(aux1[,-2])%*%cp11$vectors[,10] 
#6. Calcule nuevamente el componente principal eliminando las variables rebundantes
#7. Determine la cantidad de componentes principales a retener
cp1cov<-as.matrix(aux1[,-2])%*%cp11$vectors[,1] 
cp2cov<-as.matrix(aux1[,-2])%*%cp11$vectors[,2] 
plot(cp1cov,cp2cov)
#8. Defina un indicador a partir de estos
cor(cbind(aux1,cp1cov,cp2cov))[1:11,12:13]
bd<-cbind(aux1,cp1cov,cp2cov)
names(bd)[5]<-"MAS"
#9. Modele un modelo lineal empleando los CPs retenidos.
summary(lm(MAS~cp1cov,data=bd))
```

## Análisis de correspondencia

El análisis de correspondencia esta orientado a encontrar relaciones entre las categorías de variables cualitativas. 

Esta técnica es un método visual que va más allá del test de independencia Chi-cuadrado.


```{r,eval=F}
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/endsa.RData"))
#tarea 10 min, explorar la base de datos
library(Hmisc)
describe(endsa)
names(endsa)
aux<-attributes(endsa)

names(endsa)
#aeXX
aux$var.labels[7:15]
#nupXX
aux$var.labels[23:26]
#antXX
aux$var.labels[16:20]
#vfXX
aux$var.labels[50:59]
table(endsa$year,endsa$sexo)

table(endsa$edad,endsa$sexo)

library(dplyr)

N<-endsa %>% filter(year==2008) %>% select(ae01,ae08) %>% table()
N<-t1[1:4,]
chisq.test(N)
N
n<-sum(N)
P<-N/n
addmargins(N)
```


Para resumir la teoría, primero divida la matriz de datos $I × J$, denotada por $N$, por su gran total $n$ para obtener la llamada matriz de correspondencia $P = N / n$. Deje que los totales marginales de fila y columna de $P$ sean los vectores $r$ y $c$ respectivamente, es decir, los vectores de masas de fila y columna, y $Dr$ y $Dc$ sean las matrices diagonales de estas matrices. El algoritmo computacional para obtener coordenadas de los perfiles de fila y columna con respecto a los ejes principales, usando el SVD, es el siguiente:

1. Calcular la matriz de residuos estadarizados: $S=D_r^{-1/2}(P-rc^t)D_c^{-1/2}$
2. Calcular la descomposición SVD de $S$: $S=UD_{\alpha}V^t$, donde $U^T U=V^T V=I$
3. Coordenadas principales de filas: $F=D_r^{-1/2} U D_{\alpha}$
4. Coordenadas principales de columnas: $G=D_c^{-1/2} V D_{\alpha}$
5. Coordenadas estándar de filas: $X=D_r^{-1/2} U$
6. Coordenadas estándar de columnas: $Y=D_c^{-1/2} V$
7. Calcular la Inercia: 
$$\phi^2=\sum_i^I\sum_j^J{\frac{(p_{ij}-r_i c_j)^2}{r_i c_j}}$$
8. Graficar las coordenadas de F y G según la la inercia contenida en la matriz $D_{\alpha}$

En R, existe la libreria ca que permite acceder a las coordenadas del método de correspondencia.

```{r,message=F,warning=FALSE,eval=F}
#ejemplo CA
#install.packages("ca")
library(dplyr)
library(ca)
data("smoke")
model<-ca(smoke)
plot(model)
names(model)
summary(model)
#ejemplo ENDSA
load(url("https://github.com/AlvaroLimber/EST-384/raw/master/data/endsa.RData"))
ll<-attributes(endsa)
ll$var.labels
t1<-endsa %>% filter(year==2008) %>% select(7,14) %>% table()
t1<-t1[1:4,]
#test chi2
chisq.test(t1)
model<-ca(t1)
model
plot(model)
#programando el ca
lcol<-colnames(t1)
lrow<-rownames(t1)
P<-prop.table(t1)
r<-margin.table(P,1)
c<-margin.table(P,2)
Dr<-diag(r)
Dc<-diag(c)
##Paso 1
P-r%*%t(c)
#error en las matriz diagonales
Dr^(-0.5)%*%(P-r%*%t(c))%*% Dc^(-0.5)
# corrigiendo el problema
S<-diag(r^(-0.5))%*%(P-r%*%t(c))%*% diag(c^(-0.5))
# 2 descomposición SVD
svd(S)
U<-svd(S)$u
V<-svd(S)$v
Da<-diag(svd(S)$d)
#verificando las propiedades
U %*% t(U) 
t(V) %*% V
U %*% Da %*% t(V)
S
# 3 Coordenadas principales filas
FF<- diag(r^(-0.5)) %*% U %*% Da
# 4 Coordenadas principales columnas
G<- diag(c^(-0.5)) %*% V %*% Da
# 5 Coordenadas estandar filas
X<- diag(r^(-0.5)) %*% U 
# 6 Coordenadas estandar columnas
Y<- diag(c^(-0.5)) %*% V 
# 7 inercia
sum(((P-r%*%t(c))**2)/(r%*%t(c)))
#graficando
xmin<-min(c(FF[,1],G[,1]))
xmax<-max(c(FF[,1],G[,1]))
ymin<-min(c(FF[,2],G[,2]))
ymax<-max(c(FF[,2],G[,2]))
plot(FF[,1],FF[,2],col="red",xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1)
points(G[,1],G[,2],col="blue")
abline(h=0,v=0,lty=2)
#incluyendo el texto
plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = "n")
text(FF[,1],FF[,2],labels = lrow,col="red",cex=0.7)
text(G[,1],G[,2],labels = lcol,col="blue",cex=0.7)
abline(h=0,v=0,lty=2)
#incluyendo mas información
plot(FF[,1],FF[,2],xlim=c(xmin,xmax)*1.5,ylim=c(ymin,ymax)*1.1,type = "n")
text(FF[,1],FF[,2],labels = lrow,col="red",cex=0.5+r*2)
text(G[,1],G[,2],labels = lcol,col="blue",cex=0.5+c*2)
abline(h=0,v=0,lty=2)
#viendo solo una dimensión
plot(rep(0,dim(FF)[1]),FF[,1],type = "n", axes = F)
axis(2)
text(rep(0,dim(FF)[1]),FF[,1],labels = lrow,col="red",cex=0.5+r*2)
text(rep(0,dim(G)[1]),G[,1],labels = lcol,col="blue",cex=0.5+c*2)
abline(h=0,v=0,lty=2)
```

### Un ejemplo de correspondencia simple y múltiple

```{r,eval=F}
library(FactoMineR)
library(explor)

endsa08<-endsa %>% filter(year==2008)

model1<-MCA(endsa08[,c("ae01","ae05","ae08","ae09")])
explor(model1)

model1$eig
model1$call

model2<-PCA(endsa08[,c("nup02","nup03","edad")])
explor(model2)
```

